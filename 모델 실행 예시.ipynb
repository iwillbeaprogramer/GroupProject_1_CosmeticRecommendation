{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def readdata_and_savemodel(filename):\n",
    "    #INPUT = 1000001000100010001.csv,1000001000100010002.csv,1000001000100010003.csv,1000001000100010004.csv,1000001000100010011.csv\n",
    "    #OUTPUT : 모델이 경로에 저장됨\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    from keras.utils import to_categorical\n",
    "    from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "    import os\n",
    "    \n",
    "    # 파일로부터 X,Y데이터를 읽어서 전처리(백터화 및 원핫인코딩)\n",
    "    x_data,y_data = load_from_dataset(filename) #읽어옴\n",
    "    x_train = make_x_train(x_data) #전처리\n",
    "    y_train = make_y_train(y_data) #전처리\n",
    "    name=filename.split(\".\")[0] #파일이름에서 카테고리 키워드를 받아옴\n",
    "\n",
    "    #모델 저장경로 설정\n",
    "    MODEL_DIR = './model/'\n",
    "    if not os.path.exists(MODEL_DIR):\n",
    "        os.mkdir(MODEL_DIR)\n",
    "    modelpath = './model/'+name+'.hdf5'\n",
    "    checkpointer = ModelCheckpoint(filepath=modelpath, monitor = 'loss', save_best_only=True, verbose=1)\n",
    "\n",
    "    #모델 선언 및 layer설정\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64,input_dim = len(x_train[0]),activation='relu'))\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(Dense(len(y_train[0]),activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    #가장 마지막에 있는 모델 하나만 저장\n",
    "    history = model.fit(x_train,y_train,epochs=2000,verbose=1,batch_size=1,callbacks=[checkpointer])\n",
    "\n",
    "    \n",
    "def make_y_train(result2):\n",
    "    #INPUT : load_from_dataset로 읽어온 Y값 전처리 함수\n",
    "    import numpy as np\n",
    "    from keras.utils import to_categorical\n",
    "    from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "    f=LabelEncoder()\n",
    "    f.fit(result2)\n",
    "    y_train = f.transform(result2)\n",
    "    #OUTPUT = [0....1....0] 원핫벡터\n",
    "    return to_categorical(y_train)\n",
    "\n",
    "def x_onehot_encoding(input_list):\n",
    "    # X데이터 Preprocessing 하는 1단계함수 직접사용할 일이 거의 없음\n",
    "    # make_x_train 함수에서 사용됨\n",
    "    # INPUT = ['복합성', '웜톤', '트러블', '모공', '민감성', '잡티']\n",
    "    name_list=[\"복합성\",\"건성\",\"지성\",\"쿨톤\",\"웜톤\",\"잡티\",\"미백\",\"주름\",\"각질\",\"트러블\",\"블랙헤드\",\"피지과다\",\"민감성\",\"모공\",\"탄력\",\"홍조\",\"아토피\"]\n",
    "    result=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    for i in input_list:\n",
    "        index = name_list.index(i)\n",
    "        result[index]+=1\n",
    "    # OUTPUT = [1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0]\n",
    "    return result\n",
    "\n",
    "def make_x_train(result1):\n",
    "    #INPUT : load_from_dataset로 읽어온 Y값 전처리 함수\n",
    "    # X데이터를 가공하는함수\n",
    "    # 내부에서 x_onehot_encoding을 사용하며 for문으로 모든 x값을 처리\n",
    "    import numpy as np\n",
    "    temp=[]\n",
    "    result = list(result1)\n",
    "    for i in result:\n",
    "        temp.append(x_onehot_encoding(i))\n",
    "    #여기서 나온 값을 바로 model.fit 시키면 됨\n",
    "    #OUTPUT\n",
    "    #[\n",
    "    #    [1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0],\n",
    "    #    [1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0],\n",
    "    #    [1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0]\n",
    "    #]\n",
    "    return np.array(temp)\n",
    "\n",
    "def load_from_dataset(string):\n",
    "    # INPUT : string은 csv 파일 이름\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(string,encoding='utf-8')\n",
    "    y_data = df.values[:,1]\n",
    "    x_data = df.values[:,-1]\n",
    "    temp=[]\n",
    "    for i in x_data:\n",
    "        willappend = i.split(\" \")\n",
    "        willappend.pop()\n",
    "        temp.append(willappend)\n",
    "    # csv파일로 부터 x값, y값을 각각 읽어서 반환\n",
    "    # 아직 전처리 안됨\n",
    "    return temp,y_data\n",
    "\n",
    "def load_model_hdf5(filename):\n",
    "    # INPUT : 1000001000100010001.csv\n",
    "    # hdf파일 읽어옴\n",
    "    from tensorflow import keras\n",
    "    loaded_model = keras.models.load_model('model/'+filename)\n",
    "    # load한 모델을 리턴\n",
    "    return loaded_model\n",
    "\n",
    "def predict_code_value(category_name,input_value):\n",
    "    # categoty_name  ex)로션, 세럼, 토너\n",
    "    #input_value = [\"복합성\",등등등]\n",
    "    categoryno = Name_to_CategoryNo(category_name)\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    b=np.array(x_onehot_encoding(input_value)).reshape(1,-1)\n",
    "    model = load_model_hdf5(categoryno+'.hdf5')\n",
    "    x,y=load_from_dataset(categoryno+'.csv')\n",
    "    df = pd.read_csv(categoryno+'.csv',encoding='utf-8')\n",
    "    code = y[model.predict_classes(b)]\n",
    "    url = 'https://www.oliveyoung.co.kr/store/goods/getGoodsDetail.do?goodsNo={}&dispCatNo={}'.format(code[0],categoryno)\n",
    "    #          'A000000103112'\n",
    "    # 이름 가격 url\n",
    "    row = df[df['id']==code[0]]\n",
    "    pre_price = row['price'].astype(str)\n",
    "    pre_name = row['name'].astype(str)\n",
    "    price=text_processing(pre_price.iloc[0])\n",
    "    name=text_processing(pre_name.iloc[0])\n",
    "    return name,price,url \n",
    "\n",
    "def get_url(code,filename):\n",
    "    #url을 가공해서 리턴하는 함수\n",
    "    catno = filename.split(\".\")[0]\n",
    "    base_url='https://www.oliveyoung.co.kr/store/goods/getGoodsDetail.do?'+'goodsNo={}&dispCatNo={}'.format(code,catno)\n",
    "    # 필요에 따라 print함수와 함께 사용\n",
    "    return base_url\n",
    "\n",
    "def GoodsNo_to_Name(goodsnum):\n",
    "    # 상품번호에서 이름으로 변환하는 함수\n",
    "    if goodsnum=='1000001000100010001' or goodsnum==1000001000100010001:\n",
    "        return \"스킨/토너\"\n",
    "    elif goodsnum=='1000001000100010002' or goodsnum==1000001000100010002:\n",
    "        return \"로션\"\n",
    "    elif goodsnum=='1000001000100010003' or goodsnum==1000001000100010003:\n",
    "        return \"에센스/세럼\"\n",
    "    elif goodsnum=='1000001000100010011' or goodsnum==1000001000100010011:\n",
    "        return \"앰플\"\n",
    "    elif goodsnum=='1000001000100010004' or goodsnum==1000001000100010004:\n",
    "        return \"크림\"\n",
    "    else:\n",
    "        print(\"없는 코드입니다.\")\n",
    "        return \n",
    "\n",
    "def Name_to_CategoryNo(category_name):\n",
    "    # 이름에서 상품번호로 변환하는 함수\n",
    "    if category_name=='토너' or category_name=='스킨' or category_name=='스킨토너' or category_name=='토너스킨' or category_name==\"스킨/토너\" or category_name==\"토너/스킨\":\n",
    "        return '1000001000100010001'\n",
    "    elif category_name=='로션':\n",
    "        return '1000001000100010002'\n",
    "    elif category_name=='에센스' or category_name=='세럼' or category_name=='새럼' or category_name=='에센스/세럼' or category_name=='에센스/새럼' or category_name=='세럼/에센스' or category_name=='새럼/에센스':\n",
    "        return '1000001000100010003'\n",
    "    elif category_name=='앰플' or category_name=='엠플':\n",
    "        return '1000001000100010011'\n",
    "    elif category_name=='크림':\n",
    "        return '1000001000100010001'\n",
    "    else:\n",
    "        print(\"없는 코드코드입니다.\")\n",
    "        return \n",
    "def get_url(goods_num,filename):\n",
    "    #url을 가공해서 리턴하는 함수\n",
    "    catno = filename.split(\".\")[0]\n",
    "    base_url='https://www.oliveyoung.co.kr/store/goods/getGoodsDetail.do?'+'goodsNo={}&dispCatNo={}'.format(goods_num,catno)\n",
    "    # 필요에 따라 print함수와 함께 사용\n",
    "    return base_url\n",
    "\n",
    "def text_processing(string):\n",
    "    # 택스트 가공함수\n",
    "    # INPUT string=\"한번 써보고 너무 좋아서(^_^)매번 구입해서 사용 중이에요:). 바이오더마 제품이라 믿음도 가요.물스킨 타입이라 닦아내는 용도로 쓰고 있는 데 적당히 쿨링감 있고 좋아요저는 건성이긴 하지만 이 제품은 가볍고 깨끗한 느낌이라지성한테 더 잘 어울리는 제품인 거 같아요보습이 강하지는 않고 진정이랑 산뜻함?? 이런 느낌이 강해요. 향도 쎄지 않은 그냥 쿨한 느낌이고 어름에 쓰기 딱 좋은 거 같아요냉장고에 보관하고 사용 중인데 더 피부진정에도움이 되는 거 같아요.자극도 없이 순하고 세네통 넘게 사용 중이에요 남자친구도 안끈적거리고 시원하고 좋다고 애용중입니다:)\"\n",
    "    # 줄바꿈, 특수기호, 불규칙한 인덴트 제거\n",
    "    file = open(\"test.txt\",'w',encoding='utf-8')\n",
    "    file.write(string)\n",
    "    file.write(\"\\n\")\n",
    "    file.close()\n",
    "    file=open(\"test.txt\",'r',encoding='utf-8')\n",
    "    result=''\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        result+=line.replace(\"\\n\",'')\n",
    "    file.close()\n",
    "    return result\n",
    "\n",
    "def category_tag_to_dictionary(category_name,input_value):\n",
    "    a,b,c = predict_code_value(category_name,input_value)\n",
    "    result = {\n",
    "        'name' : a,\n",
    "        \"price\" : b,\n",
    "        \"url\" : c,\n",
    "    }\n",
    "    return result\n",
    "\n",
    "#readdata_and_savemodel(\"1000001000100010001.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "240/240 [==============================] - 0s 2ms/step - loss: 5.6000 - accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00001: loss improved from inf to 5.60004, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 2/2000\n",
      "240/240 [==============================] - 0s 802us/step - loss: 5.2889 - accuracy: 0.0250\n",
      "\n",
      "Epoch 00002: loss improved from 5.60004 to 5.28885, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 3/2000\n",
      "240/240 [==============================] - 0s 792us/step - loss: 4.3112 - accuracy: 0.0917\n",
      "\n",
      "Epoch 00003: loss improved from 5.28885 to 4.31118, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 4/2000\n",
      "240/240 [==============================] - 0s 789us/step - loss: 3.2111 - accuracy: 0.2833\n",
      "\n",
      "Epoch 00004: loss improved from 4.31118 to 3.21114, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 5/2000\n",
      "240/240 [==============================] - 0s 780us/step - loss: 2.5820 - accuracy: 0.3750\n",
      "\n",
      "Epoch 00005: loss improved from 3.21114 to 2.58197, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 6/2000\n",
      "240/240 [==============================] - 0s 801us/step - loss: 2.2924 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00006: loss improved from 2.58197 to 2.29242, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 7/2000\n",
      "240/240 [==============================] - 0s 799us/step - loss: 2.1412 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00007: loss improved from 2.29242 to 2.14124, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 8/2000\n",
      "240/240 [==============================] - 0s 782us/step - loss: 2.0654 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00008: loss improved from 2.14124 to 2.06542, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 9/2000\n",
      "240/240 [==============================] - 0s 809us/step - loss: 2.0222 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00009: loss improved from 2.06542 to 2.02219, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 10/2000\n",
      "240/240 [==============================] - 0s 775us/step - loss: 1.9828 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00010: loss improved from 2.02219 to 1.98279, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 11/2000\n",
      "240/240 [==============================] - 0s 785us/step - loss: 1.9702 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00011: loss improved from 1.98279 to 1.97021, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 12/2000\n",
      "240/240 [==============================] - 0s 782us/step - loss: 1.9335 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00012: loss improved from 1.97021 to 1.93350, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 13/2000\n",
      "240/240 [==============================] - 0s 972us/step - loss: 1.9292 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00013: loss improved from 1.93350 to 1.92925, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 14/2000\n",
      "240/240 [==============================] - 0s 793us/step - loss: 1.9119 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00014: loss improved from 1.92925 to 1.91185, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 15/2000\n",
      "240/240 [==============================] - 0s 757us/step - loss: 1.8996 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00015: loss improved from 1.91185 to 1.89964, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 16/2000\n",
      "240/240 [==============================] - 0s 766us/step - loss: 1.8946 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00016: loss improved from 1.89964 to 1.89458, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 17/2000\n",
      "240/240 [==============================] - 0s 753us/step - loss: 1.8854 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00017: loss improved from 1.89458 to 1.88541, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 18/2000\n",
      "240/240 [==============================] - 0s 756us/step - loss: 1.8742 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00018: loss improved from 1.88541 to 1.87421, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 19/2000\n",
      "240/240 [==============================] - 0s 754us/step - loss: 1.8648 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00019: loss improved from 1.87421 to 1.86484, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 20/2000\n",
      "240/240 [==============================] - 0s 823us/step - loss: 1.8626 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00020: loss improved from 1.86484 to 1.86262, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 21/2000\n",
      "240/240 [==============================] - 0s 879us/step - loss: 1.8500 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00021: loss improved from 1.86262 to 1.84997, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 22/2000\n",
      "240/240 [==============================] - 0s 791us/step - loss: 1.8471 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00022: loss improved from 1.84997 to 1.84713, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 23/2000\n",
      "240/240 [==============================] - 0s 785us/step - loss: 1.8451 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00023: loss improved from 1.84713 to 1.84509, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 24/2000\n",
      "240/240 [==============================] - 0s 792us/step - loss: 1.8388 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00024: loss improved from 1.84509 to 1.83882, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 25/2000\n",
      "240/240 [==============================] - 0s 792us/step - loss: 1.8349 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00025: loss improved from 1.83882 to 1.83495, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 26/2000\n",
      "240/240 [==============================] - 0s 808us/step - loss: 1.8319 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00026: loss improved from 1.83495 to 1.83195, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 27/2000\n",
      "240/240 [==============================] - 0s 829us/step - loss: 1.8270 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00027: loss improved from 1.83195 to 1.82703, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 28/2000\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 1.8249 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00028: loss improved from 1.82703 to 1.82489, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 29/2000\n",
      "240/240 [==============================] - 0s 734us/step - loss: 1.8243 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00029: loss improved from 1.82489 to 1.82431, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 30/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.8182 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00030: loss improved from 1.82431 to 1.81820, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 31/2000\n",
      "240/240 [==============================] - 0s 745us/step - loss: 1.8104 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00031: loss improved from 1.81820 to 1.81044, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 32/2000\n",
      "240/240 [==============================] - 0s 944us/step - loss: 1.8074 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00032: loss improved from 1.81044 to 1.80737, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 33/2000\n",
      "240/240 [==============================] - 0s 778us/step - loss: 1.8048 - accuracy: 0.4000\n",
      "\n",
      "Epoch 00033: loss improved from 1.80737 to 1.80475, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 34/2000\n",
      "240/240 [==============================] - 0s 744us/step - loss: 1.7947 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00034: loss improved from 1.80475 to 1.79471, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 35/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 1.7951 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00035: loss did not improve from 1.79471\n",
      "Epoch 36/2000\n",
      "240/240 [==============================] - 0s 923us/step - loss: 1.7935 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00036: loss improved from 1.79471 to 1.79355, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 37/2000\n",
      "240/240 [==============================] - 0s 790us/step - loss: 1.7895 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00037: loss improved from 1.79355 to 1.78951, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 38/2000\n",
      "240/240 [==============================] - 0s 697us/step - loss: 1.7828 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00038: loss improved from 1.78951 to 1.78280, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 39/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.7803 - accuracy: 0.4000\n",
      "\n",
      "Epoch 00039: loss improved from 1.78280 to 1.78026, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 40/2000\n",
      "240/240 [==============================] - 0s 898us/step - loss: 1.7780 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00040: loss improved from 1.78026 to 1.77800, saving model to ./model/1000001000100010001.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 1.7783 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00041: loss did not improve from 1.77800\n",
      "Epoch 42/2000\n",
      "240/240 [==============================] - 0s 773us/step - loss: 1.7728 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00042: loss improved from 1.77800 to 1.77282, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 43/2000\n",
      "240/240 [==============================] - 0s 756us/step - loss: 1.7719 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00043: loss improved from 1.77282 to 1.77190, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 44/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.7690 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00044: loss improved from 1.77190 to 1.76897, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 45/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.7680 - accuracy: 0.4167TA: 0s - loss: 1.7024 - accuracy: 0.44\n",
      "\n",
      "Epoch 00045: loss improved from 1.76897 to 1.76797, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 46/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.7596 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00046: loss improved from 1.76797 to 1.75963, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 47/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.7635 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00047: loss did not improve from 1.75963\n",
      "Epoch 48/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.7611 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00048: loss did not improve from 1.75963\n",
      "Epoch 49/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.7566 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00049: loss improved from 1.75963 to 1.75661, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 50/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.7573 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00050: loss did not improve from 1.75661\n",
      "Epoch 51/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.7522 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00051: loss improved from 1.75661 to 1.75222, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 52/2000\n",
      "240/240 [==============================] - 0s 848us/step - loss: 1.7493 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00052: loss improved from 1.75222 to 1.74929, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 53/2000\n",
      "240/240 [==============================] - 0s 868us/step - loss: 1.7486 - accuracy: 0.42080s - loss: 1.8792 - accuracy\n",
      "\n",
      "Epoch 00053: loss improved from 1.74929 to 1.74860, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 54/2000\n",
      "240/240 [==============================] - 0s 742us/step - loss: 1.7455 - accuracy: 0.4000\n",
      "\n",
      "Epoch 00054: loss improved from 1.74860 to 1.74554, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 55/2000\n",
      "240/240 [==============================] - 0s 754us/step - loss: 1.7451 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00055: loss improved from 1.74554 to 1.74507, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 56/2000\n",
      "240/240 [==============================] - 0s 896us/step - loss: 1.7479 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00056: loss did not improve from 1.74507\n",
      "Epoch 57/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 1.7401 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00057: loss improved from 1.74507 to 1.74009, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 58/2000\n",
      "240/240 [==============================] - 0s 791us/step - loss: 1.7408 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00058: loss did not improve from 1.74009\n",
      "Epoch 59/2000\n",
      "240/240 [==============================] - 0s 845us/step - loss: 1.7404 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00059: loss did not improve from 1.74009\n",
      "Epoch 60/2000\n",
      "240/240 [==============================] - 0s 739us/step - loss: 1.7396 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00060: loss improved from 1.74009 to 1.73962, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 61/2000\n",
      "240/240 [==============================] - 0s 842us/step - loss: 1.7431 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00061: loss did not improve from 1.73962\n",
      "Epoch 62/2000\n",
      "240/240 [==============================] - 0s 907us/step - loss: 1.7403 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00062: loss did not improve from 1.73962\n",
      "Epoch 63/2000\n",
      "240/240 [==============================] - 0s 822us/step - loss: 1.7351 - accuracy: 0.3833\n",
      "\n",
      "Epoch 00063: loss improved from 1.73962 to 1.73510, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 64/2000\n",
      "240/240 [==============================] - 0s 831us/step - loss: 1.7304 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00064: loss improved from 1.73510 to 1.73044, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 65/2000\n",
      "240/240 [==============================] - 0s 800us/step - loss: 1.7315 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00065: loss did not improve from 1.73044\n",
      "Epoch 66/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.7312 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00066: loss did not improve from 1.73044\n",
      "Epoch 67/2000\n",
      "240/240 [==============================] - 0s 752us/step - loss: 1.7293 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00067: loss improved from 1.73044 to 1.72926, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 68/2000\n",
      "240/240 [==============================] - 0s 756us/step - loss: 1.7304 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00068: loss did not improve from 1.72926\n",
      "Epoch 69/2000\n",
      "240/240 [==============================] - 0s 806us/step - loss: 1.7272 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00069: loss improved from 1.72926 to 1.72715, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 70/2000\n",
      "240/240 [==============================] - 0s 860us/step - loss: 1.7260 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00070: loss improved from 1.72715 to 1.72602, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 71/2000\n",
      "240/240 [==============================] - 0s 741us/step - loss: 1.7401 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00071: loss did not improve from 1.72602\n",
      "Epoch 72/2000\n",
      "240/240 [==============================] - 0s 870us/step - loss: 1.7327 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00072: loss did not improve from 1.72602\n",
      "Epoch 73/2000\n",
      "240/240 [==============================] - 0s 790us/step - loss: 1.7213 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00073: loss improved from 1.72602 to 1.72129, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 74/2000\n",
      "240/240 [==============================] - 0s 785us/step - loss: 1.7206 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00074: loss improved from 1.72129 to 1.72063, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 75/2000\n",
      "240/240 [==============================] - 0s 757us/step - loss: 1.7175 - accuracy: 0.4000\n",
      "\n",
      "Epoch 00075: loss improved from 1.72063 to 1.71746, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 76/2000\n",
      "240/240 [==============================] - 0s 775us/step - loss: 1.7181 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00076: loss did not improve from 1.71746\n",
      "Epoch 77/2000\n",
      "240/240 [==============================] - 0s 772us/step - loss: 1.7175 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00077: loss did not improve from 1.71746\n",
      "Epoch 78/2000\n",
      "240/240 [==============================] - 0s 771us/step - loss: 1.7176 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00078: loss did not improve from 1.71746\n",
      "Epoch 79/2000\n",
      "240/240 [==============================] - 0s 781us/step - loss: 1.7158 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00079: loss improved from 1.71746 to 1.71584, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 80/2000\n",
      "240/240 [==============================] - 0s 797us/step - loss: 1.7161 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00080: loss did not improve from 1.71584\n",
      "Epoch 81/2000\n",
      "240/240 [==============================] - 0s 765us/step - loss: 1.7138 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00081: loss improved from 1.71584 to 1.71384, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 82/2000\n",
      "240/240 [==============================] - 0s 764us/step - loss: 1.7142 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00082: loss did not improve from 1.71384\n",
      "Epoch 83/2000\n",
      "240/240 [==============================] - 0s 755us/step - loss: 1.7147 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00083: loss did not improve from 1.71384\n",
      "Epoch 84/2000\n",
      "240/240 [==============================] - 0s 845us/step - loss: 1.7132 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00084: loss improved from 1.71384 to 1.71316, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 85/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.7111 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00085: loss improved from 1.71316 to 1.71105, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 86/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.7116 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00086: loss did not improve from 1.71105\n",
      "Epoch 87/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.7085 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00087: loss improved from 1.71105 to 1.70847, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 88/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.7100 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00088: loss did not improve from 1.70847\n",
      "Epoch 89/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.7096 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00089: loss did not improve from 1.70847\n",
      "Epoch 90/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 1.7090 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00090: loss did not improve from 1.70847\n",
      "Epoch 91/2000\n",
      "240/240 [==============================] - 0s 667us/step - loss: 1.7067 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00091: loss improved from 1.70847 to 1.70674, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 92/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 1.7072 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00092: loss did not improve from 1.70674\n",
      "Epoch 93/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.7061 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00093: loss improved from 1.70674 to 1.70610, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 94/2000\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 1.7036 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00094: loss improved from 1.70610 to 1.70358, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 95/2000\n",
      "240/240 [==============================] - 0s 873us/step - loss: 1.7045 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00095: loss did not improve from 1.70358\n",
      "Epoch 96/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 1.7161 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00096: loss did not improve from 1.70358\n",
      "Epoch 97/2000\n",
      "240/240 [==============================] - 0s 960us/step - loss: 1.7202 - accuracy: 0.4000\n",
      "\n",
      "Epoch 00097: loss did not improve from 1.70358\n",
      "Epoch 98/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.7882 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00098: loss did not improve from 1.70358\n",
      "Epoch 99/2000\n",
      "240/240 [==============================] - 0s 632us/step - loss: 1.7100 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00099: loss did not improve from 1.70358\n",
      "Epoch 100/2000\n",
      "240/240 [==============================] - 0s 963us/step - loss: 1.7008 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00100: loss improved from 1.70358 to 1.70083, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 101/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 1.6989 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00101: loss improved from 1.70083 to 1.69890, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 102/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.6974 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00102: loss improved from 1.69890 to 1.69743, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 103/2000\n",
      "240/240 [==============================] - 0s 810us/step - loss: 1.6978 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00103: loss did not improve from 1.69743\n",
      "Epoch 104/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.6974 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00104: loss improved from 1.69743 to 1.69739, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 105/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 1.6988 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00105: loss did not improve from 1.69739\n",
      "Epoch 106/2000\n",
      "240/240 [==============================] - 0s 764us/step - loss: 1.6988 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00106: loss did not improve from 1.69739\n",
      "Epoch 107/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.6986 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00107: loss did not improve from 1.69739\n",
      "Epoch 108/2000\n",
      "240/240 [==============================] - 0s 746us/step - loss: 1.6979 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00108: loss did not improve from 1.69739\n",
      "Epoch 109/2000\n",
      "240/240 [==============================] - 0s 773us/step - loss: 1.6963 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00109: loss improved from 1.69739 to 1.69627, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 110/2000\n",
      "240/240 [==============================] - 0s 868us/step - loss: 1.6960 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00110: loss improved from 1.69627 to 1.69601, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 111/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 1.6966 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00111: loss did not improve from 1.69601\n",
      "Epoch 112/2000\n",
      "240/240 [==============================] - 0s 794us/step - loss: 1.6962 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00112: loss did not improve from 1.69601\n",
      "Epoch 113/2000\n",
      "240/240 [==============================] - 0s 836us/step - loss: 1.6959 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00113: loss improved from 1.69601 to 1.69590, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 114/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.6976 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00114: loss did not improve from 1.69590\n",
      "Epoch 115/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.6975 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00115: loss did not improve from 1.69590\n",
      "Epoch 116/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.6967 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00116: loss did not improve from 1.69590\n",
      "Epoch 117/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 1.6950 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00117: loss improved from 1.69590 to 1.69502, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 118/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.6939 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00118: loss improved from 1.69502 to 1.69392, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 119/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.6945 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00119: loss did not improve from 1.69392\n",
      "Epoch 120/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 1.6931 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00120: loss improved from 1.69392 to 1.69313, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 121/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 1.6934 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00121: loss did not improve from 1.69313\n",
      "Epoch 122/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.6927 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00122: loss improved from 1.69313 to 1.69268, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 123/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.6926 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00123: loss improved from 1.69268 to 1.69261, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 124/2000\n",
      "240/240 [==============================] - 0s 741us/step - loss: 1.6929 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00124: loss did not improve from 1.69261\n",
      "Epoch 125/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 1.6913 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00125: loss improved from 1.69261 to 1.69131, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 126/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.6946 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00126: loss did not improve from 1.69131\n",
      "Epoch 127/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.6926 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00127: loss did not improve from 1.69131\n",
      "Epoch 128/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 1.6918 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00128: loss did not improve from 1.69131\n",
      "Epoch 129/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.6908 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00129: loss improved from 1.69131 to 1.69079, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 130/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.6905 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00130: loss improved from 1.69079 to 1.69051, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 131/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 759us/step - loss: 1.6903 - accuracy: 0.3958\n",
      "\n",
      "Epoch 00131: loss improved from 1.69051 to 1.69030, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 132/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6897 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00132: loss improved from 1.69030 to 1.68966, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 133/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6901 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00133: loss did not improve from 1.68966\n",
      "Epoch 134/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6893 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00134: loss improved from 1.68966 to 1.68929, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 135/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.6885 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00135: loss improved from 1.68929 to 1.68851, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 136/2000\n",
      "240/240 [==============================] - 0s 784us/step - loss: 1.6914 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00136: loss did not improve from 1.68851\n",
      "Epoch 137/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.6893 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00137: loss did not improve from 1.68851\n",
      "Epoch 138/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.6945 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00138: loss did not improve from 1.68851\n",
      "Epoch 139/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6899 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00139: loss did not improve from 1.68851\n",
      "Epoch 140/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6877 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00140: loss improved from 1.68851 to 1.68770, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 141/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6884 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00141: loss did not improve from 1.68770\n",
      "Epoch 142/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6870 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00142: loss improved from 1.68770 to 1.68705, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 143/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6861 - accuracy: 0.4000\n",
      "\n",
      "Epoch 00143: loss improved from 1.68705 to 1.68615, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 144/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.6860 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00144: loss improved from 1.68615 to 1.68603, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 145/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 1.6896 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00145: loss did not improve from 1.68603\n",
      "Epoch 146/2000\n",
      "240/240 [==============================] - 0s 726us/step - loss: 1.6877 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00146: loss did not improve from 1.68603\n",
      "Epoch 147/2000\n",
      "240/240 [==============================] - 0s 765us/step - loss: 1.6872 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00147: loss did not improve from 1.68603\n",
      "Epoch 148/2000\n",
      "240/240 [==============================] - 0s 798us/step - loss: 1.6846 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00148: loss improved from 1.68603 to 1.68465, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 149/2000\n",
      "240/240 [==============================] - 0s 803us/step - loss: 1.6844 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00149: loss improved from 1.68465 to 1.68437, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 150/2000\n",
      "240/240 [==============================] - 0s 778us/step - loss: 1.6857 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00150: loss did not improve from 1.68437\n",
      "Epoch 151/2000\n",
      "240/240 [==============================] - 0s 741us/step - loss: 1.6859 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00151: loss did not improve from 1.68437\n",
      "Epoch 152/2000\n",
      "240/240 [==============================] - 0s 627us/step - loss: 1.6852 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00152: loss did not improve from 1.68437\n",
      "Epoch 153/2000\n",
      "240/240 [==============================] - 0s 631us/step - loss: 1.6849 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00153: loss did not improve from 1.68437\n",
      "Epoch 154/2000\n",
      "240/240 [==============================] - 0s 641us/step - loss: 1.6839 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00154: loss improved from 1.68437 to 1.68394, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 155/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.6879 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00155: loss did not improve from 1.68394\n",
      "Epoch 156/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.6867 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00156: loss did not improve from 1.68394\n",
      "Epoch 157/2000\n",
      "240/240 [==============================] - 0s 632us/step - loss: 1.6901 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00157: loss did not improve from 1.68394\n",
      "Epoch 158/2000\n",
      "240/240 [==============================] - 0s 638us/step - loss: 1.6856 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00158: loss did not improve from 1.68394\n",
      "Epoch 159/2000\n",
      "240/240 [==============================] - 0s 625us/step - loss: 1.6835 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00159: loss improved from 1.68394 to 1.68352, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 160/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 1.6829 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00160: loss improved from 1.68352 to 1.68294, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 161/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6826 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00161: loss improved from 1.68294 to 1.68264, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 162/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 1.6853 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00162: loss did not improve from 1.68264\n",
      "Epoch 163/2000\n",
      "240/240 [==============================] - 0s 732us/step - loss: 1.6866 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00163: loss did not improve from 1.68264\n",
      "Epoch 164/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.6843 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00164: loss did not improve from 1.68264\n",
      "Epoch 165/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 1.6823 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00165: loss improved from 1.68264 to 1.68228, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 166/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 1.6918 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00166: loss did not improve from 1.68228\n",
      "Epoch 167/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.6857 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00167: loss did not improve from 1.68228\n",
      "Epoch 168/2000\n",
      "240/240 [==============================] - 0s 762us/step - loss: 1.6821 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00168: loss improved from 1.68228 to 1.68212, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 169/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 1.6802 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00169: loss improved from 1.68212 to 1.68018, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 170/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 1.6812 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00170: loss did not improve from 1.68018\n",
      "Epoch 171/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.6804 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00171: loss did not improve from 1.68018\n",
      "Epoch 172/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 1.6810 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00172: loss did not improve from 1.68018\n",
      "Epoch 173/2000\n",
      "240/240 [==============================] - 0s 726us/step - loss: 1.6809 - accuracy: 0.4250TA: 0s - loss: 1.6066 - accuracy: 0.49\n",
      "\n",
      "Epoch 00173: loss did not improve from 1.68018\n",
      "Epoch 174/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.6804 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00174: loss did not improve from 1.68018\n",
      "Epoch 175/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.6798 - accuracy: 0.4000\n",
      "\n",
      "Epoch 00175: loss improved from 1.68018 to 1.67981, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 176/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 1.6829 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00176: loss did not improve from 1.67981\n",
      "Epoch 177/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.6806 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00177: loss did not improve from 1.67981\n",
      "Epoch 178/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 726us/step - loss: 1.6807 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00178: loss did not improve from 1.67981\n",
      "Epoch 179/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.6925 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00179: loss did not improve from 1.67981\n",
      "Epoch 180/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 1.6824 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00180: loss did not improve from 1.67981\n",
      "Epoch 181/2000\n",
      "240/240 [==============================] - 0s 718us/step - loss: 1.6814 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00181: loss did not improve from 1.67981\n",
      "Epoch 182/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 1.6817 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00182: loss did not improve from 1.67981\n",
      "Epoch 183/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.6844 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00183: loss did not improve from 1.67981\n",
      "Epoch 184/2000\n",
      "240/240 [==============================] - 0s 654us/step - loss: 1.6815 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00184: loss did not improve from 1.67981\n",
      "Epoch 185/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.6785 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00185: loss improved from 1.67981 to 1.67853, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 186/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.6785 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00186: loss improved from 1.67853 to 1.67847, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 187/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 1.6792 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00187: loss did not improve from 1.67847\n",
      "Epoch 188/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.6806 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00188: loss did not improve from 1.67847\n",
      "Epoch 189/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.6783 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00189: loss improved from 1.67847 to 1.67832, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 190/2000\n",
      "240/240 [==============================] - 0s 722us/step - loss: 1.6780 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00190: loss improved from 1.67832 to 1.67801, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 191/2000\n",
      "240/240 [==============================] - 0s 738us/step - loss: 1.6775 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00191: loss improved from 1.67801 to 1.67753, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 192/2000\n",
      "240/240 [==============================] - 0s 721us/step - loss: 1.6784 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00192: loss did not improve from 1.67753\n",
      "Epoch 193/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 1.6779 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00193: loss did not improve from 1.67753\n",
      "Epoch 194/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.6791 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00194: loss did not improve from 1.67753\n",
      "Epoch 195/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.6798 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00195: loss did not improve from 1.67753\n",
      "Epoch 196/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 1.6786 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00196: loss did not improve from 1.67753\n",
      "Epoch 197/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 1.6793 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00197: loss did not improve from 1.67753\n",
      "Epoch 198/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.6783 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00198: loss did not improve from 1.67753\n",
      "Epoch 199/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.6775 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00199: loss improved from 1.67753 to 1.67747, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 200/2000\n",
      "240/240 [==============================] - 0s 722us/step - loss: 1.6798 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00200: loss did not improve from 1.67747\n",
      "Epoch 201/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 1.6875 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00201: loss did not improve from 1.67747\n",
      "Epoch 202/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 1.6788 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00202: loss did not improve from 1.67747\n",
      "Epoch 203/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 1.6768 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00203: loss improved from 1.67747 to 1.67678, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 204/2000\n",
      "240/240 [==============================] - 0s 745us/step - loss: 1.6779 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00204: loss did not improve from 1.67678\n",
      "Epoch 205/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.6771 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00205: loss did not improve from 1.67678\n",
      "Epoch 206/2000\n",
      "240/240 [==============================] - 0s 758us/step - loss: 1.6772 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00206: loss did not improve from 1.67678\n",
      "Epoch 207/2000\n",
      "240/240 [==============================] - 0s 860us/step - loss: 1.6774 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00207: loss did not improve from 1.67678\n",
      "Epoch 208/2000\n",
      "240/240 [==============================] - 0s 770us/step - loss: 1.6760 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00208: loss improved from 1.67678 to 1.67604, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 209/2000\n",
      "240/240 [==============================] - 0s 633us/step - loss: 1.6762 - accuracy: 0.3958\n",
      "\n",
      "Epoch 00209: loss did not improve from 1.67604\n",
      "Epoch 210/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.6758 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00210: loss improved from 1.67604 to 1.67577, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 211/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.6759 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00211: loss did not improve from 1.67577\n",
      "Epoch 212/2000\n",
      "240/240 [==============================] - 0s 721us/step - loss: 1.6757 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00212: loss improved from 1.67577 to 1.67569, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 213/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 1.6801 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00213: loss did not improve from 1.67569\n",
      "Epoch 214/2000\n",
      "240/240 [==============================] - 0s 721us/step - loss: 1.6792 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00214: loss did not improve from 1.67569\n",
      "Epoch 215/2000\n",
      "240/240 [==============================] - 0s 726us/step - loss: 1.6798 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00215: loss did not improve from 1.67569\n",
      "Epoch 216/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 1.6775 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00216: loss did not improve from 1.67569\n",
      "Epoch 217/2000\n",
      "240/240 [==============================] - 0s 762us/step - loss: 1.6752 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00217: loss improved from 1.67569 to 1.67523, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 218/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6761 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00218: loss did not improve from 1.67523\n",
      "Epoch 219/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6745 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00219: loss improved from 1.67523 to 1.67448, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 220/2000\n",
      "240/240 [==============================] - 0s 722us/step - loss: 1.6740 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00220: loss improved from 1.67448 to 1.67400, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 221/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.6743 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00221: loss did not improve from 1.67400\n",
      "Epoch 222/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 1.6746 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00222: loss did not improve from 1.67400\n",
      "Epoch 223/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.6743 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00223: loss did not improve from 1.67400\n",
      "Epoch 224/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.6755 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00224: loss did not improve from 1.67400\n",
      "Epoch 225/2000\n",
      "240/240 [==============================] - 0s 738us/step - loss: 1.6775 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00225: loss did not improve from 1.67400\n",
      "Epoch 226/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.6766 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00226: loss did not improve from 1.67400\n",
      "Epoch 227/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 725us/step - loss: 1.6754 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00227: loss did not improve from 1.67400\n",
      "Epoch 228/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 1.6737 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00228: loss improved from 1.67400 to 1.67366, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 229/2000\n",
      "240/240 [==============================] - 0s 733us/step - loss: 1.6740 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00229: loss did not improve from 1.67366\n",
      "Epoch 230/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 1.6743 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00230: loss did not improve from 1.67366\n",
      "Epoch 231/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 1.6743 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00231: loss did not improve from 1.67366\n",
      "Epoch 232/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 1.6759 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00232: loss did not improve from 1.67366\n",
      "Epoch 233/2000\n",
      "240/240 [==============================] - 0s 726us/step - loss: 1.6745 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00233: loss did not improve from 1.67366\n",
      "Epoch 234/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.6753 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00234: loss did not improve from 1.67366\n",
      "Epoch 235/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.6733 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00235: loss improved from 1.67366 to 1.67327, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 236/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.6730 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00236: loss improved from 1.67327 to 1.67299, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 237/2000\n",
      "240/240 [==============================] - 0s 856us/step - loss: 1.6736 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00237: loss did not improve from 1.67299\n",
      "Epoch 238/2000\n",
      "240/240 [==============================] - 0s 793us/step - loss: 1.6733 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00238: loss did not improve from 1.67299\n",
      "Epoch 239/2000\n",
      "240/240 [==============================] - 0s 809us/step - loss: 1.6729 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00239: loss improved from 1.67299 to 1.67287, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 240/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 1.6735 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00240: loss did not improve from 1.67287\n",
      "Epoch 241/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 1.6743 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00241: loss did not improve from 1.67287\n",
      "Epoch 242/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.6731 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00242: loss did not improve from 1.67287\n",
      "Epoch 243/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.6737 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00243: loss did not improve from 1.67287\n",
      "Epoch 244/2000\n",
      "240/240 [==============================] - 0s 627us/step - loss: 1.6768 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00244: loss did not improve from 1.67287\n",
      "Epoch 245/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 1.6736 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00245: loss did not improve from 1.67287\n",
      "Epoch 246/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.6731 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00246: loss did not improve from 1.67287\n",
      "Epoch 247/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.6929 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00247: loss did not improve from 1.67287\n",
      "Epoch 248/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.9083 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00248: loss did not improve from 1.67287\n",
      "Epoch 249/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.6813 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00249: loss did not improve from 1.67287\n",
      "Epoch 250/2000\n",
      "240/240 [==============================] - 0s 642us/step - loss: 1.6728 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00250: loss improved from 1.67287 to 1.67278, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 251/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 1.6712 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00251: loss improved from 1.67278 to 1.67121, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 252/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.6711 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00252: loss improved from 1.67121 to 1.67105, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 253/2000\n",
      "240/240 [==============================] - 0s 743us/step - loss: 1.6712 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00253: loss did not improve from 1.67105\n",
      "Epoch 254/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.6706 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00254: loss improved from 1.67105 to 1.67061, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 255/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 1.6714 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00255: loss did not improve from 1.67061\n",
      "Epoch 256/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 1.6708 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00256: loss did not improve from 1.67061\n",
      "Epoch 257/2000\n",
      "240/240 [==============================] - 0s 733us/step - loss: 1.6712 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00257: loss did not improve from 1.67061\n",
      "Epoch 258/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.6709 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00258: loss did not improve from 1.67061\n",
      "Epoch 259/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.6706 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00259: loss did not improve from 1.67061\n",
      "Epoch 260/2000\n",
      "240/240 [==============================] - 0s 721us/step - loss: 1.6708 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00260: loss did not improve from 1.67061\n",
      "Epoch 261/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.6708 - accuracy: 0.4000\n",
      "\n",
      "Epoch 00261: loss did not improve from 1.67061\n",
      "Epoch 262/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.6707 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00262: loss did not improve from 1.67061\n",
      "Epoch 263/2000\n",
      "240/240 [==============================] - 0s 741us/step - loss: 1.6811 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00263: loss did not improve from 1.67061\n",
      "Epoch 264/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.6766 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00264: loss did not improve from 1.67061\n",
      "Epoch 265/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 1.6725 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00265: loss did not improve from 1.67061\n",
      "Epoch 266/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.6725 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00266: loss did not improve from 1.67061\n",
      "Epoch 267/2000\n",
      "240/240 [==============================] - 0s 721us/step - loss: 1.6714 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00267: loss did not improve from 1.67061\n",
      "Epoch 268/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.6704 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00268: loss improved from 1.67061 to 1.67040, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 269/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 1.6705 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00269: loss did not improve from 1.67040\n",
      "Epoch 270/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.6700 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00270: loss improved from 1.67040 to 1.67003, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 271/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.6704 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00271: loss did not improve from 1.67003\n",
      "Epoch 272/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.6708 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00272: loss did not improve from 1.67003\n",
      "Epoch 273/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.6733 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00273: loss did not improve from 1.67003\n",
      "Epoch 274/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.6710 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00274: loss did not improve from 1.67003\n",
      "Epoch 275/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 1.6709 - accuracy: 0.3958\n",
      "\n",
      "Epoch 00275: loss did not improve from 1.67003\n",
      "Epoch 276/2000\n",
      "240/240 [==============================] - 0s 732us/step - loss: 1.6708 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00276: loss did not improve from 1.67003\n",
      "Epoch 277/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 724us/step - loss: 1.6698 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00277: loss improved from 1.67003 to 1.66982, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 278/2000\n",
      "240/240 [==============================] - 0s 732us/step - loss: 1.6701 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00278: loss did not improve from 1.66982\n",
      "Epoch 279/2000\n",
      "240/240 [==============================] - 0s 722us/step - loss: 1.6697 - accuracy: 0.4000\n",
      "\n",
      "Epoch 00279: loss improved from 1.66982 to 1.66975, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 280/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 1.6700 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00280: loss did not improve from 1.66975\n",
      "Epoch 281/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.6708 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00281: loss did not improve from 1.66975\n",
      "Epoch 282/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 1.6714 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00282: loss did not improve from 1.66975\n",
      "Epoch 283/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 1.6706 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00283: loss did not improve from 1.66975\n",
      "Epoch 284/2000\n",
      "240/240 [==============================] - 0s 734us/step - loss: 1.6788 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00284: loss did not improve from 1.66975\n",
      "Epoch 285/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.6718 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00285: loss did not improve from 1.66975\n",
      "Epoch 286/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.6697 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00286: loss improved from 1.66975 to 1.66970, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 287/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.6689 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00287: loss improved from 1.66970 to 1.66891, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 288/2000\n",
      "240/240 [==============================] - 0s 741us/step - loss: 1.6689 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00288: loss did not improve from 1.66891\n",
      "Epoch 289/2000\n",
      "240/240 [==============================] - 0s 856us/step - loss: 1.6690 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00289: loss did not improve from 1.66891\n",
      "Epoch 290/2000\n",
      "240/240 [==============================] - 0s 762us/step - loss: 1.6698 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00290: loss did not improve from 1.66891\n",
      "Epoch 291/2000\n",
      "240/240 [==============================] - 0s 652us/step - loss: 1.6693 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00291: loss did not improve from 1.66891\n",
      "Epoch 292/2000\n",
      "240/240 [==============================] - 0s 747us/step - loss: 1.6712 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00292: loss did not improve from 1.66891\n",
      "Epoch 293/2000\n",
      "240/240 [==============================] - 0s 660us/step - loss: 1.6723 - accuracy: 0.3875\n",
      "\n",
      "Epoch 00293: loss did not improve from 1.66891\n",
      "Epoch 294/2000\n",
      "240/240 [==============================] - 0s 641us/step - loss: 1.6712 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00294: loss did not improve from 1.66891\n",
      "Epoch 295/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.6709 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00295: loss did not improve from 1.66891\n",
      "Epoch 296/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.6697 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00296: loss did not improve from 1.66891\n",
      "Epoch 297/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.6745 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00297: loss did not improve from 1.66891\n",
      "Epoch 298/2000\n",
      "240/240 [==============================] - 0s 646us/step - loss: 1.6694 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00298: loss did not improve from 1.66891\n",
      "Epoch 299/2000\n",
      "240/240 [==============================] - 0s 633us/step - loss: 1.6687 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00299: loss improved from 1.66891 to 1.66871, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 300/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6685 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00300: loss improved from 1.66871 to 1.66849, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 301/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6684 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00301: loss improved from 1.66849 to 1.66837, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 302/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.6686 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00302: loss did not improve from 1.66837\n",
      "Epoch 303/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6688 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00303: loss did not improve from 1.66837\n",
      "Epoch 304/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.6705 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00304: loss did not improve from 1.66837\n",
      "Epoch 305/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6723 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00305: loss did not improve from 1.66837\n",
      "Epoch 306/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6724 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00306: loss did not improve from 1.66837\n",
      "Epoch 307/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6707 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00307: loss did not improve from 1.66837\n",
      "Epoch 308/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6688 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00308: loss did not improve from 1.66837\n",
      "Epoch 309/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6683 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00309: loss improved from 1.66837 to 1.66825, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 310/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6683 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00310: loss did not improve from 1.66825\n",
      "Epoch 311/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6687 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00311: loss did not improve from 1.66825\n",
      "Epoch 312/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6686 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00312: loss did not improve from 1.66825\n",
      "Epoch 313/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6684 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00313: loss did not improve from 1.66825\n",
      "Epoch 314/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6680 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00314: loss improved from 1.66825 to 1.66797, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 315/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.6682 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00315: loss did not improve from 1.66797\n",
      "Epoch 316/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6685 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00316: loss did not improve from 1.66797\n",
      "Epoch 317/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6692 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00317: loss did not improve from 1.66797\n",
      "Epoch 318/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6692 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00318: loss did not improve from 1.66797\n",
      "Epoch 319/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6688 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00319: loss did not improve from 1.66797\n",
      "Epoch 320/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6692 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00320: loss did not improve from 1.66797\n",
      "Epoch 321/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6686 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00321: loss did not improve from 1.66797\n",
      "Epoch 322/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.6678 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00322: loss improved from 1.66797 to 1.66780, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 323/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 1.6679 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00323: loss did not improve from 1.66780\n",
      "Epoch 324/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6683 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00324: loss did not improve from 1.66780\n",
      "Epoch 325/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6678 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00325: loss improved from 1.66780 to 1.66776, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 326/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.6678 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00326: loss did not improve from 1.66776\n",
      "Epoch 327/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.6687 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00327: loss did not improve from 1.66776\n",
      "Epoch 328/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.6675 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00328: loss improved from 1.66776 to 1.66751, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 329/2000\n",
      "240/240 [==============================] - 0s 825us/step - loss: 1.6680 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00329: loss did not improve from 1.66751\n",
      "Epoch 330/2000\n",
      "240/240 [==============================] - 0s 814us/step - loss: 1.6683 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00330: loss did not improve from 1.66751\n",
      "Epoch 331/2000\n",
      "240/240 [==============================] - 0s 847us/step - loss: 1.6678 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00331: loss did not improve from 1.66751\n",
      "Epoch 332/2000\n",
      "240/240 [==============================] - 0s 780us/step - loss: 1.6677 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00332: loss did not improve from 1.66751\n",
      "Epoch 333/2000\n",
      "240/240 [==============================] - 0s 656us/step - loss: 1.6682 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00333: loss did not improve from 1.66751\n",
      "Epoch 334/2000\n",
      "240/240 [==============================] - 0s 624us/step - loss: 1.6695 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00334: loss did not improve from 1.66751\n",
      "Epoch 335/2000\n",
      "240/240 [==============================] - 0s 627us/step - loss: 1.6678 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00335: loss did not improve from 1.66751\n",
      "Epoch 336/2000\n",
      "240/240 [==============================] - 0s 628us/step - loss: 1.6685 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00336: loss did not improve from 1.66751\n",
      "Epoch 337/2000\n",
      "240/240 [==============================] - 0s 631us/step - loss: 1.6674 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00337: loss improved from 1.66751 to 1.66739, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 338/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.6680 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00338: loss did not improve from 1.66739\n",
      "Epoch 339/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.6679 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00339: loss did not improve from 1.66739\n",
      "Epoch 340/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.6679 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00340: loss did not improve from 1.66739\n",
      "Epoch 341/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 1.6676 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00341: loss did not improve from 1.66739\n",
      "Epoch 342/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.6673 - accuracy: 0.4000\n",
      "\n",
      "Epoch 00342: loss improved from 1.66739 to 1.66728, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 343/2000\n",
      "240/240 [==============================] - 0s 722us/step - loss: 1.6677 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00343: loss did not improve from 1.66728\n",
      "Epoch 344/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.6676 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00344: loss did not improve from 1.66728\n",
      "Epoch 345/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.6693 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00345: loss did not improve from 1.66728\n",
      "Epoch 346/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.6697 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00346: loss did not improve from 1.66728\n",
      "Epoch 347/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.6715 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00347: loss did not improve from 1.66728\n",
      "Epoch 348/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.6674 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00348: loss did not improve from 1.66728\n",
      "Epoch 349/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.6669 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00349: loss improved from 1.66728 to 1.66693, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 350/2000\n",
      "240/240 [==============================] - 0s 732us/step - loss: 1.6705 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00350: loss did not improve from 1.66693\n",
      "Epoch 351/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.6678 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00351: loss did not improve from 1.66693\n",
      "Epoch 352/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 1.6668 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00352: loss improved from 1.66693 to 1.66678, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 353/2000\n",
      "240/240 [==============================] - 0s 721us/step - loss: 1.6677 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00353: loss did not improve from 1.66678\n",
      "Epoch 354/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 1.6664 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00354: loss improved from 1.66678 to 1.66641, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 355/2000\n",
      "240/240 [==============================] - 0s 726us/step - loss: 1.7178 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00355: loss did not improve from 1.66641\n",
      "Epoch 356/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 1.8948 - accuracy: 0.3750\n",
      "\n",
      "Epoch 00356: loss did not improve from 1.66641\n",
      "Epoch 357/2000\n",
      "240/240 [==============================] - 0s 734us/step - loss: 1.6756 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00357: loss did not improve from 1.66641\n",
      "Epoch 358/2000\n",
      "240/240 [==============================] - 0s 733us/step - loss: 1.6687 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00358: loss did not improve from 1.66641\n",
      "Epoch 359/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 1.6677 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00359: loss did not improve from 1.66641\n",
      "Epoch 360/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.6671 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00360: loss did not improve from 1.66641\n",
      "Epoch 361/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6665 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00361: loss did not improve from 1.66641\n",
      "Epoch 362/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6664 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00362: loss improved from 1.66641 to 1.66638, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 363/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.6661 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00363: loss improved from 1.66638 to 1.66606, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 364/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 1.6662 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00364: loss did not improve from 1.66606\n",
      "Epoch 365/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.6659 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00365: loss improved from 1.66606 to 1.66594, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 366/2000\n",
      "240/240 [==============================] - 0s 721us/step - loss: 1.6660 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00366: loss did not improve from 1.66594\n",
      "Epoch 367/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 1.6659 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00367: loss improved from 1.66594 to 1.66594, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 368/2000\n",
      "240/240 [==============================] - 0s 739us/step - loss: 1.6658 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00368: loss improved from 1.66594 to 1.66584, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 369/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 1.6659 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00369: loss did not improve from 1.66584\n",
      "Epoch 370/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 1.6658 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00370: loss improved from 1.66584 to 1.66576, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 371/2000\n",
      "240/240 [==============================] - 0s 837us/step - loss: 1.6659 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00371: loss did not improve from 1.66576\n",
      "Epoch 372/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 1.6655 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00372: loss improved from 1.66576 to 1.66553, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 373/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.6657 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00373: loss did not improve from 1.66553\n",
      "Epoch 374/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.6654 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00374: loss improved from 1.66553 to 1.66542, saving model to ./model/1000001000100010001.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 375/2000\n",
      "240/240 [==============================] - 0s 889us/step - loss: 1.6655 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00375: loss did not improve from 1.66542\n",
      "Epoch 376/2000\n",
      "240/240 [==============================] - 0s 795us/step - loss: 1.6658 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00376: loss did not improve from 1.66542\n",
      "Epoch 377/2000\n",
      "240/240 [==============================] - 0s 660us/step - loss: 1.6656 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00377: loss did not improve from 1.66542\n",
      "Epoch 378/2000\n",
      "240/240 [==============================] - 0s 648us/step - loss: 1.6656 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00378: loss did not improve from 1.66542\n",
      "Epoch 379/2000\n",
      "240/240 [==============================] - 0s 733us/step - loss: 1.6657 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00379: loss did not improve from 1.66542\n",
      "Epoch 380/2000\n",
      "240/240 [==============================] - 0s 738us/step - loss: 1.6656 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00380: loss did not improve from 1.66542\n",
      "Epoch 381/2000\n",
      "240/240 [==============================] - 0s 747us/step - loss: 1.6661 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00381: loss did not improve from 1.66542\n",
      "Epoch 382/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.6652 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00382: loss improved from 1.66542 to 1.66522, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 383/2000\n",
      "240/240 [==============================] - 0s 747us/step - loss: 1.6663 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00383: loss did not improve from 1.66522\n",
      "Epoch 384/2000\n",
      "240/240 [==============================] - 0s 782us/step - loss: 1.6652 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00384: loss improved from 1.66522 to 1.66517, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 385/2000\n",
      "240/240 [==============================] - 0s 830us/step - loss: 1.6659 - accuracy: 0.4000\n",
      "\n",
      "Epoch 00385: loss did not improve from 1.66517\n",
      "Epoch 386/2000\n",
      "240/240 [==============================] - 0s 767us/step - loss: 1.6658 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00386: loss did not improve from 1.66517\n",
      "Epoch 387/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.6660 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00387: loss did not improve from 1.66517\n",
      "Epoch 388/2000\n",
      "240/240 [==============================] - 0s 787us/step - loss: 1.6655 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00388: loss did not improve from 1.66517\n",
      "Epoch 389/2000\n",
      "240/240 [==============================] - 0s 764us/step - loss: 1.6655 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00389: loss did not improve from 1.66517\n",
      "Epoch 390/2000\n",
      "240/240 [==============================] - 0s 864us/step - loss: 1.6665 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00390: loss did not improve from 1.66517\n",
      "Epoch 391/2000\n",
      "240/240 [==============================] - 0s 638us/step - loss: 1.6663 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00391: loss did not improve from 1.66517\n",
      "Epoch 392/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.6676 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00392: loss did not improve from 1.66517\n",
      "Epoch 393/2000\n",
      "240/240 [==============================] - 0s 637us/step - loss: 1.6670 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00393: loss did not improve from 1.66517\n",
      "Epoch 394/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.6662 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00394: loss did not improve from 1.66517\n",
      "Epoch 395/2000\n",
      "240/240 [==============================] - 0s 633us/step - loss: 1.6660 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00395: loss did not improve from 1.66517\n",
      "Epoch 396/2000\n",
      "240/240 [==============================] - 0s 627us/step - loss: 1.6662 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00396: loss did not improve from 1.66517\n",
      "Epoch 397/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.6666 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00397: loss did not improve from 1.66517\n",
      "Epoch 398/2000\n",
      "240/240 [==============================] - 0s 623us/step - loss: 1.6665 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00398: loss did not improve from 1.66517\n",
      "Epoch 399/2000\n",
      "240/240 [==============================] - 0s 640us/step - loss: 1.6653 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00399: loss did not improve from 1.66517\n",
      "Epoch 400/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.6654 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00400: loss did not improve from 1.66517\n",
      "Epoch 401/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.6652 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00401: loss did not improve from 1.66517\n",
      "Epoch 402/2000\n",
      "240/240 [==============================] - 0s 624us/step - loss: 1.6657 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00402: loss did not improve from 1.66517\n",
      "Epoch 403/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.6654 - accuracy: 0.3958\n",
      "\n",
      "Epoch 00403: loss did not improve from 1.66517\n",
      "Epoch 404/2000\n",
      "240/240 [==============================] - 0s 639us/step - loss: 1.6655 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00404: loss did not improve from 1.66517\n",
      "Epoch 405/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.6655 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00405: loss did not improve from 1.66517\n",
      "Epoch 406/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.6676 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00406: loss did not improve from 1.66517\n",
      "Epoch 407/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.6660 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00407: loss did not improve from 1.66517\n",
      "Epoch 408/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.6658 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00408: loss did not improve from 1.66517\n",
      "Epoch 409/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.6648 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00409: loss improved from 1.66517 to 1.66481, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 410/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6647 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00410: loss improved from 1.66481 to 1.66474, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 411/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.6641 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00411: loss improved from 1.66474 to 1.66410, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 412/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 1.6648 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00412: loss did not improve from 1.66410\n",
      "Epoch 413/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 1.6647 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00413: loss did not improve from 1.66410\n",
      "Epoch 414/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 1.6646 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00414: loss did not improve from 1.66410\n",
      "Epoch 415/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 1.6651 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00415: loss did not improve from 1.66410\n",
      "Epoch 416/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.6821 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00416: loss did not improve from 1.66410\n",
      "Epoch 417/2000\n",
      "240/240 [==============================] - 0s 726us/step - loss: 1.6723 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00417: loss did not improve from 1.66410\n",
      "Epoch 418/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 1.6660 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00418: loss did not improve from 1.66410\n",
      "Epoch 419/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.6653 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00419: loss did not improve from 1.66410\n",
      "Epoch 420/2000\n",
      "240/240 [==============================] - 0s 799us/step - loss: 1.6647 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00420: loss did not improve from 1.66410\n",
      "Epoch 421/2000\n",
      "240/240 [==============================] - 0s 826us/step - loss: 1.6642 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00421: loss did not improve from 1.66410\n",
      "Epoch 422/2000\n",
      "240/240 [==============================] - 0s 827us/step - loss: 1.6646 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00422: loss did not improve from 1.66410\n",
      "Epoch 423/2000\n",
      "240/240 [==============================] - 0s 822us/step - loss: 1.6672 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00423: loss did not improve from 1.66410\n",
      "Epoch 424/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 1.6651 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00424: loss did not improve from 1.66410\n",
      "Epoch 425/2000\n",
      "240/240 [==============================] - 0s 623us/step - loss: 1.6646 - accuracy: 0.4000\n",
      "\n",
      "Epoch 00425: loss did not improve from 1.66410\n",
      "Epoch 426/2000\n",
      "240/240 [==============================] - 0s 625us/step - loss: 1.6647 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00426: loss did not improve from 1.66410\n",
      "Epoch 427/2000\n",
      "240/240 [==============================] - 0s 623us/step - loss: 1.6646 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00427: loss did not improve from 1.66410\n",
      "Epoch 428/2000\n",
      "240/240 [==============================] - 0s 620us/step - loss: 1.6642 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00428: loss did not improve from 1.66410\n",
      "Epoch 429/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 1.6644 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00429: loss did not improve from 1.66410\n",
      "Epoch 430/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.6646 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00430: loss did not improve from 1.66410\n",
      "Epoch 431/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.6653 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00431: loss did not improve from 1.66410\n",
      "Epoch 432/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.6648 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00432: loss did not improve from 1.66410\n",
      "Epoch 433/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.6646 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00433: loss did not improve from 1.66410\n",
      "Epoch 434/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.6645 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00434: loss did not improve from 1.66410\n",
      "Epoch 435/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.6643 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00435: loss did not improve from 1.66410\n",
      "Epoch 436/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.6644 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00436: loss did not improve from 1.66410\n",
      "Epoch 437/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 1.6644 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00437: loss did not improve from 1.66410\n",
      "Epoch 438/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 1.6645 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00438: loss did not improve from 1.66410\n",
      "Epoch 439/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 1.6644 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00439: loss did not improve from 1.66410\n",
      "Epoch 440/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 1.6643 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00440: loss did not improve from 1.66410\n",
      "Epoch 441/2000\n",
      "240/240 [==============================] - 0s 645us/step - loss: 1.6640 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00441: loss improved from 1.66410 to 1.66400, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 442/2000\n",
      "240/240 [==============================] - 0s 631us/step - loss: 1.6645 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00442: loss did not improve from 1.66400\n",
      "Epoch 443/2000\n",
      "240/240 [==============================] - 0s 772us/step - loss: 1.6646 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00443: loss did not improve from 1.66400\n",
      "Epoch 444/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6642 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00444: loss did not improve from 1.66400\n",
      "Epoch 445/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 1.6640 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00445: loss did not improve from 1.66400\n",
      "Epoch 446/2000\n",
      "240/240 [==============================] - 0s 767us/step - loss: 1.6642 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00446: loss did not improve from 1.66400\n",
      "Epoch 447/2000\n",
      "240/240 [==============================] - 0s 788us/step - loss: 1.6683 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00447: loss did not improve from 1.66400\n",
      "Epoch 448/2000\n",
      "240/240 [==============================] - 0s 747us/step - loss: 1.6710 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00448: loss did not improve from 1.66400\n",
      "Epoch 449/2000\n",
      "240/240 [==============================] - 0s 2ms/step - loss: 1.6678 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00449: loss did not improve from 1.66400\n",
      "Epoch 450/2000\n",
      "240/240 [==============================] - 0s 866us/step - loss: 1.6649 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00450: loss did not improve from 1.66400\n",
      "Epoch 451/2000\n",
      "240/240 [==============================] - 0s 783us/step - loss: 1.6637 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00451: loss improved from 1.66400 to 1.66369, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 452/2000\n",
      "240/240 [==============================] - 0s 792us/step - loss: 1.6645 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00452: loss did not improve from 1.66369\n",
      "Epoch 453/2000\n",
      "240/240 [==============================] - 0s 840us/step - loss: 1.6639 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00453: loss did not improve from 1.66369\n",
      "Epoch 454/2000\n",
      "240/240 [==============================] - 0s 801us/step - loss: 1.6653 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00454: loss did not improve from 1.66369\n",
      "Epoch 455/2000\n",
      "240/240 [==============================] - 0s 751us/step - loss: 1.6637 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00455: loss improved from 1.66369 to 1.66369, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 456/2000\n",
      "240/240 [==============================] - 0s 651us/step - loss: 1.6637 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00456: loss improved from 1.66369 to 1.66367, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 457/2000\n",
      "240/240 [==============================] - 0s 749us/step - loss: 1.6638 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00457: loss did not improve from 1.66367\n",
      "Epoch 458/2000\n",
      "240/240 [==============================] - 0s 752us/step - loss: 1.6649 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00458: loss did not improve from 1.66367\n",
      "Epoch 459/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.6647 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00459: loss did not improve from 1.66367\n",
      "Epoch 460/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 1.6642 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00460: loss did not improve from 1.66367\n",
      "Epoch 461/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6644 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00461: loss did not improve from 1.66367\n",
      "Epoch 462/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 1.6651 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00462: loss did not improve from 1.66367\n",
      "Epoch 463/2000\n",
      "240/240 [==============================] - 0s 664us/step - loss: 1.6645 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00463: loss did not improve from 1.66367\n",
      "Epoch 464/2000\n",
      "240/240 [==============================] - 0s 633us/step - loss: 1.6638 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00464: loss did not improve from 1.66367\n",
      "Epoch 465/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.6637 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00465: loss did not improve from 1.66367\n",
      "Epoch 466/2000\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 1.6636 - accuracy: 0.4000\n",
      "\n",
      "Epoch 00466: loss improved from 1.66367 to 1.66364, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 467/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 1.6637 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00467: loss did not improve from 1.66364\n",
      "Epoch 468/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.6641 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00468: loss did not improve from 1.66364\n",
      "Epoch 469/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.6646 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00469: loss did not improve from 1.66364\n",
      "Epoch 470/2000\n",
      "240/240 [==============================] - 0s 732us/step - loss: 1.6711 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00470: loss did not improve from 1.66364\n",
      "Epoch 471/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 1.6649 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00471: loss did not improve from 1.66364\n",
      "Epoch 472/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 1.6644 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00472: loss did not improve from 1.66364\n",
      "Epoch 473/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6656 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00473: loss did not improve from 1.66364\n",
      "Epoch 474/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6640 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00474: loss did not improve from 1.66364\n",
      "Epoch 475/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 1.6637 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00475: loss did not improve from 1.66364\n",
      "Epoch 476/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.6632 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00476: loss improved from 1.66364 to 1.66324, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 477/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 736us/step - loss: 1.6636 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00477: loss did not improve from 1.66324\n",
      "Epoch 478/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 1.6633 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00478: loss did not improve from 1.66324\n",
      "Epoch 479/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.6961 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00479: loss did not improve from 1.66324\n",
      "Epoch 480/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.7570 - accuracy: 0.4000\n",
      "\n",
      "Epoch 00480: loss did not improve from 1.66324\n",
      "Epoch 481/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.6693 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00481: loss did not improve from 1.66324\n",
      "Epoch 482/2000\n",
      "240/240 [==============================] - 0s 781us/step - loss: 1.6641 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00482: loss did not improve from 1.66324\n",
      "Epoch 483/2000\n",
      "240/240 [==============================] - 0s 762us/step - loss: 1.6634 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00483: loss did not improve from 1.66324\n",
      "Epoch 484/2000\n",
      "240/240 [==============================] - 0s 742us/step - loss: 1.6634 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00484: loss did not improve from 1.66324\n",
      "Epoch 485/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 1.6630 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00485: loss improved from 1.66324 to 1.66301, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 486/2000\n",
      "240/240 [==============================] - 0s 808us/step - loss: 1.6631 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00486: loss did not improve from 1.66301\n",
      "Epoch 487/2000\n",
      "240/240 [==============================] - 0s 832us/step - loss: 1.6631 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00487: loss did not improve from 1.66301\n",
      "Epoch 488/2000\n",
      "240/240 [==============================] - 0s 777us/step - loss: 1.6628 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00488: loss improved from 1.66301 to 1.66282, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 489/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.6633 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00489: loss did not improve from 1.66282\n",
      "Epoch 490/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 1.6638 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00490: loss did not improve from 1.66282\n",
      "Epoch 491/2000\n",
      "240/240 [==============================] - 0s 812us/step - loss: 1.6643 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00491: loss did not improve from 1.66282\n",
      "Epoch 492/2000\n",
      "240/240 [==============================] - 0s 758us/step - loss: 1.6633 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00492: loss did not improve from 1.66282\n",
      "Epoch 493/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.6631 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00493: loss did not improve from 1.66282\n",
      "Epoch 494/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 1.6631 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00494: loss did not improve from 1.66282\n",
      "Epoch 495/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.6630 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00495: loss did not improve from 1.66282\n",
      "Epoch 496/2000\n",
      "240/240 [==============================] - 0s 789us/step - loss: 1.6634 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00496: loss did not improve from 1.66282\n",
      "Epoch 497/2000\n",
      "240/240 [==============================] - 0s 779us/step - loss: 1.6631 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00497: loss did not improve from 1.66282\n",
      "Epoch 498/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 1.6644 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00498: loss did not improve from 1.66282\n",
      "Epoch 499/2000\n",
      "240/240 [==============================] - 0s 652us/step - loss: 1.6636 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00499: loss did not improve from 1.66282\n",
      "Epoch 500/2000\n",
      "240/240 [==============================] - 0s 646us/step - loss: 1.6631 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00500: loss did not improve from 1.66282\n",
      "Epoch 501/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 1.6632 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00501: loss did not improve from 1.66282\n",
      "Epoch 502/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.6632 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00502: loss did not improve from 1.66282\n",
      "Epoch 503/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 1.6632 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00503: loss did not improve from 1.66282\n",
      "Epoch 504/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 1.6633 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00504: loss did not improve from 1.66282\n",
      "Epoch 505/2000\n",
      "240/240 [==============================] - 0s 661us/step - loss: 1.6655 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00505: loss did not improve from 1.66282\n",
      "Epoch 506/2000\n",
      "240/240 [==============================] - 0s 873us/step - loss: 1.6635 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00506: loss did not improve from 1.66282\n",
      "Epoch 507/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.6639 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00507: loss did not improve from 1.66282\n",
      "Epoch 508/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6642 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00508: loss did not improve from 1.66282\n",
      "Epoch 509/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 1.6633 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00509: loss did not improve from 1.66282\n",
      "Epoch 510/2000\n",
      "240/240 [==============================] - 0s 640us/step - loss: 1.6633 - accuracy: 0.4000\n",
      "\n",
      "Epoch 00510: loss did not improve from 1.66282\n",
      "Epoch 511/2000\n",
      "240/240 [==============================] - 0s 830us/step - loss: 1.6631 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00511: loss did not improve from 1.66282\n",
      "Epoch 512/2000\n",
      "240/240 [==============================] - 0s 806us/step - loss: 1.6631 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00512: loss did not improve from 1.66282\n",
      "Epoch 513/2000\n",
      "240/240 [==============================] - 0s 848us/step - loss: 1.6629 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00513: loss did not improve from 1.66282\n",
      "Epoch 514/2000\n",
      "240/240 [==============================] - 0s 781us/step - loss: 1.6648 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00514: loss did not improve from 1.66282\n",
      "Epoch 515/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 1.6649 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00515: loss did not improve from 1.66282\n",
      "Epoch 516/2000\n",
      "240/240 [==============================] - 0s 654us/step - loss: 1.6641 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00516: loss did not improve from 1.66282\n",
      "Epoch 517/2000\n",
      "240/240 [==============================] - 0s 594us/step - loss: 1.6639 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00517: loss did not improve from 1.66282\n",
      "Epoch 518/2000\n",
      "240/240 [==============================] - 0s 808us/step - loss: 1.6633 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00518: loss did not improve from 1.66282\n",
      "Epoch 519/2000\n",
      "240/240 [==============================] - 0s 667us/step - loss: 1.6630 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00519: loss did not improve from 1.66282\n",
      "Epoch 520/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.6632 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00520: loss did not improve from 1.66282\n",
      "Epoch 521/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.6631 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00521: loss did not improve from 1.66282\n",
      "Epoch 522/2000\n",
      "240/240 [==============================] - 0s 653us/step - loss: 1.6634 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00522: loss did not improve from 1.66282\n",
      "Epoch 523/2000\n",
      "240/240 [==============================] - 0s 642us/step - loss: 1.6629 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00523: loss did not improve from 1.66282\n",
      "Epoch 524/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.6627 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00524: loss improved from 1.66282 to 1.66269, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 525/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.6628 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00525: loss did not improve from 1.66269\n",
      "Epoch 526/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 1.6634 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00526: loss did not improve from 1.66269\n",
      "Epoch 527/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 1.6637 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00527: loss did not improve from 1.66269\n",
      "Epoch 528/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 1.6630 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00528: loss did not improve from 1.66269\n",
      "Epoch 529/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6638 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00529: loss did not improve from 1.66269\n",
      "Epoch 530/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.6632 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00530: loss did not improve from 1.66269\n",
      "Epoch 531/2000\n",
      "240/240 [==============================] - ETA: 0s - loss: 1.5853 - accuracy: 0.45 - 0s 715us/step - loss: 1.6644 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00531: loss did not improve from 1.66269\n",
      "Epoch 532/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6641 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00532: loss did not improve from 1.66269\n",
      "Epoch 533/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.6634 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00533: loss did not improve from 1.66269\n",
      "Epoch 534/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6628 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00534: loss did not improve from 1.66269\n",
      "Epoch 535/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.6646 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00535: loss did not improve from 1.66269\n",
      "Epoch 536/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6630 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00536: loss did not improve from 1.66269\n",
      "Epoch 537/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.6644 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00537: loss did not improve from 1.66269\n",
      "Epoch 538/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6631 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00538: loss did not improve from 1.66269\n",
      "Epoch 539/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6628 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00539: loss did not improve from 1.66269\n",
      "Epoch 540/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6627 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00540: loss improved from 1.66269 to 1.66267, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 541/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 1.6627 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00541: loss did not improve from 1.66267\n",
      "Epoch 542/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 1.6625 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00542: loss improved from 1.66267 to 1.66247, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 543/2000\n",
      "240/240 [==============================] - 0s 732us/step - loss: 1.6634 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00543: loss did not improve from 1.66247\n",
      "Epoch 544/2000\n",
      "240/240 [==============================] - 0s 722us/step - loss: 1.6657 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00544: loss did not improve from 1.66247\n",
      "Epoch 545/2000\n",
      "240/240 [==============================] - 0s 722us/step - loss: 1.6630 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00545: loss did not improve from 1.66247\n",
      "Epoch 546/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 1.6625 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00546: loss did not improve from 1.66247\n",
      "Epoch 547/2000\n",
      "240/240 [==============================] - 0s 769us/step - loss: 1.6626 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00547: loss did not improve from 1.66247\n",
      "Epoch 548/2000\n",
      "240/240 [==============================] - 0s 739us/step - loss: 1.6625 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00548: loss did not improve from 1.66247\n",
      "Epoch 549/2000\n",
      "240/240 [==============================] - 0s 740us/step - loss: 1.6628 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00549: loss did not improve from 1.66247\n",
      "Epoch 550/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 1.6623 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00550: loss improved from 1.66247 to 1.66230, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 551/2000\n",
      "240/240 [==============================] - 0s 749us/step - loss: 1.6626 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00551: loss did not improve from 1.66230\n",
      "Epoch 552/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.6675 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00552: loss did not improve from 1.66230\n",
      "Epoch 553/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 1.6659 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00553: loss did not improve from 1.66230\n",
      "Epoch 554/2000\n",
      "240/240 [==============================] - 0s 749us/step - loss: 1.6643 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00554: loss did not improve from 1.66230\n",
      "Epoch 555/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.6627 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00555: loss did not improve from 1.66230\n",
      "Epoch 556/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6626 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00556: loss did not improve from 1.66230\n",
      "Epoch 557/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6627 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00557: loss did not improve from 1.66230\n",
      "Epoch 558/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6625 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00558: loss did not improve from 1.66230\n",
      "Epoch 559/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.6626 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00559: loss did not improve from 1.66230\n",
      "Epoch 560/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6638 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00560: loss did not improve from 1.66230\n",
      "Epoch 561/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6628 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00561: loss did not improve from 1.66230\n",
      "Epoch 562/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 1.6623 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00562: loss did not improve from 1.66230\n",
      "Epoch 563/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6623 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00563: loss improved from 1.66230 to 1.66230, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 564/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6626 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00564: loss did not improve from 1.66230\n",
      "Epoch 565/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6630 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00565: loss did not improve from 1.66230\n",
      "Epoch 566/2000\n",
      "240/240 [==============================] - 0s 697us/step - loss: 1.6636 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00566: loss did not improve from 1.66230\n",
      "Epoch 567/2000\n",
      "240/240 [==============================] - ETA: 0s - loss: 1.6793 - accuracy: 0.44 - 0s 698us/step - loss: 1.6665 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00567: loss did not improve from 1.66230\n",
      "Epoch 568/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 1.6738 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00568: loss did not improve from 1.66230\n",
      "Epoch 569/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 1.6655 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00569: loss did not improve from 1.66230\n",
      "Epoch 570/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.6628 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00570: loss did not improve from 1.66230\n",
      "Epoch 571/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 1.6621 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00571: loss improved from 1.66230 to 1.66213, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 572/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 1.6626 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00572: loss did not improve from 1.66213\n",
      "Epoch 573/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.6623 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00573: loss did not improve from 1.66213\n",
      "Epoch 574/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6623 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00574: loss did not improve from 1.66213\n",
      "Epoch 575/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6627 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00575: loss did not improve from 1.66213\n",
      "Epoch 576/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6631 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00576: loss did not improve from 1.66213\n",
      "Epoch 577/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6625 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00577: loss did not improve from 1.66213\n",
      "Epoch 578/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6629 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00578: loss did not improve from 1.66213\n",
      "Epoch 579/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.6655 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00579: loss did not improve from 1.66213\n",
      "Epoch 580/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 699us/step - loss: 1.6629 - accuracy: 0.3917\n",
      "\n",
      "Epoch 00580: loss did not improve from 1.66213\n",
      "Epoch 581/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 1.6627 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00581: loss did not improve from 1.66213\n",
      "Epoch 582/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.6626 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00582: loss did not improve from 1.66213\n",
      "Epoch 583/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6627 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00583: loss did not improve from 1.66213\n",
      "Epoch 584/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.6637 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00584: loss did not improve from 1.66213\n",
      "Epoch 585/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 1.6639 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00585: loss did not improve from 1.66213\n",
      "Epoch 586/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.6626 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00586: loss did not improve from 1.66213\n",
      "Epoch 587/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.6623 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00587: loss did not improve from 1.66213\n",
      "Epoch 588/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 1.6624 - accuracy: 0.4000\n",
      "\n",
      "Epoch 00588: loss did not improve from 1.66213\n",
      "Epoch 589/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.6662 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00589: loss did not improve from 1.66213\n",
      "Epoch 590/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.6646 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00590: loss did not improve from 1.66213\n",
      "Epoch 591/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 1.6634 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00591: loss did not improve from 1.66213\n",
      "Epoch 592/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.6645 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00592: loss did not improve from 1.66213\n",
      "Epoch 593/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6632 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00593: loss did not improve from 1.66213\n",
      "Epoch 594/2000\n",
      "240/240 [==============================] - 0s 809us/step - loss: 2.2168 - accuracy: 0.3875\n",
      "\n",
      "Epoch 00594: loss did not improve from 1.66213\n",
      "Epoch 595/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.7042 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00595: loss did not improve from 1.66213\n",
      "Epoch 596/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6668 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00596: loss did not improve from 1.66213\n",
      "Epoch 597/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.6637 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00597: loss did not improve from 1.66213\n",
      "Epoch 598/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 1.6634 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00598: loss did not improve from 1.66213\n",
      "Epoch 599/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6628 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00599: loss did not improve from 1.66213\n",
      "Epoch 600/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6623 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00600: loss did not improve from 1.66213\n",
      "Epoch 601/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6622 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00601: loss did not improve from 1.66213\n",
      "Epoch 602/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6620 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00602: loss improved from 1.66213 to 1.66195, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 603/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.6619 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00603: loss improved from 1.66195 to 1.66187, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 604/2000\n",
      "240/240 [==============================] - 0s 798us/step - loss: 1.6619 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00604: loss did not improve from 1.66187\n",
      "Epoch 605/2000\n",
      "240/240 [==============================] - 0s 805us/step - loss: 1.6618 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00605: loss improved from 1.66187 to 1.66184, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 606/2000\n",
      "240/240 [==============================] - 0s 811us/step - loss: 1.6620 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00606: loss did not improve from 1.66184\n",
      "Epoch 607/2000\n",
      "240/240 [==============================] - 0s 770us/step - loss: 1.6619 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00607: loss did not improve from 1.66184\n",
      "Epoch 608/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.6619 - accuracy: 0.3917\n",
      "\n",
      "Epoch 00608: loss did not improve from 1.66184\n",
      "Epoch 609/2000\n",
      "240/240 [==============================] - 0s 627us/step - loss: 1.6619 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00609: loss did not improve from 1.66184\n",
      "Epoch 610/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.6617 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00610: loss improved from 1.66184 to 1.66172, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 611/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.6619 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00611: loss did not improve from 1.66172\n",
      "Epoch 612/2000\n",
      "240/240 [==============================] - 0s 737us/step - loss: 1.6617 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00612: loss did not improve from 1.66172\n",
      "Epoch 613/2000\n",
      "240/240 [==============================] - 0s 734us/step - loss: 1.6620 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00613: loss did not improve from 1.66172\n",
      "Epoch 614/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.6622 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00614: loss did not improve from 1.66172\n",
      "Epoch 615/2000\n",
      "240/240 [==============================] - 0s 733us/step - loss: 1.6625 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00615: loss did not improve from 1.66172\n",
      "Epoch 616/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.6624 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00616: loss did not improve from 1.66172\n",
      "Epoch 617/2000\n",
      "240/240 [==============================] - 0s 737us/step - loss: 1.6624 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00617: loss did not improve from 1.66172\n",
      "Epoch 618/2000\n",
      "240/240 [==============================] - 0s 726us/step - loss: 1.6619 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00618: loss did not improve from 1.66172\n",
      "Epoch 619/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.6620 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00619: loss did not improve from 1.66172\n",
      "Epoch 620/2000\n",
      "240/240 [==============================] - 0s 726us/step - loss: 1.6617 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00620: loss improved from 1.66172 to 1.66172, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 621/2000\n",
      "240/240 [==============================] - 0s 732us/step - loss: 1.6618 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00621: loss did not improve from 1.66172\n",
      "Epoch 622/2000\n",
      "240/240 [==============================] - 0s 741us/step - loss: 1.6620 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00622: loss did not improve from 1.66172\n",
      "Epoch 623/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.6619 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00623: loss did not improve from 1.66172\n",
      "Epoch 624/2000\n",
      "240/240 [==============================] - 0s 732us/step - loss: 1.6617 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00624: loss improved from 1.66172 to 1.66170, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 625/2000\n",
      "240/240 [==============================] - 0s 740us/step - loss: 1.6686 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00625: loss did not improve from 1.66170\n",
      "Epoch 626/2000\n",
      "240/240 [==============================] - 0s 752us/step - loss: 1.6664 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00626: loss did not improve from 1.66170\n",
      "Epoch 627/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 1.6636 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00627: loss did not improve from 1.66170\n",
      "Epoch 628/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.6654 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00628: loss did not improve from 1.66170\n",
      "Epoch 629/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.6624 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00629: loss did not improve from 1.66170\n",
      "Epoch 630/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.6619 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00630: loss did not improve from 1.66170\n",
      "Epoch 631/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 1.6621 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00631: loss did not improve from 1.66170\n",
      "Epoch 632/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.6619 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00632: loss did not improve from 1.66170\n",
      "Epoch 633/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 1.6618 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00633: loss did not improve from 1.66170\n",
      "Epoch 634/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 1.6620 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00634: loss did not improve from 1.66170\n",
      "Epoch 635/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.6620 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00635: loss did not improve from 1.66170\n",
      "Epoch 636/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.6618 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00636: loss did not improve from 1.66170\n",
      "Epoch 637/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.6617 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00637: loss did not improve from 1.66170\n",
      "Epoch 638/2000\n",
      "240/240 [==============================] - 0s 726us/step - loss: 1.6617 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00638: loss did not improve from 1.66170\n",
      "Epoch 639/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 1.6617 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00639: loss did not improve from 1.66170\n",
      "Epoch 640/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.6617 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00640: loss improved from 1.66170 to 1.66167, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 641/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.6616 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00641: loss improved from 1.66167 to 1.66159, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 642/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 1.6621 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00642: loss did not improve from 1.66159\n",
      "Epoch 643/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 1.6621 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00643: loss did not improve from 1.66159\n",
      "Epoch 644/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 1.6623 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00644: loss did not improve from 1.66159\n",
      "Epoch 645/2000\n",
      "240/240 [==============================] - 0s 738us/step - loss: 1.6636 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00645: loss did not improve from 1.66159\n",
      "Epoch 646/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 1.6657 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00646: loss did not improve from 1.66159\n",
      "Epoch 647/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.6632 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00647: loss did not improve from 1.66159\n",
      "Epoch 648/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 1.6627 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00648: loss did not improve from 1.66159\n",
      "Epoch 649/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 1.6619 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00649: loss did not improve from 1.66159\n",
      "Epoch 650/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 1.6623 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00650: loss did not improve from 1.66159\n",
      "Epoch 651/2000\n",
      "240/240 [==============================] - 0s 632us/step - loss: 1.6622 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00651: loss did not improve from 1.66159\n",
      "Epoch 652/2000\n",
      "240/240 [==============================] - 0s 752us/step - loss: 1.6629 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00652: loss did not improve from 1.66159\n",
      "Epoch 653/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.6619 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00653: loss did not improve from 1.66159\n",
      "Epoch 654/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.6619 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00654: loss did not improve from 1.66159\n",
      "Epoch 655/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 1.6621 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00655: loss did not improve from 1.66159\n",
      "Epoch 656/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6619 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00656: loss did not improve from 1.66159\n",
      "Epoch 657/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.6618 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00657: loss did not improve from 1.66159\n",
      "Epoch 658/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6617 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00658: loss did not improve from 1.66159\n",
      "Epoch 659/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6621 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00659: loss did not improve from 1.66159\n",
      "Epoch 660/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6621 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00660: loss did not improve from 1.66159\n",
      "Epoch 661/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.6616 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00661: loss improved from 1.66159 to 1.66157, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 662/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 1.6616 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00662: loss improved from 1.66157 to 1.66156, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 663/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 1.6618 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00663: loss did not improve from 1.66156\n",
      "Epoch 664/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 1.6618 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00664: loss did not improve from 1.66156\n",
      "Epoch 665/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.6618 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00665: loss did not improve from 1.66156\n",
      "Epoch 666/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6663 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00666: loss did not improve from 1.66156\n",
      "Epoch 667/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6651 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00667: loss did not improve from 1.66156\n",
      "Epoch 668/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6622 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00668: loss did not improve from 1.66156\n",
      "Epoch 669/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6622 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00669: loss did not improve from 1.66156\n",
      "Epoch 670/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6618 - accuracy: 0.4000\n",
      "\n",
      "Epoch 00670: loss did not improve from 1.66156\n",
      "Epoch 671/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.6619 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00671: loss did not improve from 1.66156\n",
      "Epoch 672/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.6620 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00672: loss did not improve from 1.66156\n",
      "Epoch 673/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.6617 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00673: loss did not improve from 1.66156\n",
      "Epoch 674/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6616 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00674: loss did not improve from 1.66156\n",
      "Epoch 675/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 1.6616 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00675: loss did not improve from 1.66156\n",
      "Epoch 676/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6637 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00676: loss did not improve from 1.66156\n",
      "Epoch 677/2000\n",
      "240/240 [==============================] - 0s 659us/step - loss: 1.6670 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00677: loss did not improve from 1.66156\n",
      "Epoch 678/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.6635 - accuracy: 0.3958\n",
      "\n",
      "Epoch 00678: loss did not improve from 1.66156\n",
      "Epoch 679/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6643 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00679: loss did not improve from 1.66156\n",
      "Epoch 680/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6630 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00680: loss did not improve from 1.66156\n",
      "Epoch 681/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6619 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00681: loss did not improve from 1.66156\n",
      "Epoch 682/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6616 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00682: loss improved from 1.66156 to 1.66156, saving model to ./model/1000001000100010001.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 683/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.6614 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00683: loss improved from 1.66156 to 1.66142, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 684/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.6614 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00684: loss improved from 1.66142 to 1.66136, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 685/2000\n",
      "240/240 [==============================] - 0s 744us/step - loss: 1.6615 - accuracy: 0.3958\n",
      "\n",
      "Epoch 00685: loss did not improve from 1.66136\n",
      "Epoch 686/2000\n",
      "240/240 [==============================] - 0s 721us/step - loss: 1.6613 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00686: loss improved from 1.66136 to 1.66129, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 687/2000\n",
      "240/240 [==============================] - 0s 740us/step - loss: 1.6614 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00687: loss did not improve from 1.66129\n",
      "Epoch 688/2000\n",
      "240/240 [==============================] - 0s 734us/step - loss: 1.6615 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00688: loss did not improve from 1.66129\n",
      "Epoch 689/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.6618 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00689: loss did not improve from 1.66129\n",
      "Epoch 690/2000\n",
      "240/240 [==============================] - 0s 744us/step - loss: 1.6617 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00690: loss did not improve from 1.66129\n",
      "Epoch 691/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 1.6618 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00691: loss did not improve from 1.66129\n",
      "Epoch 692/2000\n",
      "240/240 [==============================] - 0s 732us/step - loss: 1.6616 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00692: loss did not improve from 1.66129\n",
      "Epoch 693/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 1.6618 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00693: loss did not improve from 1.66129\n",
      "Epoch 694/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 1.6617 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00694: loss did not improve from 1.66129\n",
      "Epoch 695/2000\n",
      "240/240 [==============================] - 0s 737us/step - loss: 1.6617 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00695: loss did not improve from 1.66129\n",
      "Epoch 696/2000\n",
      "240/240 [==============================] - 0s 831us/step - loss: 1.6622 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00696: loss did not improve from 1.66129\n",
      "Epoch 697/2000\n",
      "240/240 [==============================] - 0s 820us/step - loss: 1.6618 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00697: loss did not improve from 1.66129\n",
      "Epoch 698/2000\n",
      "240/240 [==============================] - 0s 811us/step - loss: 1.6629 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00698: loss did not improve from 1.66129\n",
      "Epoch 699/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.6712 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00699: loss did not improve from 1.66129\n",
      "Epoch 700/2000\n",
      "240/240 [==============================] - 0s 632us/step - loss: 1.6628 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00700: loss did not improve from 1.66129\n",
      "Epoch 701/2000\n",
      "240/240 [==============================] - 0s 662us/step - loss: 1.6620 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00701: loss did not improve from 1.66129\n",
      "Epoch 702/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.6615 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00702: loss did not improve from 1.66129\n",
      "Epoch 703/2000\n",
      "240/240 [==============================] - 0s 633us/step - loss: 1.6614 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00703: loss did not improve from 1.66129\n",
      "Epoch 704/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.6612 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00704: loss improved from 1.66129 to 1.66117, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 705/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.6612 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00705: loss improved from 1.66117 to 1.66116, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 706/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 1.6612 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00706: loss did not improve from 1.66116\n",
      "Epoch 707/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 1.6614 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00707: loss did not improve from 1.66116\n",
      "Epoch 708/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.6616 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00708: loss did not improve from 1.66116\n",
      "Epoch 709/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.6613 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00709: loss did not improve from 1.66116\n",
      "Epoch 710/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.6612 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00710: loss did not improve from 1.66116\n",
      "Epoch 711/2000\n",
      "240/240 [==============================] - 0s 798us/step - loss: 1.6616 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00711: loss did not improve from 1.66116\n",
      "Epoch 712/2000\n",
      "240/240 [==============================] - 0s 819us/step - loss: 1.6613 - accuracy: 0.3917\n",
      "\n",
      "Epoch 00712: loss did not improve from 1.66116\n",
      "Epoch 713/2000\n",
      "240/240 [==============================] - 0s 817us/step - loss: 1.6613 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00713: loss did not improve from 1.66116\n",
      "Epoch 714/2000\n",
      "240/240 [==============================] - 0s 779us/step - loss: 1.6613 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00714: loss did not improve from 1.66116\n",
      "Epoch 715/2000\n",
      "240/240 [==============================] - 0s 763us/step - loss: 1.6616 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00715: loss did not improve from 1.66116\n",
      "Epoch 716/2000\n",
      "240/240 [==============================] - 0s 718us/step - loss: 1.6644 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00716: loss did not improve from 1.66116\n",
      "Epoch 717/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.6616 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00717: loss did not improve from 1.66116\n",
      "Epoch 718/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.6644 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00718: loss did not improve from 1.66116\n",
      "Epoch 719/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.7023 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00719: loss did not improve from 1.66116\n",
      "Epoch 720/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.6724 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00720: loss did not improve from 1.66116\n",
      "Epoch 721/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 1.6618 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00721: loss did not improve from 1.66116\n",
      "Epoch 722/2000\n",
      "240/240 [==============================] - 0s 718us/step - loss: 1.6615 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00722: loss did not improve from 1.66116\n",
      "Epoch 723/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.6616 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00723: loss did not improve from 1.66116\n",
      "Epoch 724/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 1.6612 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00724: loss did not improve from 1.66116\n",
      "Epoch 725/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 1.6610 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00725: loss improved from 1.66116 to 1.66103, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 726/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.6611 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00726: loss did not improve from 1.66103\n",
      "Epoch 727/2000\n",
      "240/240 [==============================] - 0s 752us/step - loss: 1.6611 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00727: loss did not improve from 1.66103\n",
      "Epoch 728/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.6611 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00728: loss did not improve from 1.66103\n",
      "Epoch 729/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.6612 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00729: loss did not improve from 1.66103\n",
      "Epoch 730/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 1.6612 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00730: loss did not improve from 1.66103\n",
      "Epoch 731/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.6612 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00731: loss did not improve from 1.66103\n",
      "Epoch 732/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.6612 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00732: loss did not improve from 1.66103\n",
      "Epoch 733/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 1.6612 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00733: loss did not improve from 1.66103\n",
      "Epoch 734/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 717us/step - loss: 1.6613 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00734: loss did not improve from 1.66103\n",
      "Epoch 735/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.6612 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00735: loss did not improve from 1.66103\n",
      "Epoch 736/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 1.6612 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00736: loss did not improve from 1.66103\n",
      "Epoch 737/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.6612 - accuracy: 0.4000\n",
      "\n",
      "Epoch 00737: loss did not improve from 1.66103\n",
      "Epoch 738/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.6613 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00738: loss did not improve from 1.66103\n",
      "Epoch 739/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 1.6611 - accuracy: 0.4000\n",
      "\n",
      "Epoch 00739: loss did not improve from 1.66103\n",
      "Epoch 740/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 1.6617 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00740: loss did not improve from 1.66103\n",
      "Epoch 741/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.6613 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00741: loss did not improve from 1.66103\n",
      "Epoch 742/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.6617 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00742: loss did not improve from 1.66103\n",
      "Epoch 743/2000\n",
      "240/240 [==============================] - 0s 751us/step - loss: 1.6846 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00743: loss did not improve from 1.66103\n",
      "Epoch 744/2000\n",
      "240/240 [==============================] - 0s 763us/step - loss: 1.6663 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00744: loss did not improve from 1.66103\n",
      "Epoch 745/2000\n",
      "240/240 [==============================] - 0s 815us/step - loss: 1.6632 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00745: loss did not improve from 1.66103\n",
      "Epoch 746/2000\n",
      "240/240 [==============================] - 0s 747us/step - loss: 1.6624 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00746: loss did not improve from 1.66103\n",
      "Epoch 747/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.6612 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00747: loss did not improve from 1.66103\n",
      "Epoch 748/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.6636 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00748: loss did not improve from 1.66103\n",
      "Epoch 749/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.6621 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00749: loss did not improve from 1.66103\n",
      "Epoch 750/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.6618 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00750: loss did not improve from 1.66103\n",
      "Epoch 751/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.6615 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00751: loss did not improve from 1.66103\n",
      "Epoch 752/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.6614 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00752: loss did not improve from 1.66103\n",
      "Epoch 753/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.6621 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00753: loss did not improve from 1.66103\n",
      "Epoch 754/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.6617 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00754: loss did not improve from 1.66103\n",
      "Epoch 755/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.6618 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00755: loss did not improve from 1.66103\n",
      "Epoch 756/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 1.6621 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00756: loss did not improve from 1.66103\n",
      "Epoch 757/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 1.6626 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00757: loss did not improve from 1.66103\n",
      "Epoch 758/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.6622 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00758: loss did not improve from 1.66103\n",
      "Epoch 759/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.6620 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00759: loss did not improve from 1.66103\n",
      "Epoch 760/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.6617 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00760: loss did not improve from 1.66103\n",
      "Epoch 761/2000\n",
      "240/240 [==============================] - 0s 718us/step - loss: 1.6615 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00761: loss did not improve from 1.66103\n",
      "Epoch 762/2000\n",
      "240/240 [==============================] - 0s 756us/step - loss: 1.6612 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00762: loss did not improve from 1.66103\n",
      "Epoch 763/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.6612 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00763: loss did not improve from 1.66103\n",
      "Epoch 764/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.6613 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00764: loss did not improve from 1.66103\n",
      "Epoch 765/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.6614 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00765: loss did not improve from 1.66103\n",
      "Epoch 766/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 1.6615 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00766: loss did not improve from 1.66103\n",
      "Epoch 767/2000\n",
      "240/240 [==============================] - 0s 742us/step - loss: 1.6613 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00767: loss did not improve from 1.66103\n",
      "Epoch 768/2000\n",
      "240/240 [==============================] - 0s 721us/step - loss: 1.6614 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00768: loss did not improve from 1.66103\n",
      "Epoch 769/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.6668 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00769: loss did not improve from 1.66103\n",
      "Epoch 770/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.6633 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00770: loss did not improve from 1.66103\n",
      "Epoch 771/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 1.6618 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00771: loss did not improve from 1.66103\n",
      "Epoch 772/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 1.6617 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00772: loss did not improve from 1.66103\n",
      "Epoch 773/2000\n",
      "240/240 [==============================] - 0s 662us/step - loss: 1.6617 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00773: loss did not improve from 1.66103\n",
      "Epoch 774/2000\n",
      "240/240 [==============================] - 0s 765us/step - loss: 1.6614 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00774: loss did not improve from 1.66103\n",
      "Epoch 775/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.6613 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00775: loss did not improve from 1.66103\n",
      "Epoch 776/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.6613 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00776: loss did not improve from 1.66103\n",
      "Epoch 777/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.6612 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00777: loss did not improve from 1.66103\n",
      "Epoch 778/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.6612 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00778: loss did not improve from 1.66103\n",
      "Epoch 779/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 1.6788 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00779: loss did not improve from 1.66103\n",
      "Epoch 780/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.6996 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00780: loss did not improve from 1.66103\n",
      "Epoch 781/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.7632 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00781: loss did not improve from 1.66103\n",
      "Epoch 782/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.6688 - accuracy: 0.4000\n",
      "\n",
      "Epoch 00782: loss did not improve from 1.66103\n",
      "Epoch 783/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.6627 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00783: loss did not improve from 1.66103\n",
      "Epoch 784/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.6620 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00784: loss did not improve from 1.66103\n",
      "Epoch 785/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.6614 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00785: loss did not improve from 1.66103\n",
      "Epoch 786/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 1.6618 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00786: loss did not improve from 1.66103\n",
      "Epoch 787/2000\n",
      "240/240 [==============================] - 0s 753us/step - loss: 1.6614 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00787: loss did not improve from 1.66103\n",
      "Epoch 788/2000\n",
      "240/240 [==============================] - 0s 839us/step - loss: 1.6612 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00788: loss did not improve from 1.66103\n",
      "Epoch 789/2000\n",
      "240/240 [==============================] - 0s 847us/step - loss: 1.6613 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00789: loss did not improve from 1.66103\n",
      "Epoch 790/2000\n",
      "240/240 [==============================] - 0s 830us/step - loss: 1.6613 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00790: loss did not improve from 1.66103\n",
      "Epoch 791/2000\n",
      "240/240 [==============================] - 0s 721us/step - loss: 1.6613 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00791: loss did not improve from 1.66103\n",
      "Epoch 792/2000\n",
      "240/240 [==============================] - 0s 661us/step - loss: 1.6612 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00792: loss did not improve from 1.66103\n",
      "Epoch 793/2000\n",
      "240/240 [==============================] - 0s 655us/step - loss: 1.6612 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00793: loss did not improve from 1.66103\n",
      "Epoch 794/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.6611 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00794: loss did not improve from 1.66103\n",
      "Epoch 795/2000\n",
      "240/240 [==============================] - 0s 647us/step - loss: 1.6612 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00795: loss did not improve from 1.66103\n",
      "Epoch 796/2000\n",
      "240/240 [==============================] - 0s 654us/step - loss: 1.6612 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00796: loss did not improve from 1.66103\n",
      "Epoch 797/2000\n",
      "240/240 [==============================] - 0s 652us/step - loss: 1.6609 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00797: loss improved from 1.66103 to 1.66094, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 798/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.6610 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00798: loss did not improve from 1.66094\n",
      "Epoch 799/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 1.6609 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00799: loss improved from 1.66094 to 1.66087, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 800/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 1.6609 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00800: loss did not improve from 1.66087\n",
      "Epoch 801/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 1.6610 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00801: loss did not improve from 1.66087\n",
      "Epoch 802/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.6609 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00802: loss did not improve from 1.66087\n",
      "Epoch 803/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.6609 - accuracy: 0.4000\n",
      "\n",
      "Epoch 00803: loss did not improve from 1.66087\n",
      "Epoch 804/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.6609 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00804: loss did not improve from 1.66087\n",
      "Epoch 805/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 1.6609 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00805: loss did not improve from 1.66087\n",
      "Epoch 806/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.6612 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00806: loss did not improve from 1.66087\n",
      "Epoch 807/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.6608 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00807: loss improved from 1.66087 to 1.66078, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 808/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.6609 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00808: loss did not improve from 1.66078\n",
      "Epoch 809/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.6616 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00809: loss did not improve from 1.66078\n",
      "Epoch 810/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.6633 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00810: loss did not improve from 1.66078\n",
      "Epoch 811/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.6683 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00811: loss did not improve from 1.66078\n",
      "Epoch 812/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.6622 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00812: loss did not improve from 1.66078\n",
      "Epoch 813/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.6621 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00813: loss did not improve from 1.66078\n",
      "Epoch 814/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 1.6678 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00814: loss did not improve from 1.66078\n",
      "Epoch 815/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.6618 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00815: loss did not improve from 1.66078\n",
      "Epoch 816/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6612 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00816: loss did not improve from 1.66078\n",
      "Epoch 817/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6609 - accuracy: 0.3958\n",
      "\n",
      "Epoch 00817: loss did not improve from 1.66078\n",
      "Epoch 818/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6610 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00818: loss did not improve from 1.66078\n",
      "Epoch 819/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6607 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00819: loss improved from 1.66078 to 1.66072, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 820/2000\n",
      "240/240 [==============================] - 0s 732us/step - loss: 1.6608 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00820: loss did not improve from 1.66072\n",
      "Epoch 821/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 1.6609 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00821: loss did not improve from 1.66072\n",
      "Epoch 822/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.6608 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00822: loss did not improve from 1.66072\n",
      "Epoch 823/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 1.6606 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00823: loss improved from 1.66072 to 1.66063, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 824/2000\n",
      "240/240 [==============================] - 0s 726us/step - loss: 1.6608 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00824: loss did not improve from 1.66063\n",
      "Epoch 825/2000\n",
      "240/240 [==============================] - 0s 733us/step - loss: 1.6608 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00825: loss did not improve from 1.66063\n",
      "Epoch 826/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.6610 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00826: loss did not improve from 1.66063\n",
      "Epoch 827/2000\n",
      "240/240 [==============================] - 0s 722us/step - loss: 1.6608 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00827: loss did not improve from 1.66063\n",
      "Epoch 828/2000\n",
      "240/240 [==============================] - 0s 733us/step - loss: 1.6609 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00828: loss did not improve from 1.66063\n",
      "Epoch 829/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.6611 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00829: loss did not improve from 1.66063\n",
      "Epoch 830/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 1.6609 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00830: loss did not improve from 1.66063\n",
      "Epoch 831/2000\n",
      "240/240 [==============================] - 0s 726us/step - loss: 1.6607 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00831: loss did not improve from 1.66063\n",
      "Epoch 832/2000\n",
      "240/240 [==============================] - 0s 742us/step - loss: 1.6613 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00832: loss did not improve from 1.66063\n",
      "Epoch 833/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 1.6629 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00833: loss did not improve from 1.66063\n",
      "Epoch 834/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.6621 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00834: loss did not improve from 1.66063\n",
      "Epoch 835/2000\n",
      "240/240 [==============================] - 0s 721us/step - loss: 1.6610 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00835: loss did not improve from 1.66063\n",
      "Epoch 836/2000\n",
      "240/240 [==============================] - 0s 742us/step - loss: 1.6609 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00836: loss did not improve from 1.66063\n",
      "Epoch 837/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 1.6609 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00837: loss did not improve from 1.66063\n",
      "Epoch 838/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 730us/step - loss: 1.6607 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00838: loss did not improve from 1.66063\n",
      "Epoch 839/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.6610 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00839: loss did not improve from 1.66063\n",
      "Epoch 840/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 1.6609 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00840: loss did not improve from 1.66063\n",
      "Epoch 841/2000\n",
      "240/240 [==============================] - 0s 739us/step - loss: 1.6608 - accuracy: 0.4000\n",
      "\n",
      "Epoch 00841: loss did not improve from 1.66063\n",
      "Epoch 842/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.6609 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00842: loss did not improve from 1.66063\n",
      "Epoch 843/2000\n",
      "240/240 [==============================] - 0s 722us/step - loss: 1.6609 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00843: loss did not improve from 1.66063\n",
      "Epoch 844/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 1.6627 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00844: loss did not improve from 1.66063\n",
      "Epoch 845/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 1.6618 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00845: loss did not improve from 1.66063\n",
      "Epoch 846/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.6625 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00846: loss did not improve from 1.66063\n",
      "Epoch 847/2000\n",
      "240/240 [==============================] - 0s 756us/step - loss: 1.6614 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00847: loss did not improve from 1.66063\n",
      "Epoch 848/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 1.6612 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00848: loss did not improve from 1.66063\n",
      "Epoch 849/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.6607 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00849: loss did not improve from 1.66063\n",
      "Epoch 850/2000\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 1.6606 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00850: loss improved from 1.66063 to 1.66062, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 851/2000\n",
      "240/240 [==============================] - 0s 902us/step - loss: 1.6607 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00851: loss did not improve from 1.66062\n",
      "Epoch 852/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.6610 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00852: loss did not improve from 1.66062\n",
      "Epoch 853/2000\n",
      "240/240 [==============================] - 0s 655us/step - loss: 1.6610 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00853: loss did not improve from 1.66062\n",
      "Epoch 854/2000\n",
      "240/240 [==============================] - 0s 654us/step - loss: 1.6609 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00854: loss did not improve from 1.66062\n",
      "Epoch 855/2000\n",
      "240/240 [==============================] - 0s 655us/step - loss: 1.6618 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00855: loss did not improve from 1.66062\n",
      "Epoch 856/2000\n",
      "240/240 [==============================] - 0s 652us/step - loss: 1.6639 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00856: loss did not improve from 1.66062\n",
      "Epoch 857/2000\n",
      "240/240 [==============================] - 0s 782us/step - loss: 1.6608 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00857: loss did not improve from 1.66062\n",
      "Epoch 858/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.6606 - accuracy: 0.4000\n",
      "\n",
      "Epoch 00858: loss improved from 1.66062 to 1.66056, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 859/2000\n",
      "240/240 [==============================] - 0s 751us/step - loss: 1.6616 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00859: loss did not improve from 1.66056\n",
      "Epoch 860/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.6607 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00860: loss did not improve from 1.66056\n",
      "Epoch 861/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.6607 - accuracy: 0.3958\n",
      "\n",
      "Epoch 00861: loss did not improve from 1.66056\n",
      "Epoch 862/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 1.6607 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00862: loss did not improve from 1.66056\n",
      "Epoch 863/2000\n",
      "240/240 [==============================] - 0s 638us/step - loss: 1.6609 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00863: loss did not improve from 1.66056\n",
      "Epoch 864/2000\n",
      "240/240 [==============================] - 0s 797us/step - loss: 1.6607 - accuracy: 0.4000\n",
      "\n",
      "Epoch 00864: loss did not improve from 1.66056\n",
      "Epoch 865/2000\n",
      "240/240 [==============================] - 0s 808us/step - loss: 1.6606 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00865: loss did not improve from 1.66056\n",
      "Epoch 866/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.6608 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00866: loss did not improve from 1.66056\n",
      "Epoch 867/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.6607 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00867: loss did not improve from 1.66056\n",
      "Epoch 868/2000\n",
      "240/240 [==============================] - 0s 647us/step - loss: 1.6607 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00868: loss did not improve from 1.66056\n",
      "Epoch 869/2000\n",
      "240/240 [==============================] - 0s 654us/step - loss: 1.6608 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00869: loss did not improve from 1.66056\n",
      "Epoch 870/2000\n",
      "240/240 [==============================] - 0s 646us/step - loss: 1.6606 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00870: loss did not improve from 1.66056\n",
      "Epoch 871/2000\n",
      "240/240 [==============================] - 0s 658us/step - loss: 1.6614 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00871: loss did not improve from 1.66056\n",
      "Epoch 872/2000\n",
      "240/240 [==============================] - 0s 638us/step - loss: 1.6619 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00872: loss did not improve from 1.66056\n",
      "Epoch 873/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.6622 - accuracy: 0.3958\n",
      "\n",
      "Epoch 00873: loss did not improve from 1.66056\n",
      "Epoch 874/2000\n",
      "240/240 [==============================] - 0s 632us/step - loss: 1.6634 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00874: loss did not improve from 1.66056\n",
      "Epoch 875/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.6611 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00875: loss did not improve from 1.66056\n",
      "Epoch 876/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.6608 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00876: loss did not improve from 1.66056\n",
      "Epoch 877/2000\n",
      "240/240 [==============================] - 0s 623us/step - loss: 1.6606 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00877: loss did not improve from 1.66056\n",
      "Epoch 878/2000\n",
      "240/240 [==============================] - 0s 769us/step - loss: 1.6761 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00878: loss did not improve from 1.66056\n",
      "Epoch 879/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 1.6655 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00879: loss did not improve from 1.66056\n",
      "Epoch 880/2000\n",
      "240/240 [==============================] - 0s 821us/step - loss: 1.6620 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00880: loss did not improve from 1.66056\n",
      "Epoch 881/2000\n",
      "240/240 [==============================] - 0s 806us/step - loss: 1.6620 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00881: loss did not improve from 1.66056\n",
      "Epoch 882/2000\n",
      "240/240 [==============================] - 0s 828us/step - loss: 1.6612 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00882: loss did not improve from 1.66056\n",
      "Epoch 883/2000\n",
      "240/240 [==============================] - 0s 798us/step - loss: 1.6610 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00883: loss did not improve from 1.66056\n",
      "Epoch 884/2000\n",
      "240/240 [==============================] - 0s 806us/step - loss: 1.6610 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00884: loss did not improve from 1.66056\n",
      "Epoch 885/2000\n",
      "240/240 [==============================] - 0s 740us/step - loss: 1.6608 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00885: loss did not improve from 1.66056\n",
      "Epoch 886/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.6610 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00886: loss did not improve from 1.66056\n",
      "Epoch 887/2000\n",
      "240/240 [==============================] - 0s 640us/step - loss: 1.6607 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00887: loss did not improve from 1.66056\n",
      "Epoch 888/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.6610 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00888: loss did not improve from 1.66056\n",
      "Epoch 889/2000\n",
      "240/240 [==============================] - 0s 667us/step - loss: 1.6613 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00889: loss did not improve from 1.66056\n",
      "Epoch 890/2000\n",
      "240/240 [==============================] - 0s 923us/step - loss: 1.6610 - accuracy: 0.4000\n",
      "\n",
      "Epoch 00890: loss did not improve from 1.66056\n",
      "Epoch 891/2000\n",
      "240/240 [==============================] - 0s 951us/step - loss: 1.6634 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00891: loss did not improve from 1.66056\n",
      "Epoch 892/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.6609 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00892: loss did not improve from 1.66056\n",
      "Epoch 893/2000\n",
      "240/240 [==============================] - 0s 810us/step - loss: 1.6605 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00893: loss improved from 1.66056 to 1.66051, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 894/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6610 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00894: loss did not improve from 1.66051\n",
      "Epoch 895/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.6611 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00895: loss did not improve from 1.66051\n",
      "Epoch 896/2000\n",
      "240/240 [==============================] - 0s 759us/step - loss: 1.6609 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00896: loss did not improve from 1.66051\n",
      "Epoch 897/2000\n",
      "240/240 [==============================] - 0s 648us/step - loss: 1.6604 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00897: loss improved from 1.66051 to 1.66039, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 898/2000\n",
      "240/240 [==============================] - 0s 823us/step - loss: 1.6607 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00898: loss did not improve from 1.66039\n",
      "Epoch 899/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 1.6606 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00899: loss did not improve from 1.66039\n",
      "Epoch 900/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 1.6607 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00900: loss did not improve from 1.66039\n",
      "Epoch 901/2000\n",
      "240/240 [==============================] - 0s 778us/step - loss: 1.6605 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00901: loss did not improve from 1.66039\n",
      "Epoch 902/2000\n",
      "240/240 [==============================] - 0s 646us/step - loss: 1.6607 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00902: loss did not improve from 1.66039\n",
      "Epoch 903/2000\n",
      "240/240 [==============================] - 0s 633us/step - loss: 1.6606 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00903: loss did not improve from 1.66039\n",
      "Epoch 904/2000\n",
      "240/240 [==============================] - 0s 641us/step - loss: 1.6609 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00904: loss did not improve from 1.66039\n",
      "Epoch 905/2000\n",
      "240/240 [==============================] - 0s 652us/step - loss: 1.6621 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00905: loss did not improve from 1.66039\n",
      "Epoch 906/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 1.6613 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00906: loss did not improve from 1.66039\n",
      "Epoch 907/2000\n",
      "240/240 [==============================] - 0s 667us/step - loss: 1.6612 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00907: loss did not improve from 1.66039\n",
      "Epoch 908/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.6614 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00908: loss did not improve from 1.66039\n",
      "Epoch 909/2000\n",
      "240/240 [==============================] - 0s 631us/step - loss: 1.6609 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00909: loss did not improve from 1.66039\n",
      "Epoch 910/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.6610 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00910: loss did not improve from 1.66039\n",
      "Epoch 911/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.6609 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00911: loss did not improve from 1.66039\n",
      "Epoch 912/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 1.6605 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00912: loss did not improve from 1.66039\n",
      "Epoch 913/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 1.6609 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00913: loss did not improve from 1.66039\n",
      "Epoch 914/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.6607 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00914: loss did not improve from 1.66039\n",
      "Epoch 915/2000\n",
      "240/240 [==============================] - 0s 642us/step - loss: 1.6605 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00915: loss did not improve from 1.66039\n",
      "Epoch 916/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.6605 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00916: loss did not improve from 1.66039\n",
      "Epoch 917/2000\n",
      "240/240 [==============================] - 0s 624us/step - loss: 1.6746 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00917: loss did not improve from 1.66039\n",
      "Epoch 918/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.6667 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00918: loss did not improve from 1.66039\n",
      "Epoch 919/2000\n",
      "240/240 [==============================] - 0s 635us/step - loss: 1.6635 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00919: loss did not improve from 1.66039\n",
      "Epoch 920/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.6613 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00920: loss did not improve from 1.66039\n",
      "Epoch 921/2000\n",
      "240/240 [==============================] - 0s 653us/step - loss: 1.6611 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00921: loss did not improve from 1.66039\n",
      "Epoch 922/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 1.6607 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00922: loss did not improve from 1.66039\n",
      "Epoch 923/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.6608 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00923: loss did not improve from 1.66039\n",
      "Epoch 924/2000\n",
      "240/240 [==============================] - 0s 631us/step - loss: 1.6609 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00924: loss did not improve from 1.66039\n",
      "Epoch 925/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.6607 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00925: loss did not improve from 1.66039\n",
      "Epoch 926/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.6607 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00926: loss did not improve from 1.66039\n",
      "Epoch 927/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 1.6608 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00927: loss did not improve from 1.66039\n",
      "Epoch 928/2000\n",
      "240/240 [==============================] - 0s 624us/step - loss: 1.6606 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00928: loss did not improve from 1.66039\n",
      "Epoch 929/2000\n",
      "240/240 [==============================] - 0s 631us/step - loss: 1.6640 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00929: loss did not improve from 1.66039\n",
      "Epoch 930/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.6610 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00930: loss did not improve from 1.66039\n",
      "Epoch 931/2000\n",
      "240/240 [==============================] - 0s 655us/step - loss: 1.6647 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00931: loss did not improve from 1.66039\n",
      "Epoch 932/2000\n",
      "240/240 [==============================] - 0s 638us/step - loss: 1.6611 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00932: loss did not improve from 1.66039\n",
      "Epoch 933/2000\n",
      "240/240 [==============================] - 0s 619us/step - loss: 1.6604 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00933: loss did not improve from 1.66039\n",
      "Epoch 934/2000\n",
      "240/240 [==============================] - 0s 625us/step - loss: 1.6605 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00934: loss did not improve from 1.66039\n",
      "Epoch 935/2000\n",
      "240/240 [==============================] - 0s 629us/step - loss: 1.6604 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00935: loss improved from 1.66039 to 1.66038, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 936/2000\n",
      "240/240 [==============================] - 0s 652us/step - loss: 1.6604 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00936: loss did not improve from 1.66038\n",
      "Epoch 937/2000\n",
      "240/240 [==============================] - 0s 628us/step - loss: 1.6603 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00937: loss improved from 1.66038 to 1.66033, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 938/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 1.6604 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00938: loss did not improve from 1.66033\n",
      "Epoch 939/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 1.6608 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00939: loss did not improve from 1.66033\n",
      "Epoch 940/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 1.6603 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00940: loss did not improve from 1.66033\n",
      "Epoch 941/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 1.6605 - accuracy: 0.3958\n",
      "\n",
      "Epoch 00941: loss did not improve from 1.66033\n",
      "Epoch 942/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 701us/step - loss: 1.6602 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00942: loss improved from 1.66033 to 1.66024, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 943/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 1.6603 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00943: loss did not improve from 1.66024\n",
      "Epoch 944/2000\n",
      "240/240 [==============================] - 0s 672us/step - loss: 1.6603 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00944: loss did not improve from 1.66024\n",
      "Epoch 945/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.6607 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00945: loss did not improve from 1.66024\n",
      "Epoch 946/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6604 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00946: loss did not improve from 1.66024\n",
      "Epoch 947/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.6604 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00947: loss did not improve from 1.66024\n",
      "Epoch 948/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.1071 - accuracy: 0.3667\n",
      "\n",
      "Epoch 00948: loss did not improve from 1.66024\n",
      "Epoch 949/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 2.0159 - accuracy: 0.3958\n",
      "\n",
      "Epoch 00949: loss did not improve from 1.66024\n",
      "Epoch 950/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6700 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00950: loss did not improve from 1.66024\n",
      "Epoch 951/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 1.6634 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00951: loss did not improve from 1.66024\n",
      "Epoch 952/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 1.6630 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00952: loss did not improve from 1.66024\n",
      "Epoch 953/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 1.6623 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00953: loss did not improve from 1.66024\n",
      "Epoch 954/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.6622 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00954: loss did not improve from 1.66024\n",
      "Epoch 955/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6615 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00955: loss did not improve from 1.66024\n",
      "Epoch 956/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6614 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00956: loss did not improve from 1.66024\n",
      "Epoch 957/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.6612 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00957: loss did not improve from 1.66024\n",
      "Epoch 958/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 1.6612 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00958: loss did not improve from 1.66024\n",
      "Epoch 959/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6609 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00959: loss did not improve from 1.66024\n",
      "Epoch 960/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6608 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00960: loss did not improve from 1.66024\n",
      "Epoch 961/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6607 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00961: loss did not improve from 1.66024\n",
      "Epoch 962/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6607 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00962: loss did not improve from 1.66024\n",
      "Epoch 963/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.6605 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00963: loss did not improve from 1.66024\n",
      "Epoch 964/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.6605 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00964: loss did not improve from 1.66024\n",
      "Epoch 965/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.6606 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00965: loss did not improve from 1.66024\n",
      "Epoch 966/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.6606 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00966: loss did not improve from 1.66024\n",
      "Epoch 967/2000\n",
      "240/240 [==============================] - 0s 877us/step - loss: 1.6605 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00967: loss did not improve from 1.66024\n",
      "Epoch 968/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.6604 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00968: loss did not improve from 1.66024\n",
      "Epoch 969/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6605 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00969: loss did not improve from 1.66024\n",
      "Epoch 970/2000\n",
      "240/240 [==============================] - 0s 869us/step - loss: 1.6606 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00970: loss did not improve from 1.66024\n",
      "Epoch 971/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.6604 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00971: loss did not improve from 1.66024\n",
      "Epoch 972/2000\n",
      "240/240 [==============================] - 0s 881us/step - loss: 1.6604 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00972: loss did not improve from 1.66024\n",
      "Epoch 973/2000\n",
      "240/240 [==============================] - 0s 765us/step - loss: 1.6603 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00973: loss did not improve from 1.66024\n",
      "Epoch 974/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 1.6602 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00974: loss improved from 1.66024 to 1.66022, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 975/2000\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 1.6625 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00975: loss did not improve from 1.66022\n",
      "Epoch 976/2000\n",
      "240/240 [==============================] - 0s 817us/step - loss: 1.6619 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00976: loss did not improve from 1.66022\n",
      "Epoch 977/2000\n",
      "240/240 [==============================] - 0s 828us/step - loss: 1.6607 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00977: loss did not improve from 1.66022\n",
      "Epoch 978/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.6608 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00978: loss did not improve from 1.66022\n",
      "Epoch 979/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.6606 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00979: loss did not improve from 1.66022\n",
      "Epoch 980/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 1.6607 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00980: loss did not improve from 1.66022\n",
      "Epoch 981/2000\n",
      "240/240 [==============================] - 0s 644us/step - loss: 1.6606 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00981: loss did not improve from 1.66022\n",
      "Epoch 982/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 1.6604 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00982: loss did not improve from 1.66022\n",
      "Epoch 983/2000\n",
      "240/240 [==============================] - 0s 654us/step - loss: 1.6603 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00983: loss did not improve from 1.66022\n",
      "Epoch 984/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6604 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00984: loss did not improve from 1.66022\n",
      "Epoch 985/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.6602 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00985: loss improved from 1.66022 to 1.66017, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 986/2000\n",
      "240/240 [==============================] - 0s 654us/step - loss: 1.6602 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00986: loss did not improve from 1.66017\n",
      "Epoch 987/2000\n",
      "240/240 [==============================] - 0s 642us/step - loss: 1.6605 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00987: loss did not improve from 1.66017\n",
      "Epoch 988/2000\n",
      "240/240 [==============================] - 0s 651us/step - loss: 1.6612 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00988: loss did not improve from 1.66017\n",
      "Epoch 989/2000\n",
      "240/240 [==============================] - 0s 666us/step - loss: 1.6614 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00989: loss did not improve from 1.66017\n",
      "Epoch 990/2000\n",
      "240/240 [==============================] - 0s 652us/step - loss: 1.6606 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00990: loss did not improve from 1.66017\n",
      "Epoch 991/2000\n",
      "240/240 [==============================] - 0s 644us/step - loss: 1.6603 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00991: loss did not improve from 1.66017\n",
      "Epoch 992/2000\n",
      "240/240 [==============================] - 0s 638us/step - loss: 1.6608 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00992: loss did not improve from 1.66017\n",
      "Epoch 993/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.6612 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00993: loss did not improve from 1.66017\n",
      "Epoch 994/2000\n",
      "240/240 [==============================] - 0s 662us/step - loss: 1.6606 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00994: loss did not improve from 1.66017\n",
      "Epoch 995/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.6605 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00995: loss did not improve from 1.66017\n",
      "Epoch 996/2000\n",
      "240/240 [==============================] - 0s 651us/step - loss: 1.6603 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00996: loss did not improve from 1.66017\n",
      "Epoch 997/2000\n",
      "240/240 [==============================] - 0s 632us/step - loss: 1.6602 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00997: loss did not improve from 1.66017\n",
      "Epoch 998/2000\n",
      "240/240 [==============================] - 0s 773us/step - loss: 1.6602 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00998: loss did not improve from 1.66017\n",
      "Epoch 999/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.6603 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00999: loss did not improve from 1.66017\n",
      "Epoch 1000/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6602 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01000: loss did not improve from 1.66017\n",
      "Epoch 1001/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6603 - accuracy: 0.4000\n",
      "\n",
      "Epoch 01001: loss did not improve from 1.66017\n",
      "Epoch 1002/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6603 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01002: loss did not improve from 1.66017\n",
      "Epoch 1003/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6602 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01003: loss did not improve from 1.66017\n",
      "Epoch 1004/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.6603 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01004: loss did not improve from 1.66017\n",
      "Epoch 1005/2000\n",
      "240/240 [==============================] - 0s 740us/step - loss: 1.6603 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01005: loss did not improve from 1.66017\n",
      "Epoch 1006/2000\n",
      "240/240 [==============================] - 0s 844us/step - loss: 1.6602 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01006: loss improved from 1.66017 to 1.66015, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 1007/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.6600 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01007: loss improved from 1.66015 to 1.65999, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 1008/2000\n",
      "240/240 [==============================] - 0s 873us/step - loss: 1.6660 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01008: loss did not improve from 1.65999\n",
      "Epoch 1009/2000\n",
      "240/240 [==============================] - 0s 807us/step - loss: 1.6667 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01009: loss did not improve from 1.65999\n",
      "Epoch 1010/2000\n",
      "240/240 [==============================] - 0s 823us/step - loss: 1.6638 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01010: loss did not improve from 1.65999\n",
      "Epoch 1011/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.6621 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01011: loss did not improve from 1.65999\n",
      "Epoch 1012/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.6612 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01012: loss did not improve from 1.65999\n",
      "Epoch 1013/2000\n",
      "240/240 [==============================] - 0s 848us/step - loss: 1.6610 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01013: loss did not improve from 1.65999\n",
      "Epoch 1014/2000\n",
      "240/240 [==============================] - 0s 697us/step - loss: 1.6609 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01014: loss did not improve from 1.65999\n",
      "Epoch 1015/2000\n",
      "240/240 [==============================] - 0s 733us/step - loss: 1.6607 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01015: loss did not improve from 1.65999\n",
      "Epoch 1016/2000\n",
      "240/240 [==============================] - 0s 794us/step - loss: 1.6609 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01016: loss did not improve from 1.65999\n",
      "Epoch 1017/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.6607 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01017: loss did not improve from 1.65999\n",
      "Epoch 1018/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6608 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01018: loss did not improve from 1.65999\n",
      "Epoch 1019/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.6607 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01019: loss did not improve from 1.65999\n",
      "Epoch 1020/2000\n",
      "240/240 [==============================] - 0s 780us/step - loss: 1.6604 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01020: loss did not improve from 1.65999\n",
      "Epoch 1021/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.6608 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01021: loss did not improve from 1.65999\n",
      "Epoch 1022/2000\n",
      "240/240 [==============================] - 0s 672us/step - loss: 1.6606 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01022: loss did not improve from 1.65999\n",
      "Epoch 1023/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.6605 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01023: loss did not improve from 1.65999\n",
      "Epoch 1024/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.6606 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01024: loss did not improve from 1.65999\n",
      "Epoch 1025/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 1.6603 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01025: loss did not improve from 1.65999\n",
      "Epoch 1026/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.6604 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01026: loss did not improve from 1.65999\n",
      "Epoch 1027/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.6604 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01027: loss did not improve from 1.65999\n",
      "Epoch 1028/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6604 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01028: loss did not improve from 1.65999\n",
      "Epoch 1029/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6623 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01029: loss did not improve from 1.65999\n",
      "Epoch 1030/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.6607 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01030: loss did not improve from 1.65999\n",
      "Epoch 1031/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.6608 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01031: loss did not improve from 1.65999\n",
      "Epoch 1032/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 1.6607 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01032: loss did not improve from 1.65999\n",
      "Epoch 1033/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 1.6604 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01033: loss did not improve from 1.65999\n",
      "Epoch 1034/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.6604 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01034: loss did not improve from 1.65999\n",
      "Epoch 1035/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.6604 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01035: loss did not improve from 1.65999\n",
      "Epoch 1036/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6604 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01036: loss did not improve from 1.65999\n",
      "Epoch 1037/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.6606 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01037: loss did not improve from 1.65999\n",
      "Epoch 1038/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.6602 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01038: loss did not improve from 1.65999\n",
      "Epoch 1039/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.6603 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01039: loss did not improve from 1.65999\n",
      "Epoch 1040/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.6612 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01040: loss did not improve from 1.65999\n",
      "Epoch 1041/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 1.6617 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01041: loss did not improve from 1.65999\n",
      "Epoch 1042/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6608 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01042: loss did not improve from 1.65999\n",
      "Epoch 1043/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6605 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01043: loss did not improve from 1.65999\n",
      "Epoch 1044/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.6603 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01044: loss did not improve from 1.65999\n",
      "Epoch 1045/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.6607 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01045: loss did not improve from 1.65999\n",
      "Epoch 1046/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 710us/step - loss: 1.6608 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01046: loss did not improve from 1.65999\n",
      "Epoch 1047/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 1.6611 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01047: loss did not improve from 1.65999\n",
      "Epoch 1048/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.6606 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01048: loss did not improve from 1.65999\n",
      "Epoch 1049/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.6603 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01049: loss did not improve from 1.65999\n",
      "Epoch 1050/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.6603 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01050: loss did not improve from 1.65999\n",
      "Epoch 1051/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.6603 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01051: loss did not improve from 1.65999\n",
      "Epoch 1052/2000\n",
      "240/240 [==============================] - 0s 744us/step - loss: 1.6607 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01052: loss did not improve from 1.65999\n",
      "Epoch 1053/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.6607 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01053: loss did not improve from 1.65999\n",
      "Epoch 1054/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6605 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01054: loss did not improve from 1.65999\n",
      "Epoch 1055/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6603 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01055: loss did not improve from 1.65999\n",
      "Epoch 1056/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.6604 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01056: loss did not improve from 1.65999\n",
      "Epoch 1057/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6602 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01057: loss did not improve from 1.65999\n",
      "Epoch 1058/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6604 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01058: loss did not improve from 1.65999\n",
      "Epoch 1059/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6603 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01059: loss did not improve from 1.65999\n",
      "Epoch 1060/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 1.6602 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01060: loss did not improve from 1.65999\n",
      "Epoch 1061/2000\n",
      "240/240 [==============================] - 0s 745us/step - loss: 1.6603 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01061: loss did not improve from 1.65999\n",
      "Epoch 1062/2000\n",
      "240/240 [==============================] - 0s 747us/step - loss: 1.6607 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01062: loss did not improve from 1.65999\n",
      "Epoch 1063/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 1.6605 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01063: loss did not improve from 1.65999\n",
      "Epoch 1064/2000\n",
      "240/240 [==============================] - 0s 734us/step - loss: 1.6601 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01064: loss did not improve from 1.65999\n",
      "Epoch 1065/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 1.6609 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01065: loss did not improve from 1.65999\n",
      "Epoch 1066/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.6604 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01066: loss did not improve from 1.65999\n",
      "Epoch 1067/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 1.6605 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01067: loss did not improve from 1.65999\n",
      "Epoch 1068/2000\n",
      "240/240 [==============================] - 0s 848us/step - loss: 1.6602 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01068: loss did not improve from 1.65999\n",
      "Epoch 1069/2000\n",
      "240/240 [==============================] - 0s 813us/step - loss: 1.6604 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01069: loss did not improve from 1.65999\n",
      "Epoch 1070/2000\n",
      "240/240 [==============================] - 0s 793us/step - loss: 1.6604 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01070: loss did not improve from 1.65999\n",
      "Epoch 1071/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 1.6605 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01071: loss did not improve from 1.65999\n",
      "Epoch 1072/2000\n",
      "240/240 [==============================] - 0s 660us/step - loss: 1.6601 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01072: loss did not improve from 1.65999\n",
      "Epoch 1073/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6602 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01073: loss did not improve from 1.65999\n",
      "Epoch 1074/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6601 - accuracy: 0.4000\n",
      "\n",
      "Epoch 01074: loss did not improve from 1.65999\n",
      "Epoch 1075/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6602 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01075: loss did not improve from 1.65999\n",
      "Epoch 1076/2000\n",
      "240/240 [==============================] - 0s 611us/step - loss: 1.6601 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01076: loss did not improve from 1.65999\n",
      "Epoch 1077/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6602 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01077: loss did not improve from 1.65999\n",
      "Epoch 1078/2000\n",
      "240/240 [==============================] - 0s 644us/step - loss: 1.6601 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01078: loss did not improve from 1.65999\n",
      "Epoch 1079/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.6601 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01079: loss did not improve from 1.65999\n",
      "Epoch 1080/2000\n",
      "240/240 [==============================] - 0s 642us/step - loss: 1.6602 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01080: loss did not improve from 1.65999\n",
      "Epoch 1081/2000\n",
      "240/240 [==============================] - 0s 629us/step - loss: 1.6604 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01081: loss did not improve from 1.65999\n",
      "Epoch 1082/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.6608 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01082: loss did not improve from 1.65999\n",
      "Epoch 1083/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.6604 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01083: loss did not improve from 1.65999\n",
      "Epoch 1084/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.6631 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01084: loss did not improve from 1.65999\n",
      "Epoch 1085/2000\n",
      "240/240 [==============================] - 0s 643us/step - loss: 1.6618 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01085: loss did not improve from 1.65999\n",
      "Epoch 1086/2000\n",
      "240/240 [==============================] - 0s 656us/step - loss: 1.6601 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01086: loss did not improve from 1.65999\n",
      "Epoch 1087/2000\n",
      "240/240 [==============================] - 0s 631us/step - loss: 1.6601 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01087: loss did not improve from 1.65999\n",
      "Epoch 1088/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.6606 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01088: loss did not improve from 1.65999\n",
      "Epoch 1089/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.6602 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01089: loss did not improve from 1.65999\n",
      "Epoch 1090/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.6636 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01090: loss did not improve from 1.65999\n",
      "Epoch 1091/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.7061 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01091: loss did not improve from 1.65999\n",
      "Epoch 1092/2000\n",
      "240/240 [==============================] - 0s 646us/step - loss: 2.4253 - accuracy: 0.3958\n",
      "\n",
      "Epoch 01092: loss did not improve from 1.65999\n",
      "Epoch 1093/2000\n",
      "240/240 [==============================] - 0s 644us/step - loss: 1.6766 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01093: loss did not improve from 1.65999\n",
      "Epoch 1094/2000\n",
      "240/240 [==============================] - 0s 628us/step - loss: 1.6647 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01094: loss did not improve from 1.65999\n",
      "Epoch 1095/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.6614 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01095: loss did not improve from 1.65999\n",
      "Epoch 1096/2000\n",
      "240/240 [==============================] - 0s 640us/step - loss: 1.6610 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01096: loss did not improve from 1.65999\n",
      "Epoch 1097/2000\n",
      "240/240 [==============================] - 0s 768us/step - loss: 1.6608 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01097: loss did not improve from 1.65999\n",
      "Epoch 1098/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.6609 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01098: loss did not improve from 1.65999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1099/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.6606 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01099: loss did not improve from 1.65999\n",
      "Epoch 1100/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.6604 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01100: loss did not improve from 1.65999\n",
      "Epoch 1101/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 1.6604 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01101: loss did not improve from 1.65999\n",
      "Epoch 1102/2000\n",
      "240/240 [==============================] - 0s 624us/step - loss: 1.6604 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01102: loss did not improve from 1.65999\n",
      "Epoch 1103/2000\n",
      "240/240 [==============================] - 0s 642us/step - loss: 1.6603 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01103: loss did not improve from 1.65999\n",
      "Epoch 1104/2000\n",
      "240/240 [==============================] - 0s 651us/step - loss: 1.6602 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01104: loss did not improve from 1.65999\n",
      "Epoch 1105/2000\n",
      "240/240 [==============================] - 0s 649us/step - loss: 1.6601 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01105: loss did not improve from 1.65999\n",
      "Epoch 1106/2000\n",
      "240/240 [==============================] - 0s 638us/step - loss: 1.6602 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01106: loss did not improve from 1.65999\n",
      "Epoch 1107/2000\n",
      "240/240 [==============================] - 0s 649us/step - loss: 1.6600 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01107: loss improved from 1.65999 to 1.65996, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 1108/2000\n",
      "240/240 [==============================] - 0s 761us/step - loss: 1.6600 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01108: loss did not improve from 1.65996\n",
      "Epoch 1109/2000\n",
      "240/240 [==============================] - 0s 738us/step - loss: 1.6601 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01109: loss did not improve from 1.65996\n",
      "Epoch 1110/2000\n",
      "240/240 [==============================] - 0s 738us/step - loss: 1.6600 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01110: loss did not improve from 1.65996\n",
      "Epoch 1111/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 1.6600 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01111: loss did not improve from 1.65996\n",
      "Epoch 1112/2000\n",
      "240/240 [==============================] - 0s 758us/step - loss: 1.6600 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01112: loss did not improve from 1.65996\n",
      "Epoch 1113/2000\n",
      "240/240 [==============================] - 0s 759us/step - loss: 1.6601 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01113: loss did not improve from 1.65996\n",
      "Epoch 1114/2000\n",
      "240/240 [==============================] - 0s 766us/step - loss: 1.6600 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01114: loss did not improve from 1.65996\n",
      "Epoch 1115/2000\n",
      "240/240 [==============================] - 0s 756us/step - loss: 1.6598 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01115: loss improved from 1.65996 to 1.65980, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 1116/2000\n",
      "240/240 [==============================] - 0s 774us/step - loss: 1.6601 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01116: loss did not improve from 1.65980\n",
      "Epoch 1117/2000\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 1.6600 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01117: loss did not improve from 1.65980\n",
      "Epoch 1118/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.6599 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01118: loss did not improve from 1.65980\n",
      "Epoch 1119/2000\n",
      "240/240 [==============================] - 0s 662us/step - loss: 1.6601 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01119: loss did not improve from 1.65980\n",
      "Epoch 1120/2000\n",
      "240/240 [==============================] - 0s 653us/step - loss: 1.6599 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01120: loss did not improve from 1.65980\n",
      "Epoch 1121/2000\n",
      "240/240 [==============================] - 0s 659us/step - loss: 1.6600 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01121: loss did not improve from 1.65980\n",
      "Epoch 1122/2000\n",
      "240/240 [==============================] - 0s 654us/step - loss: 1.6600 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01122: loss did not improve from 1.65980\n",
      "Epoch 1123/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.6601 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01123: loss did not improve from 1.65980\n",
      "Epoch 1124/2000\n",
      "240/240 [==============================] - 0s 620us/step - loss: 1.6599 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01124: loss did not improve from 1.65980\n",
      "Epoch 1125/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 1.6603 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01125: loss did not improve from 1.65980\n",
      "Epoch 1126/2000\n",
      "240/240 [==============================] - 0s 619us/step - loss: 1.6602 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01126: loss did not improve from 1.65980\n",
      "Epoch 1127/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.6601 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01127: loss did not improve from 1.65980\n",
      "Epoch 1128/2000\n",
      "240/240 [==============================] - 0s 629us/step - loss: 1.6599 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01128: loss did not improve from 1.65980\n",
      "Epoch 1129/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.6649 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01129: loss did not improve from 1.65980\n",
      "Epoch 1130/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.6635 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01130: loss did not improve from 1.65980\n",
      "Epoch 1131/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 1.6609 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01131: loss did not improve from 1.65980\n",
      "Epoch 1132/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 1.6604 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01132: loss did not improve from 1.65980\n",
      "Epoch 1133/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.6600 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01133: loss did not improve from 1.65980\n",
      "Epoch 1134/2000\n",
      "240/240 [==============================] - 0s 646us/step - loss: 1.6600 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01134: loss did not improve from 1.65980\n",
      "Epoch 1135/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 1.6598 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01135: loss did not improve from 1.65980\n",
      "Epoch 1136/2000\n",
      "240/240 [==============================] - 0s 629us/step - loss: 1.6599 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01136: loss did not improve from 1.65980\n",
      "Epoch 1137/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.6598 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01137: loss did not improve from 1.65980\n",
      "Epoch 1138/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.6599 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01138: loss did not improve from 1.65980\n",
      "Epoch 1139/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.6599 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01139: loss did not improve from 1.65980\n",
      "Epoch 1140/2000\n",
      "240/240 [==============================] - 0s 652us/step - loss: 1.6601 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01140: loss did not improve from 1.65980\n",
      "Epoch 1141/2000\n",
      "240/240 [==============================] - 0s 633us/step - loss: 1.6597 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01141: loss improved from 1.65980 to 1.65972, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 1142/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.6599 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01142: loss did not improve from 1.65972\n",
      "Epoch 1143/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 1.6598 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01143: loss did not improve from 1.65972\n",
      "Epoch 1144/2000\n",
      "240/240 [==============================] - 0s 734us/step - loss: 1.6601 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01144: loss did not improve from 1.65972\n",
      "Epoch 1145/2000\n",
      "240/240 [==============================] - 0s 756us/step - loss: 1.6599 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01145: loss did not improve from 1.65972\n",
      "Epoch 1146/2000\n",
      "240/240 [==============================] - 0s 733us/step - loss: 1.6600 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01146: loss did not improve from 1.65972\n",
      "Epoch 1147/2000\n",
      "240/240 [==============================] - 0s 740us/step - loss: 1.6600 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01147: loss did not improve from 1.65972\n",
      "Epoch 1148/2000\n",
      "240/240 [==============================] - 0s 744us/step - loss: 1.6599 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01148: loss did not improve from 1.65972\n",
      "Epoch 1149/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.6608 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01149: loss did not improve from 1.65972\n",
      "Epoch 1150/2000\n",
      "240/240 [==============================] - 0s 737us/step - loss: 1.6604 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01150: loss did not improve from 1.65972\n",
      "Epoch 1151/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 723us/step - loss: 1.6609 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01151: loss did not improve from 1.65972\n",
      "Epoch 1152/2000\n",
      "240/240 [==============================] - 0s 718us/step - loss: 1.6603 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01152: loss did not improve from 1.65972\n",
      "Epoch 1153/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 1.6604 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01153: loss did not improve from 1.65972\n",
      "Epoch 1154/2000\n",
      "240/240 [==============================] - 0s 722us/step - loss: 1.6615 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01154: loss did not improve from 1.65972\n",
      "Epoch 1155/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 1.6607 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01155: loss did not improve from 1.65972\n",
      "Epoch 1156/2000\n",
      "240/240 [==============================] - 0s 744us/step - loss: 1.6609 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01156: loss did not improve from 1.65972\n",
      "Epoch 1157/2000\n",
      "240/240 [==============================] - 0s 738us/step - loss: 1.6601 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01157: loss did not improve from 1.65972\n",
      "Epoch 1158/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.6600 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01158: loss did not improve from 1.65972\n",
      "Epoch 1159/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.6600 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01159: loss did not improve from 1.65972\n",
      "Epoch 1160/2000\n",
      "240/240 [==============================] - 0s 732us/step - loss: 1.6601 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01160: loss did not improve from 1.65972\n",
      "Epoch 1161/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 1.6597 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01161: loss did not improve from 1.65972\n",
      "Epoch 1162/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.6599 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01162: loss did not improve from 1.65972\n",
      "Epoch 1163/2000\n",
      "240/240 [==============================] - 0s 786us/step - loss: 1.6598 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01163: loss did not improve from 1.65972\n",
      "Epoch 1164/2000\n",
      "240/240 [==============================] - 0s 805us/step - loss: 1.6598 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01164: loss did not improve from 1.65972\n",
      "Epoch 1165/2000\n",
      "240/240 [==============================] - 0s 819us/step - loss: 1.6604 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01165: loss did not improve from 1.65972\n",
      "Epoch 1166/2000\n",
      "240/240 [==============================] - 0s 783us/step - loss: 1.6625 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01166: loss did not improve from 1.65972\n",
      "Epoch 1167/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.6633 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01167: loss did not improve from 1.65972\n",
      "Epoch 1168/2000\n",
      "240/240 [==============================] - 0s 624us/step - loss: 1.6624 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01168: loss did not improve from 1.65972\n",
      "Epoch 1169/2000\n",
      "240/240 [==============================] - 0s 620us/step - loss: 1.6600 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01169: loss did not improve from 1.65972\n",
      "Epoch 1170/2000\n",
      "240/240 [==============================] - 0s 628us/step - loss: 1.6598 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01170: loss did not improve from 1.65972\n",
      "Epoch 1171/2000\n",
      "240/240 [==============================] - 0s 650us/step - loss: 1.6597 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01171: loss improved from 1.65972 to 1.65967, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 1172/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.6597 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01172: loss did not improve from 1.65967\n",
      "Epoch 1173/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 1.6597 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01173: loss did not improve from 1.65967\n",
      "Epoch 1174/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.6599 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01174: loss did not improve from 1.65967\n",
      "Epoch 1175/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.6597 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01175: loss did not improve from 1.65967\n",
      "Epoch 1176/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6599 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01176: loss did not improve from 1.65967\n",
      "Epoch 1177/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6597 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01177: loss did not improve from 1.65967\n",
      "Epoch 1178/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.6598 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01178: loss did not improve from 1.65967\n",
      "Epoch 1179/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.6599 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01179: loss did not improve from 1.65967\n",
      "Epoch 1180/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6605 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01180: loss did not improve from 1.65967\n",
      "Epoch 1181/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6618 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01181: loss did not improve from 1.65967\n",
      "Epoch 1182/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.6603 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01182: loss did not improve from 1.65967\n",
      "Epoch 1183/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 1.6603 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01183: loss did not improve from 1.65967\n",
      "Epoch 1184/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6597 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01184: loss did not improve from 1.65967\n",
      "Epoch 1185/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6597 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01185: loss did not improve from 1.65967\n",
      "Epoch 1186/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.6597 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01186: loss did not improve from 1.65967\n",
      "Epoch 1187/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.6610 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01187: loss did not improve from 1.65967\n",
      "Epoch 1188/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.6615 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01188: loss did not improve from 1.65967\n",
      "Epoch 1189/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6604 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01189: loss did not improve from 1.65967\n",
      "Epoch 1190/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6612 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01190: loss did not improve from 1.65967\n",
      "Epoch 1191/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.6622 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01191: loss did not improve from 1.65967\n",
      "Epoch 1192/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 1.6600 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01192: loss did not improve from 1.65967\n",
      "Epoch 1193/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 1.6598 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01193: loss did not improve from 1.65967\n",
      "Epoch 1194/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.6599 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01194: loss did not improve from 1.65967\n",
      "Epoch 1195/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6598 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01195: loss did not improve from 1.65967\n",
      "Epoch 1196/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6597 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01196: loss did not improve from 1.65967\n",
      "Epoch 1197/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.6596 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01197: loss improved from 1.65967 to 1.65963, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 1198/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6598 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01198: loss did not improve from 1.65963\n",
      "Epoch 1199/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6602 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01199: loss did not improve from 1.65963\n",
      "Epoch 1200/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.6599 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01200: loss did not improve from 1.65963\n",
      "Epoch 1201/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.6599 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01201: loss did not improve from 1.65963\n",
      "Epoch 1202/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.6610 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01202: loss did not improve from 1.65963\n",
      "Epoch 1203/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6601 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01203: loss did not improve from 1.65963\n",
      "Epoch 1204/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6600 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01204: loss did not improve from 1.65963\n",
      "Epoch 1205/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6601 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01205: loss did not improve from 1.65963\n",
      "Epoch 1206/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6599 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01206: loss did not improve from 1.65963\n",
      "Epoch 1207/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6675 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01207: loss did not improve from 1.65963\n",
      "Epoch 1208/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6606 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01208: loss did not improve from 1.65963\n",
      "Epoch 1209/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.6602 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01209: loss did not improve from 1.65963\n",
      "Epoch 1210/2000\n",
      "240/240 [==============================] - 0s 718us/step - loss: 1.6610 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01210: loss did not improve from 1.65963\n",
      "Epoch 1211/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.6604 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01211: loss did not improve from 1.65963\n",
      "Epoch 1212/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6601 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01212: loss did not improve from 1.65963\n",
      "Epoch 1213/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6603 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01213: loss did not improve from 1.65963\n",
      "Epoch 1214/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6599 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01214: loss did not improve from 1.65963\n",
      "Epoch 1215/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6598 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01215: loss did not improve from 1.65963\n",
      "Epoch 1216/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6600 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01216: loss did not improve from 1.65963\n",
      "Epoch 1217/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6599 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01217: loss did not improve from 1.65963\n",
      "Epoch 1218/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6599 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01218: loss did not improve from 1.65963\n",
      "Epoch 1219/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6602 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01219: loss did not improve from 1.65963\n",
      "Epoch 1220/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6613 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01220: loss did not improve from 1.65963\n",
      "Epoch 1221/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6603 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01221: loss did not improve from 1.65963\n",
      "Epoch 1222/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6602 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01222: loss did not improve from 1.65963\n",
      "Epoch 1223/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6600 - accuracy: 0.3917\n",
      "\n",
      "Epoch 01223: loss did not improve from 1.65963\n",
      "Epoch 1224/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6600 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01224: loss did not improve from 1.65963\n",
      "Epoch 1225/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 1.6598 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01225: loss did not improve from 1.65963\n",
      "Epoch 1226/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6598 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01226: loss did not improve from 1.65963\n",
      "Epoch 1227/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 1.6598 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01227: loss did not improve from 1.65963\n",
      "Epoch 1228/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6599 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01228: loss did not improve from 1.65963\n",
      "Epoch 1229/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.6597 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01229: loss did not improve from 1.65963\n",
      "Epoch 1230/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6600 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01230: loss did not improve from 1.65963\n",
      "Epoch 1231/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 1.6598 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01231: loss did not improve from 1.65963\n",
      "Epoch 1232/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6598 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01232: loss did not improve from 1.65963\n",
      "Epoch 1233/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.6598 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01233: loss did not improve from 1.65963\n",
      "Epoch 1234/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.6598 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01234: loss did not improve from 1.65963\n",
      "Epoch 1235/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6602 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01235: loss did not improve from 1.65963\n",
      "Epoch 1236/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6620 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01236: loss did not improve from 1.65963\n",
      "Epoch 1237/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.6608 - accuracy: 0.3958\n",
      "\n",
      "Epoch 01237: loss did not improve from 1.65963\n",
      "Epoch 1238/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 1.6601 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01238: loss did not improve from 1.65963\n",
      "Epoch 1239/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.6598 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01239: loss did not improve from 1.65963\n",
      "Epoch 1240/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.6597 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01240: loss did not improve from 1.65963\n",
      "Epoch 1241/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 1.6596 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01241: loss improved from 1.65963 to 1.65957, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 1242/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.6597 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01242: loss did not improve from 1.65957\n",
      "Epoch 1243/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6610 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01243: loss did not improve from 1.65957\n",
      "Epoch 1244/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6613 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01244: loss did not improve from 1.65957\n",
      "Epoch 1245/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6604 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01245: loss did not improve from 1.65957\n",
      "Epoch 1246/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6600 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01246: loss did not improve from 1.65957\n",
      "Epoch 1247/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6670 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01247: loss did not improve from 1.65957\n",
      "Epoch 1248/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6622 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01248: loss did not improve from 1.65957\n",
      "Epoch 1249/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6601 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01249: loss did not improve from 1.65957\n",
      "Epoch 1250/2000\n",
      "240/240 [==============================] - 0s 672us/step - loss: 1.6617 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01250: loss did not improve from 1.65957\n",
      "Epoch 1251/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6599 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01251: loss did not improve from 1.65957\n",
      "Epoch 1252/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6598 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01252: loss did not improve from 1.65957\n",
      "Epoch 1253/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 2.2977 - accuracy: 0.3833\n",
      "\n",
      "Epoch 01253: loss did not improve from 1.65957\n",
      "Epoch 1254/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.8516 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01254: loss did not improve from 1.65957\n",
      "Epoch 1255/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 678us/step - loss: 1.6723 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01255: loss did not improve from 1.65957\n",
      "Epoch 1256/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.6687 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01256: loss did not improve from 1.65957\n",
      "Epoch 1257/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.8477 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01257: loss did not improve from 1.65957\n",
      "Epoch 1258/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.6606 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01258: loss did not improve from 1.65957\n",
      "Epoch 1259/2000\n",
      "240/240 [==============================] - 0s 799us/step - loss: 1.6605 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01259: loss did not improve from 1.65957\n",
      "Epoch 1260/2000\n",
      "240/240 [==============================] - 0s 830us/step - loss: 1.6602 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01260: loss did not improve from 1.65957\n",
      "Epoch 1261/2000\n",
      "240/240 [==============================] - 0s 815us/step - loss: 1.6600 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01261: loss did not improve from 1.65957\n",
      "Epoch 1262/2000\n",
      "240/240 [==============================] - 0s 776us/step - loss: 1.6601 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01262: loss did not improve from 1.65957\n",
      "Epoch 1263/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.6598 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01263: loss did not improve from 1.65957\n",
      "Epoch 1264/2000\n",
      "240/240 [==============================] - 0s 632us/step - loss: 1.6599 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01264: loss did not improve from 1.65957\n",
      "Epoch 1265/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 1.6599 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01265: loss did not improve from 1.65957\n",
      "Epoch 1266/2000\n",
      "240/240 [==============================] - 0s 648us/step - loss: 1.6599 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01266: loss did not improve from 1.65957\n",
      "Epoch 1267/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 1.6599 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01267: loss did not improve from 1.65957\n",
      "Epoch 1268/2000\n",
      "240/240 [==============================] - 0s 659us/step - loss: 1.6598 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01268: loss did not improve from 1.65957\n",
      "Epoch 1269/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6597 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01269: loss did not improve from 1.65957\n",
      "Epoch 1270/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.6598 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01270: loss did not improve from 1.65957\n",
      "Epoch 1271/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.6597 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01271: loss did not improve from 1.65957\n",
      "Epoch 1272/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6597 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01272: loss did not improve from 1.65957\n",
      "Epoch 1273/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6596 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01273: loss did not improve from 1.65957\n",
      "Epoch 1274/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.6597 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01274: loss did not improve from 1.65957\n",
      "Epoch 1275/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 1.6596 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01275: loss did not improve from 1.65957\n",
      "Epoch 1276/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6597 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01276: loss did not improve from 1.65957\n",
      "Epoch 1277/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.6598 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01277: loss did not improve from 1.65957\n",
      "Epoch 1278/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.6595 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01278: loss improved from 1.65957 to 1.65955, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 1279/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.6597 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01279: loss did not improve from 1.65955\n",
      "Epoch 1280/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 1.6596 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01280: loss did not improve from 1.65955\n",
      "Epoch 1281/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6597 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01281: loss did not improve from 1.65955\n",
      "Epoch 1282/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6596 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01282: loss did not improve from 1.65955\n",
      "Epoch 1283/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.6597 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01283: loss did not improve from 1.65955\n",
      "Epoch 1284/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 1.6597 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01284: loss did not improve from 1.65955\n",
      "Epoch 1285/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6597 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01285: loss did not improve from 1.65955\n",
      "Epoch 1286/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 1.6597 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01286: loss did not improve from 1.65955\n",
      "Epoch 1287/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.6595 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01287: loss improved from 1.65955 to 1.65947, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 1288/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.6612 - accuracy: 0.4000\n",
      "\n",
      "Epoch 01288: loss did not improve from 1.65947\n",
      "Epoch 1289/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6602 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01289: loss did not improve from 1.65947\n",
      "Epoch 1290/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6600 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01290: loss did not improve from 1.65947\n",
      "Epoch 1291/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 1.6598 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01291: loss did not improve from 1.65947\n",
      "Epoch 1292/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6596 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01292: loss did not improve from 1.65947\n",
      "Epoch 1293/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.6596 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01293: loss did not improve from 1.65947\n",
      "Epoch 1294/2000\n",
      "240/240 [==============================] - 0s 697us/step - loss: 1.6598 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01294: loss did not improve from 1.65947\n",
      "Epoch 1295/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.6595 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01295: loss did not improve from 1.65947\n",
      "Epoch 1296/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6595 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01296: loss did not improve from 1.65947\n",
      "Epoch 1297/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.6598 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01297: loss did not improve from 1.65947\n",
      "Epoch 1298/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.6598 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01298: loss did not improve from 1.65947\n",
      "Epoch 1299/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6598 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01299: loss did not improve from 1.65947\n",
      "Epoch 1300/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.6597 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01300: loss did not improve from 1.65947\n",
      "Epoch 1301/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6598 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01301: loss did not improve from 1.65947\n",
      "Epoch 1302/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6600 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01302: loss did not improve from 1.65947\n",
      "Epoch 1303/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.6599 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01303: loss did not improve from 1.65947\n",
      "Epoch 1304/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6631 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01304: loss did not improve from 1.65947\n",
      "Epoch 1305/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6607 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01305: loss did not improve from 1.65947\n",
      "Epoch 1306/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6605 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01306: loss did not improve from 1.65947\n",
      "Epoch 1307/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6598 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01307: loss did not improve from 1.65947\n",
      "Epoch 1308/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6602 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01308: loss did not improve from 1.65947\n",
      "Epoch 1309/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.6600 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01309: loss did not improve from 1.65947\n",
      "Epoch 1310/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6596 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01310: loss did not improve from 1.65947\n",
      "Epoch 1311/2000\n",
      "240/240 [==============================] - 0s 828us/step - loss: 1.6595 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01311: loss did not improve from 1.65947\n",
      "Epoch 1312/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 1.6597 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01312: loss did not improve from 1.65947\n",
      "Epoch 1313/2000\n",
      "240/240 [==============================] - 0s 631us/step - loss: 1.6595 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01313: loss did not improve from 1.65947\n",
      "Epoch 1314/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.6597 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01314: loss did not improve from 1.65947\n",
      "Epoch 1315/2000\n",
      "240/240 [==============================] - 0s 649us/step - loss: 1.6597 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01315: loss did not improve from 1.65947\n",
      "Epoch 1316/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.6596 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01316: loss did not improve from 1.65947\n",
      "Epoch 1317/2000\n",
      "240/240 [==============================] - 0s 632us/step - loss: 1.6601 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01317: loss did not improve from 1.65947\n",
      "Epoch 1318/2000\n",
      "240/240 [==============================] - 0s 647us/step - loss: 1.6597 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01318: loss did not improve from 1.65947\n",
      "Epoch 1319/2000\n",
      "240/240 [==============================] - 0s 646us/step - loss: 1.6597 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01319: loss did not improve from 1.65947\n",
      "Epoch 1320/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 1.6598 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01320: loss did not improve from 1.65947\n",
      "Epoch 1321/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.6596 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01321: loss did not improve from 1.65947\n",
      "Epoch 1322/2000\n",
      "240/240 [==============================] - 0s 775us/step - loss: 1.6608 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01322: loss did not improve from 1.65947\n",
      "Epoch 1323/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 1.6616 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01323: loss did not improve from 1.65947\n",
      "Epoch 1324/2000\n",
      "240/240 [==============================] - 0s 629us/step - loss: 1.6628 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01324: loss did not improve from 1.65947\n",
      "Epoch 1325/2000\n",
      "240/240 [==============================] - 0s 646us/step - loss: 1.6611 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01325: loss did not improve from 1.65947\n",
      "Epoch 1326/2000\n",
      "240/240 [==============================] - 0s 631us/step - loss: 1.6607 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01326: loss did not improve from 1.65947\n",
      "Epoch 1327/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.6600 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01327: loss did not improve from 1.65947\n",
      "Epoch 1328/2000\n",
      "240/240 [==============================] - 0s 600us/step - loss: 1.6598 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01328: loss did not improve from 1.65947\n",
      "Epoch 1329/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.6598 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01329: loss did not improve from 1.65947\n",
      "Epoch 1330/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 1.6597 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01330: loss did not improve from 1.65947\n",
      "Epoch 1331/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.6598 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01331: loss did not improve from 1.65947\n",
      "Epoch 1332/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.6595 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01332: loss improved from 1.65947 to 1.65945, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 1333/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.6596 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01333: loss did not improve from 1.65945\n",
      "Epoch 1334/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6639 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01334: loss did not improve from 1.65945\n",
      "Epoch 1335/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.6776 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01335: loss did not improve from 1.65945\n",
      "Epoch 1336/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 2.0215 - accuracy: 0.3875\n",
      "\n",
      "Epoch 01336: loss did not improve from 1.65945\n",
      "Epoch 1337/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.7396 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01337: loss did not improve from 1.65945\n",
      "Epoch 1338/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.6619 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01338: loss did not improve from 1.65945\n",
      "Epoch 1339/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6609 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01339: loss did not improve from 1.65945\n",
      "Epoch 1340/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.6609 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01340: loss did not improve from 1.65945\n",
      "Epoch 1341/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.6606 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01341: loss did not improve from 1.65945\n",
      "Epoch 1342/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6603 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01342: loss did not improve from 1.65945\n",
      "Epoch 1343/2000\n",
      "240/240 [==============================] - 0s 782us/step - loss: 1.6603 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01343: loss did not improve from 1.65945\n",
      "Epoch 1344/2000\n",
      "240/240 [==============================] - 0s 772us/step - loss: 1.6599 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01344: loss did not improve from 1.65945\n",
      "Epoch 1345/2000\n",
      "240/240 [==============================] - 0s 784us/step - loss: 1.6601 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01345: loss did not improve from 1.65945\n",
      "Epoch 1346/2000\n",
      "240/240 [==============================] - 0s 773us/step - loss: 1.6600 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01346: loss did not improve from 1.65945\n",
      "Epoch 1347/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.6599 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01347: loss did not improve from 1.65945\n",
      "Epoch 1348/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 1.6597 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01348: loss did not improve from 1.65945\n",
      "Epoch 1349/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.6597 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01349: loss did not improve from 1.65945\n",
      "Epoch 1350/2000\n",
      "240/240 [==============================] - 0s 740us/step - loss: 1.6597 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01350: loss did not improve from 1.65945\n",
      "Epoch 1351/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 1.6597 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01351: loss did not improve from 1.65945\n",
      "Epoch 1352/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.6597 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01352: loss did not improve from 1.65945\n",
      "Epoch 1353/2000\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 1.6597 - accuracy: 0.4000\n",
      "\n",
      "Epoch 01353: loss did not improve from 1.65945\n",
      "Epoch 1354/2000\n",
      "240/240 [==============================] - 0s 873us/step - loss: 1.6596 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01354: loss did not improve from 1.65945\n",
      "Epoch 1355/2000\n",
      "240/240 [==============================] - 0s 889us/step - loss: 1.6597 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01355: loss did not improve from 1.65945\n",
      "Epoch 1356/2000\n",
      "240/240 [==============================] - 0s 849us/step - loss: 1.6596 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01356: loss did not improve from 1.65945\n",
      "Epoch 1357/2000\n",
      "240/240 [==============================] - 0s 766us/step - loss: 1.6595 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01357: loss did not improve from 1.65945\n",
      "Epoch 1358/2000\n",
      "240/240 [==============================] - 0s 658us/step - loss: 1.6596 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01358: loss did not improve from 1.65945\n",
      "Epoch 1359/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 651us/step - loss: 1.6595 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01359: loss did not improve from 1.65945\n",
      "Epoch 1360/2000\n",
      "240/240 [==============================] - 0s 628us/step - loss: 1.6596 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01360: loss did not improve from 1.65945\n",
      "Epoch 1361/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.6595 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01361: loss did not improve from 1.65945\n",
      "Epoch 1362/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 1.6596 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01362: loss did not improve from 1.65945\n",
      "Epoch 1363/2000\n",
      "240/240 [==============================] - 0s 672us/step - loss: 1.6595 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01363: loss did not improve from 1.65945\n",
      "Epoch 1364/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 1.6596 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01364: loss did not improve from 1.65945\n",
      "Epoch 1365/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.6595 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01365: loss did not improve from 1.65945\n",
      "Epoch 1366/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 1.6596 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01366: loss did not improve from 1.65945\n",
      "Epoch 1367/2000\n",
      "240/240 [==============================] - 0s 740us/step - loss: 1.6596 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01367: loss did not improve from 1.65945\n",
      "Epoch 1368/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.6596 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01368: loss did not improve from 1.65945\n",
      "Epoch 1369/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6595 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01369: loss did not improve from 1.65945\n",
      "Epoch 1370/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.6597 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01370: loss did not improve from 1.65945\n",
      "Epoch 1371/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6600 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01371: loss did not improve from 1.65945\n",
      "Epoch 1372/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6597 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01372: loss did not improve from 1.65945\n",
      "Epoch 1373/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6639 - accuracy: 0.4000\n",
      "\n",
      "Epoch 01373: loss did not improve from 1.65945\n",
      "Epoch 1374/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.6616 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01374: loss did not improve from 1.65945\n",
      "Epoch 1375/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 1.6603 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01375: loss did not improve from 1.65945\n",
      "Epoch 1376/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6607 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01376: loss did not improve from 1.65945\n",
      "Epoch 1377/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.6605 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01377: loss did not improve from 1.65945\n",
      "Epoch 1378/2000\n",
      "240/240 [==============================] - 0s 845us/step - loss: 1.6598 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01378: loss did not improve from 1.65945\n",
      "Epoch 1379/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6594 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01379: loss improved from 1.65945 to 1.65945, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 1380/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.6596 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01380: loss did not improve from 1.65945\n",
      "Epoch 1381/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 1.6595 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01381: loss did not improve from 1.65945\n",
      "Epoch 1382/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.6595 - accuracy: 0.4000\n",
      "\n",
      "Epoch 01382: loss did not improve from 1.65945\n",
      "Epoch 1383/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.6594 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01383: loss improved from 1.65945 to 1.65939, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 1384/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 1.6595 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01384: loss did not improve from 1.65939\n",
      "Epoch 1385/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 1.6596 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01385: loss did not improve from 1.65939\n",
      "Epoch 1386/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 1.6596 - accuracy: 0.4000\n",
      "\n",
      "Epoch 01386: loss did not improve from 1.65939\n",
      "Epoch 1387/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6595 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01387: loss did not improve from 1.65939\n",
      "Epoch 1388/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.6595 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01388: loss did not improve from 1.65939\n",
      "Epoch 1389/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 1.6596 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01389: loss did not improve from 1.65939\n",
      "Epoch 1390/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.6607 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01390: loss did not improve from 1.65939\n",
      "Epoch 1391/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6614 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01391: loss did not improve from 1.65939\n",
      "Epoch 1392/2000\n",
      "240/240 [==============================] - 0s 794us/step - loss: 1.6789 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01392: loss did not improve from 1.65939\n",
      "Epoch 1393/2000\n",
      "240/240 [==============================] - 0s 827us/step - loss: 1.6624 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01393: loss did not improve from 1.65939\n",
      "Epoch 1394/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.6723 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01394: loss did not improve from 1.65939\n",
      "Epoch 1395/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 1.8557 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01395: loss did not improve from 1.65939\n",
      "Epoch 1396/2000\n",
      "240/240 [==============================] - 0s 893us/step - loss: 1.7097 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01396: loss did not improve from 1.65939\n",
      "Epoch 1397/2000\n",
      "240/240 [==============================] - 0s 722us/step - loss: 1.6609 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01397: loss did not improve from 1.65939\n",
      "Epoch 1398/2000\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 1.6604 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01398: loss did not improve from 1.65939\n",
      "Epoch 1399/2000\n",
      "240/240 [==============================] - 0s 782us/step - loss: 1.6603 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01399: loss did not improve from 1.65939\n",
      "Epoch 1400/2000\n",
      "240/240 [==============================] - 0s 785us/step - loss: 1.6600 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01400: loss did not improve from 1.65939\n",
      "Epoch 1401/2000\n",
      "240/240 [==============================] - 0s 749us/step - loss: 1.6599 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01401: loss did not improve from 1.65939\n",
      "Epoch 1402/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.6598 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01402: loss did not improve from 1.65939\n",
      "Epoch 1403/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.6599 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01403: loss did not improve from 1.65939\n",
      "Epoch 1404/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.6599 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01404: loss did not improve from 1.65939\n",
      "Epoch 1405/2000\n",
      "240/240 [==============================] - 0s 787us/step - loss: 1.6597 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01405: loss did not improve from 1.65939\n",
      "Epoch 1406/2000\n",
      "240/240 [==============================] - 0s 848us/step - loss: 1.6597 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01406: loss did not improve from 1.65939\n",
      "Epoch 1407/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.6598 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01407: loss did not improve from 1.65939\n",
      "Epoch 1408/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.6595 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01408: loss did not improve from 1.65939\n",
      "Epoch 1409/2000\n",
      "240/240 [==============================] - 0s 848us/step - loss: 1.6598 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01409: loss did not improve from 1.65939\n",
      "Epoch 1410/2000\n",
      "240/240 [==============================] - 0s 773us/step - loss: 1.6596 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01410: loss did not improve from 1.65939\n",
      "Epoch 1411/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 1.6598 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01411: loss did not improve from 1.65939\n",
      "Epoch 1412/2000\n",
      "240/240 [==============================] - 0s 749us/step - loss: 1.6597 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01412: loss did not improve from 1.65939\n",
      "Epoch 1413/2000\n",
      "240/240 [==============================] - 0s 740us/step - loss: 1.6597 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01413: loss did not improve from 1.65939\n",
      "Epoch 1414/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.6596 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01414: loss did not improve from 1.65939\n",
      "Epoch 1415/2000\n",
      "240/240 [==============================] - 0s 664us/step - loss: 1.6596 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01415: loss did not improve from 1.65939\n",
      "Epoch 1416/2000\n",
      "240/240 [==============================] - 0s 800us/step - loss: 1.6596 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01416: loss did not improve from 1.65939\n",
      "Epoch 1417/2000\n",
      "240/240 [==============================] - 0s 697us/step - loss: 1.6595 - accuracy: 0.4000\n",
      "\n",
      "Epoch 01417: loss did not improve from 1.65939\n",
      "Epoch 1418/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 1.6594 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01418: loss did not improve from 1.65939\n",
      "Epoch 1419/2000\n",
      "240/240 [==============================] - 0s 752us/step - loss: 1.6595 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01419: loss did not improve from 1.65939\n",
      "Epoch 1420/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 1.6595 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01420: loss did not improve from 1.65939\n",
      "Epoch 1421/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.6594 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01421: loss improved from 1.65939 to 1.65935, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 1422/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.6605 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01422: loss did not improve from 1.65935\n",
      "Epoch 1423/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 1.6595 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01423: loss did not improve from 1.65935\n",
      "Epoch 1424/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.6596 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01424: loss did not improve from 1.65935\n",
      "Epoch 1425/2000\n",
      "240/240 [==============================] - 0s 740us/step - loss: 1.6594 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01425: loss did not improve from 1.65935\n",
      "Epoch 1426/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 1.6593 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01426: loss improved from 1.65935 to 1.65932, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 1427/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.6594 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01427: loss did not improve from 1.65932\n",
      "Epoch 1428/2000\n",
      "240/240 [==============================] - 0s 663us/step - loss: 1.6595 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01428: loss did not improve from 1.65932\n",
      "Epoch 1429/2000\n",
      "240/240 [==============================] - 0s 763us/step - loss: 1.6595 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01429: loss did not improve from 1.65932\n",
      "Epoch 1430/2000\n",
      "240/240 [==============================] - 0s 767us/step - loss: 1.6594 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01430: loss did not improve from 1.65932\n",
      "Epoch 1431/2000\n",
      "240/240 [==============================] - 0s 798us/step - loss: 1.6594 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01431: loss did not improve from 1.65932\n",
      "Epoch 1432/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.6595 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01432: loss did not improve from 1.65932\n",
      "Epoch 1433/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 1.6602 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01433: loss did not improve from 1.65932\n",
      "Epoch 1434/2000\n",
      "240/240 [==============================] - 0s 743us/step - loss: 1.6613 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01434: loss did not improve from 1.65932\n",
      "Epoch 1435/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.6598 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01435: loss did not improve from 1.65932\n",
      "Epoch 1436/2000\n",
      "240/240 [==============================] - 0s 726us/step - loss: 1.6596 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01436: loss did not improve from 1.65932\n",
      "Epoch 1437/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 1.6595 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01437: loss did not improve from 1.65932\n",
      "Epoch 1438/2000\n",
      "240/240 [==============================] - 0s 697us/step - loss: 1.6606 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01438: loss did not improve from 1.65932\n",
      "Epoch 1439/2000\n",
      "240/240 [==============================] - 0s 741us/step - loss: 1.6595 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01439: loss did not improve from 1.65932\n",
      "Epoch 1440/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 1.6596 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01440: loss did not improve from 1.65932\n",
      "Epoch 1441/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 1.6595 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01441: loss did not improve from 1.65932\n",
      "Epoch 1442/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.6594 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01442: loss did not improve from 1.65932\n",
      "Epoch 1443/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 1.6595 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01443: loss did not improve from 1.65932\n",
      "Epoch 1444/2000\n",
      "240/240 [==============================] - 0s 831us/step - loss: 1.6595 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01444: loss did not improve from 1.65932\n",
      "Epoch 1445/2000\n",
      "240/240 [==============================] - 0s 830us/step - loss: 1.6594 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01445: loss did not improve from 1.65932\n",
      "Epoch 1446/2000\n",
      "240/240 [==============================] - 0s 834us/step - loss: 1.6594 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01446: loss did not improve from 1.65932\n",
      "Epoch 1447/2000\n",
      "240/240 [==============================] - 0s 830us/step - loss: 1.6596 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01447: loss did not improve from 1.65932\n",
      "Epoch 1448/2000\n",
      "240/240 [==============================] - 0s 651us/step - loss: 1.6600 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01448: loss did not improve from 1.65932\n",
      "Epoch 1449/2000\n",
      "240/240 [==============================] - 0s 667us/step - loss: 1.6599 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01449: loss did not improve from 1.65932\n",
      "Epoch 1450/2000\n",
      "240/240 [==============================] - 0s 663us/step - loss: 1.6605 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01450: loss did not improve from 1.65932\n",
      "Epoch 1451/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.6595 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01451: loss did not improve from 1.65932\n",
      "Epoch 1452/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 1.6595 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01452: loss did not improve from 1.65932\n",
      "Epoch 1453/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.6591 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01453: loss improved from 1.65932 to 1.65913, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 1454/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.6594 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01454: loss did not improve from 1.65913\n",
      "Epoch 1455/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 1.6610 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01455: loss did not improve from 1.65913\n",
      "Epoch 1456/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.6609 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01456: loss did not improve from 1.65913\n",
      "Epoch 1457/2000\n",
      "240/240 [==============================] - 0s 645us/step - loss: 1.6608 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01457: loss did not improve from 1.65913\n",
      "Epoch 1458/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.6598 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01458: loss did not improve from 1.65913\n",
      "Epoch 1459/2000\n",
      "240/240 [==============================] - 0s 640us/step - loss: 1.6596 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01459: loss did not improve from 1.65913\n",
      "Epoch 1460/2000\n",
      "240/240 [==============================] - 0s 637us/step - loss: 1.6620 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01460: loss did not improve from 1.65913\n",
      "Epoch 1461/2000\n",
      "240/240 [==============================] - 0s 633us/step - loss: 1.6595 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01461: loss did not improve from 1.65913\n",
      "Epoch 1462/2000\n",
      "240/240 [==============================] - 0s 752us/step - loss: 1.6595 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01462: loss did not improve from 1.65913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1463/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 1.6594 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01463: loss did not improve from 1.65913\n",
      "Epoch 1464/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6593 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01464: loss did not improve from 1.65913\n",
      "Epoch 1465/2000\n",
      "240/240 [==============================] - 0s 641us/step - loss: 1.6620 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01465: loss did not improve from 1.65913\n",
      "Epoch 1466/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 1.6617 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01466: loss did not improve from 1.65913\n",
      "Epoch 1467/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 1.6595 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01467: loss did not improve from 1.65913\n",
      "Epoch 1468/2000\n",
      "240/240 [==============================] - 0s 652us/step - loss: 1.6593 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01468: loss did not improve from 1.65913\n",
      "Epoch 1469/2000\n",
      "240/240 [==============================] - 0s 643us/step - loss: 1.6622 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01469: loss did not improve from 1.65913\n",
      "Epoch 1470/2000\n",
      "240/240 [==============================] - 0s 659us/step - loss: 1.6594 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01470: loss did not improve from 1.65913\n",
      "Epoch 1471/2000\n",
      "240/240 [==============================] - 0s 637us/step - loss: 1.6592 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01471: loss did not improve from 1.65913\n",
      "Epoch 1472/2000\n",
      "240/240 [==============================] - 0s 640us/step - loss: 1.6593 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01472: loss did not improve from 1.65913\n",
      "Epoch 1473/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.6602 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01473: loss did not improve from 1.65913\n",
      "Epoch 1474/2000\n",
      "240/240 [==============================] - 0s 640us/step - loss: 1.6593 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01474: loss did not improve from 1.65913\n",
      "Epoch 1475/2000\n",
      "240/240 [==============================] - 0s 651us/step - loss: 1.7538 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01475: loss did not improve from 1.65913\n",
      "Epoch 1476/2000\n",
      "240/240 [==============================] - 0s 645us/step - loss: 1.6937 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01476: loss did not improve from 1.65913\n",
      "Epoch 1477/2000\n",
      "240/240 [==============================] - 0s 639us/step - loss: 1.6616 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01477: loss did not improve from 1.65913\n",
      "Epoch 1478/2000\n",
      "240/240 [==============================] - 0s 650us/step - loss: 1.6599 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01478: loss did not improve from 1.65913\n",
      "Epoch 1479/2000\n",
      "240/240 [==============================] - 0s 642us/step - loss: 1.6595 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01479: loss did not improve from 1.65913\n",
      "Epoch 1480/2000\n",
      "240/240 [==============================] - 0s 652us/step - loss: 1.6593 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01480: loss did not improve from 1.65913\n",
      "Epoch 1481/2000\n",
      "240/240 [==============================] - 0s 662us/step - loss: 1.6592 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01481: loss did not improve from 1.65913\n",
      "Epoch 1482/2000\n",
      "240/240 [==============================] - 0s 643us/step - loss: 1.6594 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01482: loss did not improve from 1.65913\n",
      "Epoch 1483/2000\n",
      "240/240 [==============================] - 0s 663us/step - loss: 1.6591 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01483: loss improved from 1.65913 to 1.65913, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 1484/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 1.6593 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01484: loss did not improve from 1.65913\n",
      "Epoch 1485/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.6594 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01485: loss did not improve from 1.65913\n",
      "Epoch 1486/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 1.6592 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01486: loss did not improve from 1.65913\n",
      "Epoch 1487/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.6593 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01487: loss did not improve from 1.65913\n",
      "Epoch 1488/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6593 - accuracy: 0.4583\n",
      "\n",
      "Epoch 01488: loss did not improve from 1.65913\n",
      "Epoch 1489/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 1.6591 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01489: loss improved from 1.65913 to 1.65907, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 1490/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.6594 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01490: loss did not improve from 1.65907\n",
      "Epoch 1491/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 1.6594 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01491: loss did not improve from 1.65907\n",
      "Epoch 1492/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.6593 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01492: loss did not improve from 1.65907\n",
      "Epoch 1493/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.6595 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01493: loss did not improve from 1.65907\n",
      "Epoch 1494/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.6594 - accuracy: 0.4000\n",
      "\n",
      "Epoch 01494: loss did not improve from 1.65907\n",
      "Epoch 1495/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 1.6594 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01495: loss did not improve from 1.65907\n",
      "Epoch 1496/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.6594 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01496: loss did not improve from 1.65907\n",
      "Epoch 1497/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.6593 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01497: loss did not improve from 1.65907\n",
      "Epoch 1498/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.6597 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01498: loss did not improve from 1.65907\n",
      "Epoch 1499/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.6592 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01499: loss did not improve from 1.65907\n",
      "Epoch 1500/2000\n",
      "240/240 [==============================] - 0s 737us/step - loss: 1.6593 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01500: loss did not improve from 1.65907\n",
      "Epoch 1501/2000\n",
      "240/240 [==============================] - 0s 769us/step - loss: 1.6593 - accuracy: 0.4000\n",
      "\n",
      "Epoch 01501: loss did not improve from 1.65907\n",
      "Epoch 1502/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.6593 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01502: loss did not improve from 1.65907\n",
      "Epoch 1503/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.6596 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01503: loss did not improve from 1.65907\n",
      "Epoch 1504/2000\n",
      "240/240 [==============================] - 0s 740us/step - loss: 1.6594 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01504: loss did not improve from 1.65907\n",
      "Epoch 1505/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 1.6592 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01505: loss did not improve from 1.65907\n",
      "Epoch 1506/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 1.6594 - accuracy: 0.4000\n",
      "\n",
      "Epoch 01506: loss did not improve from 1.65907\n",
      "Epoch 1507/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 1.6602 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01507: loss did not improve from 1.65907\n",
      "Epoch 1508/2000\n",
      "240/240 [==============================] - 0s 757us/step - loss: 1.6596 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01508: loss did not improve from 1.65907\n",
      "Epoch 1509/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.6599 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01509: loss did not improve from 1.65907\n",
      "Epoch 1510/2000\n",
      "240/240 [==============================] - 0s 781us/step - loss: 1.6593 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01510: loss did not improve from 1.65907\n",
      "Epoch 1511/2000\n",
      "240/240 [==============================] - 0s 752us/step - loss: 1.6592 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01511: loss did not improve from 1.65907\n",
      "Epoch 1512/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.6593 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01512: loss did not improve from 1.65907\n",
      "Epoch 1513/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.6593 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01513: loss did not improve from 1.65907\n",
      "Epoch 1514/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 1.6591 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01514: loss did not improve from 1.65907\n",
      "Epoch 1515/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.6597 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01515: loss did not improve from 1.65907\n",
      "Epoch 1516/2000\n",
      "240/240 [==============================] - 0s 744us/step - loss: 1.6599 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01516: loss did not improve from 1.65907\n",
      "Epoch 1517/2000\n",
      "240/240 [==============================] - 0s 760us/step - loss: 1.6595 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01517: loss did not improve from 1.65907\n",
      "Epoch 1518/2000\n",
      "240/240 [==============================] - 0s 757us/step - loss: 1.6595 - accuracy: 0.4000\n",
      "\n",
      "Epoch 01518: loss did not improve from 1.65907\n",
      "Epoch 1519/2000\n",
      "240/240 [==============================] - 0s 738us/step - loss: 1.6593 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01519: loss did not improve from 1.65907\n",
      "Epoch 1520/2000\n",
      "240/240 [==============================] - 0s 756us/step - loss: 1.6593 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01520: loss did not improve from 1.65907\n",
      "Epoch 1521/2000\n",
      "240/240 [==============================] - 0s 738us/step - loss: 1.6591 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01521: loss did not improve from 1.65907\n",
      "Epoch 1522/2000\n",
      "240/240 [==============================] - 0s 732us/step - loss: 1.6593 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01522: loss did not improve from 1.65907\n",
      "Epoch 1523/2000\n",
      "240/240 [==============================] - 0s 746us/step - loss: 1.6593 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01523: loss did not improve from 1.65907\n",
      "Epoch 1524/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.6593 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01524: loss did not improve from 1.65907\n",
      "Epoch 1525/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6592 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01525: loss did not improve from 1.65907\n",
      "Epoch 1526/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6599 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01526: loss did not improve from 1.65907\n",
      "Epoch 1527/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.6595 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01527: loss did not improve from 1.65907\n",
      "Epoch 1528/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.6594 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01528: loss did not improve from 1.65907\n",
      "Epoch 1529/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.6593 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01529: loss did not improve from 1.65907\n",
      "Epoch 1530/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 1.6595 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01530: loss did not improve from 1.65907\n",
      "Epoch 1531/2000\n",
      "240/240 [==============================] - 0s 756us/step - loss: 1.6595 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01531: loss did not improve from 1.65907\n",
      "Epoch 1532/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.6593 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01532: loss did not improve from 1.65907\n",
      "Epoch 1533/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.6596 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01533: loss did not improve from 1.65907\n",
      "Epoch 1534/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6595 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01534: loss did not improve from 1.65907\n",
      "Epoch 1535/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6594 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01535: loss did not improve from 1.65907\n",
      "Epoch 1536/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.6593 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01536: loss did not improve from 1.65907\n",
      "Epoch 1537/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.6595 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01537: loss did not improve from 1.65907\n",
      "Epoch 1538/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.6594 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01538: loss did not improve from 1.65907\n",
      "Epoch 1539/2000\n",
      "240/240 [==============================] - 0s 795us/step - loss: 1.6597 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01539: loss did not improve from 1.65907\n",
      "Epoch 1540/2000\n",
      "240/240 [==============================] - 0s 848us/step - loss: 1.6599 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01540: loss did not improve from 1.65907\n",
      "Epoch 1541/2000\n",
      "240/240 [==============================] - 0s 799us/step - loss: 1.6593 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01541: loss did not improve from 1.65907\n",
      "Epoch 1542/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.6592 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01542: loss did not improve from 1.65907\n",
      "Epoch 1543/2000\n",
      "240/240 [==============================] - 0s 635us/step - loss: 1.6594 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01543: loss did not improve from 1.65907\n",
      "Epoch 1544/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.6650 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01544: loss did not improve from 1.65907\n",
      "Epoch 1545/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.6600 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01545: loss did not improve from 1.65907\n",
      "Epoch 1546/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.6594 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01546: loss did not improve from 1.65907\n",
      "Epoch 1547/2000\n",
      "240/240 [==============================] - 0s 641us/step - loss: 1.6596 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01547: loss did not improve from 1.65907\n",
      "Epoch 1548/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.6594 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01548: loss did not improve from 1.65907\n",
      "Epoch 1549/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.6593 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01549: loss did not improve from 1.65907\n",
      "Epoch 1550/2000\n",
      "240/240 [==============================] - 0s 646us/step - loss: 1.6592 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01550: loss did not improve from 1.65907\n",
      "Epoch 1551/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.6593 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01551: loss did not improve from 1.65907\n",
      "Epoch 1552/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.6592 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01552: loss did not improve from 1.65907\n",
      "Epoch 1553/2000\n",
      "240/240 [==============================] - 0s 651us/step - loss: 1.6603 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01553: loss did not improve from 1.65907\n",
      "Epoch 1554/2000\n",
      "240/240 [==============================] - 0s 645us/step - loss: 1.6606 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01554: loss did not improve from 1.65907\n",
      "Epoch 1555/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 1.6606 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01555: loss did not improve from 1.65907\n",
      "Epoch 1556/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.6594 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01556: loss did not improve from 1.65907\n",
      "Epoch 1557/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.6593 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01557: loss did not improve from 1.65907\n",
      "Epoch 1558/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.6594 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01558: loss did not improve from 1.65907\n",
      "Epoch 1559/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.6592 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01559: loss did not improve from 1.65907\n",
      "Epoch 1560/2000\n",
      "240/240 [==============================] - 0s 639us/step - loss: 1.6594 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01560: loss did not improve from 1.65907\n",
      "Epoch 1561/2000\n",
      "240/240 [==============================] - 0s 624us/step - loss: 1.6593 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01561: loss did not improve from 1.65907\n",
      "Epoch 1562/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.6593 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01562: loss did not improve from 1.65907\n",
      "Epoch 1563/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.6593 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01563: loss did not improve from 1.65907\n",
      "Epoch 1564/2000\n",
      "240/240 [==============================] - 0s 651us/step - loss: 1.6593 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01564: loss did not improve from 1.65907\n",
      "Epoch 1565/2000\n",
      "240/240 [==============================] - 0s 629us/step - loss: 1.6594 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01565: loss did not improve from 1.65907\n",
      "Epoch 1566/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.6592 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01566: loss did not improve from 1.65907\n",
      "Epoch 1567/2000\n",
      "240/240 [==============================] - 0s 628us/step - loss: 1.6594 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01567: loss did not improve from 1.65907\n",
      "Epoch 1568/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 634us/step - loss: 1.6594 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01568: loss did not improve from 1.65907\n",
      "Epoch 1569/2000\n",
      "240/240 [==============================] - 0s 624us/step - loss: 1.6594 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01569: loss did not improve from 1.65907\n",
      "Epoch 1570/2000\n",
      "240/240 [==============================] - 0s 642us/step - loss: 1.6594 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01570: loss did not improve from 1.65907\n",
      "Epoch 1571/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.6592 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01571: loss did not improve from 1.65907\n",
      "Epoch 1572/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6592 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01572: loss did not improve from 1.65907\n",
      "Epoch 1573/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 1.6592 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01573: loss did not improve from 1.65907\n",
      "Epoch 1574/2000\n",
      "240/240 [==============================] - 0s 742us/step - loss: 1.6596 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01574: loss did not improve from 1.65907\n",
      "Epoch 1575/2000\n",
      "240/240 [==============================] - 0s 852us/step - loss: 1.6606 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01575: loss did not improve from 1.65907\n",
      "Epoch 1576/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.6594 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01576: loss did not improve from 1.65907\n",
      "Epoch 1577/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.6593 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01577: loss did not improve from 1.65907\n",
      "Epoch 1578/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6599 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01578: loss did not improve from 1.65907\n",
      "Epoch 1579/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6593 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01579: loss did not improve from 1.65907\n",
      "Epoch 1580/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6591 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01580: loss did not improve from 1.65907\n",
      "Epoch 1581/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6594 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01581: loss did not improve from 1.65907\n",
      "Epoch 1582/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6592 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01582: loss did not improve from 1.65907\n",
      "Epoch 1583/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 1.6596 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01583: loss did not improve from 1.65907\n",
      "Epoch 1584/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.6596 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01584: loss did not improve from 1.65907\n",
      "Epoch 1585/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.6593 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01585: loss did not improve from 1.65907\n",
      "Epoch 1586/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.6593 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01586: loss did not improve from 1.65907\n",
      "Epoch 1587/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.6592 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01587: loss did not improve from 1.65907\n",
      "Epoch 1588/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 1.6593 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01588: loss did not improve from 1.65907\n",
      "Epoch 1589/2000\n",
      "240/240 [==============================] - 0s 873us/step - loss: 1.6618 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01589: loss did not improve from 1.65907\n",
      "Epoch 1590/2000\n",
      "240/240 [==============================] - 0s 726us/step - loss: 1.6597 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01590: loss did not improve from 1.65907\n",
      "Epoch 1591/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.6595 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01591: loss did not improve from 1.65907\n",
      "Epoch 1592/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6754 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01592: loss did not improve from 1.65907\n",
      "Epoch 1593/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6822 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01593: loss did not improve from 1.65907\n",
      "Epoch 1594/2000\n",
      "240/240 [==============================] - 0s 768us/step - loss: 1.6609 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01594: loss did not improve from 1.65907\n",
      "Epoch 1595/2000\n",
      "240/240 [==============================] - 0s 652us/step - loss: 1.6594 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01595: loss did not improve from 1.65907\n",
      "Epoch 1596/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 1.6596 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01596: loss did not improve from 1.65907\n",
      "Epoch 1597/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.6595 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01597: loss did not improve from 1.65907\n",
      "Epoch 1598/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.6593 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01598: loss did not improve from 1.65907\n",
      "Epoch 1599/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.6632 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01599: loss did not improve from 1.65907\n",
      "Epoch 1600/2000\n",
      "240/240 [==============================] - 0s 642us/step - loss: 1.6595 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01600: loss did not improve from 1.65907\n",
      "Epoch 1601/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.6593 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01601: loss did not improve from 1.65907\n",
      "Epoch 1602/2000\n",
      "240/240 [==============================] - 0s 623us/step - loss: 1.6747 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01602: loss did not improve from 1.65907\n",
      "Epoch 1603/2000\n",
      "240/240 [==============================] - 0s 633us/step - loss: 1.6609 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01603: loss did not improve from 1.65907\n",
      "Epoch 1604/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.6597 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01604: loss did not improve from 1.65907\n",
      "Epoch 1605/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.6595 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01605: loss did not improve from 1.65907\n",
      "Epoch 1606/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 1.6593 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01606: loss did not improve from 1.65907\n",
      "Epoch 1607/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.6595 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01607: loss did not improve from 1.65907\n",
      "Epoch 1608/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6595 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01608: loss did not improve from 1.65907\n",
      "Epoch 1609/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.6593 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01609: loss did not improve from 1.65907\n",
      "Epoch 1610/2000\n",
      "240/240 [==============================] - 0s 734us/step - loss: 1.6595 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01610: loss did not improve from 1.65907\n",
      "Epoch 1611/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.6597 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01611: loss did not improve from 1.65907\n",
      "Epoch 1612/2000\n",
      "240/240 [==============================] - 0s 752us/step - loss: 1.6594 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01612: loss did not improve from 1.65907\n",
      "Epoch 1613/2000\n",
      "240/240 [==============================] - 0s 755us/step - loss: 1.6593 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01613: loss did not improve from 1.65907\n",
      "Epoch 1614/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.8289 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01614: loss did not improve from 1.65907\n",
      "Epoch 1615/2000\n",
      "240/240 [==============================] - 0s 732us/step - loss: 2.0354 - accuracy: 0.3917\n",
      "\n",
      "Epoch 01615: loss did not improve from 1.65907\n",
      "Epoch 1616/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.6882 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01616: loss did not improve from 1.65907\n",
      "Epoch 1617/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6789 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01617: loss did not improve from 1.65907\n",
      "Epoch 1618/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 1.6609 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01618: loss did not improve from 1.65907\n",
      "Epoch 1619/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6601 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01619: loss did not improve from 1.65907\n",
      "Epoch 1620/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 1.6598 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01620: loss did not improve from 1.65907\n",
      "Epoch 1621/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 706us/step - loss: 1.6596 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01621: loss did not improve from 1.65907\n",
      "Epoch 1622/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6596 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01622: loss did not improve from 1.65907\n",
      "Epoch 1623/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6595 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01623: loss did not improve from 1.65907\n",
      "Epoch 1624/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6594 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01624: loss did not improve from 1.65907\n",
      "Epoch 1625/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.6594 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01625: loss did not improve from 1.65907\n",
      "Epoch 1626/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.6592 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01626: loss did not improve from 1.65907\n",
      "Epoch 1627/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.6594 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01627: loss did not improve from 1.65907\n",
      "Epoch 1628/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 1.6591 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01628: loss improved from 1.65907 to 1.65907, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 1629/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6594 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01629: loss did not improve from 1.65907\n",
      "Epoch 1630/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6593 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01630: loss did not improve from 1.65907\n",
      "Epoch 1631/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.6592 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01631: loss did not improve from 1.65907\n",
      "Epoch 1632/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 1.6592 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01632: loss did not improve from 1.65907\n",
      "Epoch 1633/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6593 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01633: loss did not improve from 1.65907\n",
      "Epoch 1634/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6592 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01634: loss did not improve from 1.65907\n",
      "Epoch 1635/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.6592 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01635: loss did not improve from 1.65907\n",
      "Epoch 1636/2000\n",
      "240/240 [==============================] - 0s 830us/step - loss: 1.6591 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01636: loss did not improve from 1.65907\n",
      "Epoch 1637/2000\n",
      "240/240 [==============================] - 0s 843us/step - loss: 1.6591 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01637: loss did not improve from 1.65907\n",
      "Epoch 1638/2000\n",
      "240/240 [==============================] - 0s 855us/step - loss: 1.6592 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01638: loss did not improve from 1.65907\n",
      "Epoch 1639/2000\n",
      "240/240 [==============================] - 0s 744us/step - loss: 1.6592 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01639: loss did not improve from 1.65907\n",
      "Epoch 1640/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.6593 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01640: loss did not improve from 1.65907\n",
      "Epoch 1641/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 1.6593 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01641: loss did not improve from 1.65907\n",
      "Epoch 1642/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6592 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01642: loss did not improve from 1.65907\n",
      "Epoch 1643/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 1.6592 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01643: loss did not improve from 1.65907\n",
      "Epoch 1644/2000\n",
      "240/240 [==============================] - 0s 662us/step - loss: 1.6591 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01644: loss did not improve from 1.65907\n",
      "Epoch 1645/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.6592 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01645: loss did not improve from 1.65907\n",
      "Epoch 1646/2000\n",
      "240/240 [==============================] - 0s 601us/step - loss: 1.6593 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01646: loss did not improve from 1.65907\n",
      "Epoch 1647/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6592 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01647: loss did not improve from 1.65907\n",
      "Epoch 1648/2000\n",
      "240/240 [==============================] - 0s 658us/step - loss: 1.6592 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01648: loss did not improve from 1.65907\n",
      "Epoch 1649/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 1.6591 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01649: loss improved from 1.65907 to 1.65906, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 1650/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.6591 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01650: loss did not improve from 1.65906\n",
      "Epoch 1651/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.6594 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01651: loss did not improve from 1.65906\n",
      "Epoch 1652/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 1.6591 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01652: loss did not improve from 1.65906\n",
      "Epoch 1653/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 1.6591 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01653: loss did not improve from 1.65906\n",
      "Epoch 1654/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6593 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01654: loss did not improve from 1.65906\n",
      "Epoch 1655/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.6592 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01655: loss did not improve from 1.65906\n",
      "Epoch 1656/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.6600 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01656: loss did not improve from 1.65906\n",
      "Epoch 1657/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6618 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01657: loss did not improve from 1.65906\n",
      "Epoch 1658/2000\n",
      "240/240 [==============================] - 0s 738us/step - loss: 1.6607 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01658: loss did not improve from 1.65906\n",
      "Epoch 1659/2000\n",
      "240/240 [==============================] - 0s 726us/step - loss: 1.6600 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01659: loss did not improve from 1.65906\n",
      "Epoch 1660/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.6595 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01660: loss did not improve from 1.65906\n",
      "Epoch 1661/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6594 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01661: loss did not improve from 1.65906\n",
      "Epoch 1662/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6593 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01662: loss did not improve from 1.65906\n",
      "Epoch 1663/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.6593 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01663: loss did not improve from 1.65906\n",
      "Epoch 1664/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6592 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01664: loss did not improve from 1.65906\n",
      "Epoch 1665/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6592 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01665: loss did not improve from 1.65906\n",
      "Epoch 1666/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6592 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01666: loss did not improve from 1.65906\n",
      "Epoch 1667/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6593 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01667: loss did not improve from 1.65906\n",
      "Epoch 1668/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6592 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01668: loss did not improve from 1.65906\n",
      "Epoch 1669/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.6592 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01669: loss did not improve from 1.65906\n",
      "Epoch 1670/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6593 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01670: loss did not improve from 1.65906\n",
      "Epoch 1671/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6594 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01671: loss did not improve from 1.65906\n",
      "Epoch 1672/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.6592 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01672: loss did not improve from 1.65906\n",
      "Epoch 1673/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.6591 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01673: loss did not improve from 1.65906\n",
      "Epoch 1674/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6592 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01674: loss did not improve from 1.65906\n",
      "Epoch 1675/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.6596 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01675: loss did not improve from 1.65906\n",
      "Epoch 1676/2000\n",
      "240/240 [==============================] - 0s 741us/step - loss: 1.6593 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01676: loss did not improve from 1.65906\n",
      "Epoch 1677/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.6601 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01677: loss did not improve from 1.65906\n",
      "Epoch 1678/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 1.6621 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01678: loss did not improve from 1.65906\n",
      "Epoch 1679/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.6628 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01679: loss did not improve from 1.65906\n",
      "Epoch 1680/2000\n",
      "240/240 [==============================] - 0s 718us/step - loss: 1.6601 - accuracy: 0.3958\n",
      "\n",
      "Epoch 01680: loss did not improve from 1.65906\n",
      "Epoch 1681/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 1.6594 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01681: loss did not improve from 1.65906\n",
      "Epoch 1682/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 1.6593 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01682: loss did not improve from 1.65906\n",
      "Epoch 1683/2000\n",
      "240/240 [==============================] - 0s 750us/step - loss: 1.6593 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01683: loss did not improve from 1.65906\n",
      "Epoch 1684/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.6593 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01684: loss did not improve from 1.65906\n",
      "Epoch 1685/2000\n",
      "240/240 [==============================] - 0s 739us/step - loss: 1.6592 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01685: loss did not improve from 1.65906\n",
      "Epoch 1686/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6595 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01686: loss did not improve from 1.65906\n",
      "Epoch 1687/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.6594 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01687: loss did not improve from 1.65906\n",
      "Epoch 1688/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 1.6594 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01688: loss did not improve from 1.65906\n",
      "Epoch 1689/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.6595 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01689: loss did not improve from 1.65906\n",
      "Epoch 1690/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6593 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01690: loss did not improve from 1.65906\n",
      "Epoch 1691/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6594 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01691: loss did not improve from 1.65906\n",
      "Epoch 1692/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6593 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01692: loss did not improve from 1.65906\n",
      "Epoch 1693/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6592 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01693: loss did not improve from 1.65906\n",
      "Epoch 1694/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 1.6594 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01694: loss did not improve from 1.65906\n",
      "Epoch 1695/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 1.6594 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01695: loss did not improve from 1.65906\n",
      "Epoch 1696/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 1.6593 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01696: loss did not improve from 1.65906\n",
      "Epoch 1697/2000\n",
      "240/240 [==============================] - 0s 738us/step - loss: 1.6595 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01697: loss did not improve from 1.65906\n",
      "Epoch 1698/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.6594 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01698: loss did not improve from 1.65906\n",
      "Epoch 1699/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6593 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01699: loss did not improve from 1.65906\n",
      "Epoch 1700/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6591 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01700: loss did not improve from 1.65906\n",
      "Epoch 1701/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6594 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01701: loss did not improve from 1.65906\n",
      "Epoch 1702/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6604 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01702: loss did not improve from 1.65906\n",
      "Epoch 1703/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.6600 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01703: loss did not improve from 1.65906\n",
      "Epoch 1704/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.6601 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01704: loss did not improve from 1.65906\n",
      "Epoch 1705/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.6603 - accuracy: 0.4000\n",
      "\n",
      "Epoch 01705: loss did not improve from 1.65906\n",
      "Epoch 1706/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.6595 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01706: loss did not improve from 1.65906\n",
      "Epoch 1707/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6593 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01707: loss did not improve from 1.65906\n",
      "Epoch 1708/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.6592 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01708: loss did not improve from 1.65906\n",
      "Epoch 1709/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.6592 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01709: loss did not improve from 1.65906\n",
      "Epoch 1710/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6590 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01710: loss improved from 1.65906 to 1.65905, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 1711/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6591 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01711: loss did not improve from 1.65905\n",
      "Epoch 1712/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.6591 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01712: loss did not improve from 1.65905\n",
      "Epoch 1713/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6593 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01713: loss did not improve from 1.65905\n",
      "Epoch 1714/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6683 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01714: loss did not improve from 1.65905\n",
      "Epoch 1715/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.6605 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01715: loss did not improve from 1.65905\n",
      "Epoch 1716/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6598 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01716: loss did not improve from 1.65905\n",
      "Epoch 1717/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.6591 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01717: loss did not improve from 1.65905\n",
      "Epoch 1718/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 1.6592 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01718: loss did not improve from 1.65905\n",
      "Epoch 1719/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.8211 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01719: loss did not improve from 1.65905\n",
      "Epoch 1720/2000\n",
      "240/240 [==============================] - 0s 697us/step - loss: 1.7747 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01720: loss did not improve from 1.65905\n",
      "Epoch 1721/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.7247 - accuracy: 0.4000\n",
      "\n",
      "Epoch 01721: loss did not improve from 1.65905\n",
      "Epoch 1722/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6621 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01722: loss did not improve from 1.65905\n",
      "Epoch 1723/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6609 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01723: loss did not improve from 1.65905\n",
      "Epoch 1724/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6605 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01724: loss did not improve from 1.65905\n",
      "Epoch 1725/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 691us/step - loss: 1.6602 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01725: loss did not improve from 1.65905\n",
      "Epoch 1726/2000\n",
      "240/240 [==============================] - 0s 757us/step - loss: 1.6601 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01726: loss did not improve from 1.65905\n",
      "Epoch 1727/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.6599 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01727: loss did not improve from 1.65905\n",
      "Epoch 1728/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.6597 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01728: loss did not improve from 1.65905\n",
      "Epoch 1729/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6596 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01729: loss did not improve from 1.65905\n",
      "Epoch 1730/2000\n",
      "240/240 [==============================] - 0s 811us/step - loss: 1.6594 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01730: loss did not improve from 1.65905\n",
      "Epoch 1731/2000\n",
      "240/240 [==============================] - 0s 789us/step - loss: 1.6592 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01731: loss did not improve from 1.65905\n",
      "Epoch 1732/2000\n",
      "240/240 [==============================] - 0s 794us/step - loss: 1.6591 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01732: loss did not improve from 1.65905\n",
      "Epoch 1733/2000\n",
      "240/240 [==============================] - 0s 763us/step - loss: 1.6591 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01733: loss did not improve from 1.65905\n",
      "Epoch 1734/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 1.6591 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01734: loss did not improve from 1.65905\n",
      "Epoch 1735/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.6592 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01735: loss did not improve from 1.65905\n",
      "Epoch 1736/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.6590 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01736: loss improved from 1.65905 to 1.65900, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 1737/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 1.6590 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01737: loss did not improve from 1.65900\n",
      "Epoch 1738/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.6592 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01738: loss did not improve from 1.65900\n",
      "Epoch 1739/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.6592 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01739: loss did not improve from 1.65900\n",
      "Epoch 1740/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.6592 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01740: loss did not improve from 1.65900\n",
      "Epoch 1741/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 1.6591 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01741: loss did not improve from 1.65900\n",
      "Epoch 1742/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 1.6591 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01742: loss did not improve from 1.65900\n",
      "Epoch 1743/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 1.6590 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01743: loss improved from 1.65900 to 1.65900, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 1744/2000\n",
      "240/240 [==============================] - 0s 739us/step - loss: 1.6591 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01744: loss did not improve from 1.65900\n",
      "Epoch 1745/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.6591 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01745: loss did not improve from 1.65900\n",
      "Epoch 1746/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.6592 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01746: loss did not improve from 1.65900\n",
      "Epoch 1747/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6592 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01747: loss did not improve from 1.65900\n",
      "Epoch 1748/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.6590 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01748: loss did not improve from 1.65900\n",
      "Epoch 1749/2000\n",
      "240/240 [==============================] - 0s 744us/step - loss: 1.6591 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01749: loss did not improve from 1.65900\n",
      "Epoch 1750/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.6591 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01750: loss did not improve from 1.65900\n",
      "Epoch 1751/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6592 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01751: loss did not improve from 1.65900\n",
      "Epoch 1752/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6592 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01752: loss did not improve from 1.65900\n",
      "Epoch 1753/2000\n",
      "240/240 [==============================] - 0s 741us/step - loss: 1.6592 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01753: loss did not improve from 1.65900\n",
      "Epoch 1754/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 1.6590 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01754: loss did not improve from 1.65900\n",
      "Epoch 1755/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6596 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01755: loss did not improve from 1.65900\n",
      "Epoch 1756/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 1.6604 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01756: loss did not improve from 1.65900\n",
      "Epoch 1757/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6596 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01757: loss did not improve from 1.65900\n",
      "Epoch 1758/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.6593 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01758: loss did not improve from 1.65900\n",
      "Epoch 1759/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.6594 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01759: loss did not improve from 1.65900\n",
      "Epoch 1760/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.6592 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01760: loss did not improve from 1.65900\n",
      "Epoch 1761/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 1.6600 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01761: loss did not improve from 1.65900\n",
      "Epoch 1762/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.6738 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01762: loss did not improve from 1.65900\n",
      "Epoch 1763/2000\n",
      "240/240 [==============================] - 0s 767us/step - loss: 1.6603 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01763: loss did not improve from 1.65900\n",
      "Epoch 1764/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.6600 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01764: loss did not improve from 1.65900\n",
      "Epoch 1765/2000\n",
      "240/240 [==============================] - 0s 744us/step - loss: 1.6595 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01765: loss did not improve from 1.65900\n",
      "Epoch 1766/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 1.6593 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01766: loss did not improve from 1.65900\n",
      "Epoch 1767/2000\n",
      "240/240 [==============================] - 0s 759us/step - loss: 1.6592 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01767: loss did not improve from 1.65900\n",
      "Epoch 1768/2000\n",
      "240/240 [==============================] - 0s 721us/step - loss: 1.6593 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01768: loss did not improve from 1.65900\n",
      "Epoch 1769/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.6595 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01769: loss did not improve from 1.65900\n",
      "Epoch 1770/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.6592 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01770: loss did not improve from 1.65900\n",
      "Epoch 1771/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 1.6593 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01771: loss did not improve from 1.65900\n",
      "Epoch 1772/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6591 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01772: loss did not improve from 1.65900\n",
      "Epoch 1773/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.6591 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01773: loss did not improve from 1.65900\n",
      "Epoch 1774/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.6591 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01774: loss did not improve from 1.65900\n",
      "Epoch 1775/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 1.6591 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01775: loss did not improve from 1.65900\n",
      "Epoch 1776/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 1.6591 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01776: loss did not improve from 1.65900\n",
      "Epoch 1777/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.6591 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01777: loss did not improve from 1.65900\n",
      "Epoch 1778/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.6592 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01778: loss did not improve from 1.65900\n",
      "Epoch 1779/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.6591 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01779: loss did not improve from 1.65900\n",
      "Epoch 1780/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6591 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01780: loss did not improve from 1.65900\n",
      "Epoch 1781/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 1.6603 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01781: loss did not improve from 1.65900\n",
      "Epoch 1782/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.6613 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01782: loss did not improve from 1.65900\n",
      "Epoch 1783/2000\n",
      "240/240 [==============================] - 0s 718us/step - loss: 1.6598 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01783: loss did not improve from 1.65900\n",
      "Epoch 1784/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 1.6597 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01784: loss did not improve from 1.65900\n",
      "Epoch 1785/2000\n",
      "240/240 [==============================] - 0s 741us/step - loss: 1.6594 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01785: loss did not improve from 1.65900\n",
      "Epoch 1786/2000\n",
      "240/240 [==============================] - 0s 752us/step - loss: 1.6594 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01786: loss did not improve from 1.65900\n",
      "Epoch 1787/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 1.6590 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01787: loss improved from 1.65900 to 1.65897, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 1788/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.8377 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01788: loss did not improve from 1.65897\n",
      "Epoch 1789/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.6617 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01789: loss did not improve from 1.65897\n",
      "Epoch 1790/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6602 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01790: loss did not improve from 1.65897\n",
      "Epoch 1791/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 1.6599 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01791: loss did not improve from 1.65897\n",
      "Epoch 1792/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6595 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01792: loss did not improve from 1.65897\n",
      "Epoch 1793/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.6594 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01793: loss did not improve from 1.65897\n",
      "Epoch 1794/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.6592 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01794: loss did not improve from 1.65897\n",
      "Epoch 1795/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6594 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01795: loss did not improve from 1.65897\n",
      "Epoch 1796/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.6592 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01796: loss did not improve from 1.65897\n",
      "Epoch 1797/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.6593 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01797: loss did not improve from 1.65897\n",
      "Epoch 1798/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.6593 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01798: loss did not improve from 1.65897\n",
      "Epoch 1799/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.6591 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01799: loss did not improve from 1.65897\n",
      "Epoch 1800/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6592 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01800: loss did not improve from 1.65897\n",
      "Epoch 1801/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.6593 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01801: loss did not improve from 1.65897\n",
      "Epoch 1802/2000\n",
      "240/240 [==============================] - 0s 631us/step - loss: 1.6593 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01802: loss did not improve from 1.65897\n",
      "Epoch 1803/2000\n",
      "240/240 [==============================] - 0s 739us/step - loss: 1.6594 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01803: loss did not improve from 1.65897\n",
      "Epoch 1804/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6593 - accuracy: 0.3958\n",
      "\n",
      "Epoch 01804: loss did not improve from 1.65897\n",
      "Epoch 1805/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 1.6592 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01805: loss did not improve from 1.65897\n",
      "Epoch 1806/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 1.6591 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01806: loss did not improve from 1.65897\n",
      "Epoch 1807/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.6593 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01807: loss did not improve from 1.65897\n",
      "Epoch 1808/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6592 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01808: loss did not improve from 1.65897\n",
      "Epoch 1809/2000\n",
      "240/240 [==============================] - 0s 631us/step - loss: 1.6594 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01809: loss did not improve from 1.65897\n",
      "Epoch 1810/2000\n",
      "240/240 [==============================] - 0s 740us/step - loss: 1.6593 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01810: loss did not improve from 1.65897\n",
      "Epoch 1811/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 1.6593 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01811: loss did not improve from 1.65897\n",
      "Epoch 1812/2000\n",
      "240/240 [==============================] - 0s 663us/step - loss: 1.6592 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01812: loss did not improve from 1.65897\n",
      "Epoch 1813/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 1.6592 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01813: loss did not improve from 1.65897\n",
      "Epoch 1814/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.6592 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01814: loss did not improve from 1.65897\n",
      "Epoch 1815/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.6639 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01815: loss did not improve from 1.65897\n",
      "Epoch 1816/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.6607 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01816: loss did not improve from 1.65897\n",
      "Epoch 1817/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.6599 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01817: loss did not improve from 1.65897\n",
      "Epoch 1818/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 1.6593 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01818: loss did not improve from 1.65897\n",
      "Epoch 1819/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.6594 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01819: loss did not improve from 1.65897\n",
      "Epoch 1820/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6631 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01820: loss did not improve from 1.65897\n",
      "Epoch 1821/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.6605 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01821: loss did not improve from 1.65897\n",
      "Epoch 1822/2000\n",
      "240/240 [==============================] - 0s 627us/step - loss: 1.6598 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01822: loss did not improve from 1.65897\n",
      "Epoch 1823/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.6594 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01823: loss did not improve from 1.65897\n",
      "Epoch 1824/2000\n",
      "240/240 [==============================] - 0s 841us/step - loss: 1.6592 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01824: loss did not improve from 1.65897\n",
      "Epoch 1825/2000\n",
      "240/240 [==============================] - 0s 819us/step - loss: 1.6591 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01825: loss did not improve from 1.65897\n",
      "Epoch 1826/2000\n",
      "240/240 [==============================] - 0s 795us/step - loss: 1.6591 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01826: loss did not improve from 1.65897\n",
      "Epoch 1827/2000\n",
      "240/240 [==============================] - 0s 790us/step - loss: 1.6592 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01827: loss did not improve from 1.65897\n",
      "Epoch 1828/2000\n",
      "240/240 [==============================] - 0s 655us/step - loss: 1.6590 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01828: loss did not improve from 1.65897\n",
      "Epoch 1829/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 622us/step - loss: 1.6590 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01829: loss did not improve from 1.65897\n",
      "Epoch 1830/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 1.6591 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01830: loss did not improve from 1.65897\n",
      "Epoch 1831/2000\n",
      "240/240 [==============================] - 0s 613us/step - loss: 1.6591 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01831: loss did not improve from 1.65897\n",
      "Epoch 1832/2000\n",
      "240/240 [==============================] - 0s 647us/step - loss: 1.6591 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01832: loss did not improve from 1.65897\n",
      "Epoch 1833/2000\n",
      "240/240 [==============================] - 0s 628us/step - loss: 1.6590 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01833: loss did not improve from 1.65897\n",
      "Epoch 1834/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.6590 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01834: loss did not improve from 1.65897\n",
      "Epoch 1835/2000\n",
      "240/240 [==============================] - 0s 625us/step - loss: 1.6589 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01835: loss improved from 1.65897 to 1.65893, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 1836/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.6594 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01836: loss did not improve from 1.65893\n",
      "Epoch 1837/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.6593 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01837: loss did not improve from 1.65893\n",
      "Epoch 1838/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.6591 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01838: loss did not improve from 1.65893\n",
      "Epoch 1839/2000\n",
      "240/240 [==============================] - 0s 745us/step - loss: 1.6589 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01839: loss improved from 1.65893 to 1.65891, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 1840/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 1.6591 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01840: loss did not improve from 1.65891\n",
      "Epoch 1841/2000\n",
      "240/240 [==============================] - 0s 624us/step - loss: 1.6592 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01841: loss did not improve from 1.65891\n",
      "Epoch 1842/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.6606 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01842: loss did not improve from 1.65891\n",
      "Epoch 1843/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.6596 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01843: loss did not improve from 1.65891\n",
      "Epoch 1844/2000\n",
      "240/240 [==============================] - 0s 642us/step - loss: 1.6592 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01844: loss did not improve from 1.65891\n",
      "Epoch 1845/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.6590 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01845: loss did not improve from 1.65891\n",
      "Epoch 1846/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 1.6590 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01846: loss did not improve from 1.65891\n",
      "Epoch 1847/2000\n",
      "240/240 [==============================] - 0s 658us/step - loss: 1.6589 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01847: loss did not improve from 1.65891\n",
      "Epoch 1848/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.6590 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01848: loss did not improve from 1.65891\n",
      "Epoch 1849/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.6589 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01849: loss improved from 1.65891 to 1.65887, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 1850/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.6590 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01850: loss did not improve from 1.65887\n",
      "Epoch 1851/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.6669 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01851: loss did not improve from 1.65887\n",
      "Epoch 1852/2000\n",
      "240/240 [==============================] - 0s 754us/step - loss: 1.6606 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01852: loss did not improve from 1.65887\n",
      "Epoch 1853/2000\n",
      "240/240 [==============================] - 0s 750us/step - loss: 1.6597 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01853: loss did not improve from 1.65887\n",
      "Epoch 1854/2000\n",
      "240/240 [==============================] - 0s 734us/step - loss: 1.6594 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01854: loss did not improve from 1.65887\n",
      "Epoch 1855/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.6591 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01855: loss did not improve from 1.65887\n",
      "Epoch 1856/2000\n",
      "240/240 [==============================] - 0s 732us/step - loss: 1.6591 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01856: loss did not improve from 1.65887\n",
      "Epoch 1857/2000\n",
      "240/240 [==============================] - 0s 722us/step - loss: 1.6590 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01857: loss did not improve from 1.65887\n",
      "Epoch 1858/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 1.6591 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01858: loss did not improve from 1.65887\n",
      "Epoch 1859/2000\n",
      "240/240 [==============================] - 0s 722us/step - loss: 1.6590 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01859: loss did not improve from 1.65887\n",
      "Epoch 1860/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 1.6591 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01860: loss did not improve from 1.65887\n",
      "Epoch 1861/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.6642 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01861: loss did not improve from 1.65887\n",
      "Epoch 1862/2000\n",
      "240/240 [==============================] - 0s 740us/step - loss: 1.6601 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01862: loss did not improve from 1.65887\n",
      "Epoch 1863/2000\n",
      "240/240 [==============================] - 0s 645us/step - loss: 1.6591 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01863: loss did not improve from 1.65887\n",
      "Epoch 1864/2000\n",
      "240/240 [==============================] - 0s 768us/step - loss: 1.6590 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01864: loss did not improve from 1.65887\n",
      "Epoch 1865/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.6590 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01865: loss did not improve from 1.65887\n",
      "Epoch 1866/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.6589 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01866: loss improved from 1.65887 to 1.65887, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 1867/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 1.7710 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01867: loss did not improve from 1.65887\n",
      "Epoch 1868/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 1.6600 - accuracy: 0.4000\n",
      "\n",
      "Epoch 01868: loss did not improve from 1.65887\n",
      "Epoch 1869/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6804 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01869: loss did not improve from 1.65887\n",
      "Epoch 1870/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 1.6626 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01870: loss did not improve from 1.65887\n",
      "Epoch 1871/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6597 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01871: loss did not improve from 1.65887\n",
      "Epoch 1872/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6594 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01872: loss did not improve from 1.65887\n",
      "Epoch 1873/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6593 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01873: loss did not improve from 1.65887\n",
      "Epoch 1874/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6592 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01874: loss did not improve from 1.65887\n",
      "Epoch 1875/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 1.6592 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01875: loss did not improve from 1.65887\n",
      "Epoch 1876/2000\n",
      "240/240 [==============================] - 0s 737us/step - loss: 1.6592 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01876: loss did not improve from 1.65887\n",
      "Epoch 1877/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.6591 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01877: loss did not improve from 1.65887\n",
      "Epoch 1878/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6595 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01878: loss did not improve from 1.65887\n",
      "Epoch 1879/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6594 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01879: loss did not improve from 1.65887\n",
      "Epoch 1880/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.6591 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01880: loss did not improve from 1.65887\n",
      "Epoch 1881/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.6590 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01881: loss did not improve from 1.65887\n",
      "Epoch 1882/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6591 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01882: loss did not improve from 1.65887\n",
      "Epoch 1883/2000\n",
      "240/240 [==============================] - 0s 615us/step - loss: 1.6591 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01883: loss did not improve from 1.65887\n",
      "Epoch 1884/2000\n",
      "240/240 [==============================] - 0s 761us/step - loss: 1.6592 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01884: loss did not improve from 1.65887\n",
      "Epoch 1885/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.6589 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01885: loss did not improve from 1.65887\n",
      "Epoch 1886/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.6744 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01886: loss did not improve from 1.65887\n",
      "Epoch 1887/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6651 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01887: loss did not improve from 1.65887\n",
      "Epoch 1888/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6620 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01888: loss did not improve from 1.65887\n",
      "Epoch 1889/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6616 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01889: loss did not improve from 1.65887\n",
      "Epoch 1890/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6611 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01890: loss did not improve from 1.65887\n",
      "Epoch 1891/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6610 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01891: loss did not improve from 1.65887\n",
      "Epoch 1892/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.7742 - accuracy: 0.4000\n",
      "\n",
      "Epoch 01892: loss did not improve from 1.65887\n",
      "Epoch 1893/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.7729 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01893: loss did not improve from 1.65887\n",
      "Epoch 1894/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 1.6634 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01894: loss did not improve from 1.65887\n",
      "Epoch 1895/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6627 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01895: loss did not improve from 1.65887\n",
      "Epoch 1896/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6618 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01896: loss did not improve from 1.65887\n",
      "Epoch 1897/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.6614 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01897: loss did not improve from 1.65887\n",
      "Epoch 1898/2000\n",
      "240/240 [==============================] - 0s 762us/step - loss: 1.6612 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01898: loss did not improve from 1.65887\n",
      "Epoch 1899/2000\n",
      "240/240 [==============================] - 0s 777us/step - loss: 1.6608 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01899: loss did not improve from 1.65887\n",
      "Epoch 1900/2000\n",
      "240/240 [==============================] - 0s 777us/step - loss: 1.6602 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01900: loss did not improve from 1.65887\n",
      "Epoch 1901/2000\n",
      "240/240 [==============================] - 0s 759us/step - loss: 1.6605 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01901: loss did not improve from 1.65887\n",
      "Epoch 1902/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.6603 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01902: loss did not improve from 1.65887\n",
      "Epoch 1903/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.6601 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01903: loss did not improve from 1.65887\n",
      "Epoch 1904/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6602 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01904: loss did not improve from 1.65887\n",
      "Epoch 1905/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.6598 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01905: loss did not improve from 1.65887\n",
      "Epoch 1906/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6598 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01906: loss did not improve from 1.65887\n",
      "Epoch 1907/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.6597 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01907: loss did not improve from 1.65887\n",
      "Epoch 1908/2000\n",
      "240/240 [==============================] - 0s 627us/step - loss: 1.6596 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01908: loss did not improve from 1.65887\n",
      "Epoch 1909/2000\n",
      "240/240 [==============================] - 0s 739us/step - loss: 1.6595 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01909: loss did not improve from 1.65887\n",
      "Epoch 1910/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.6595 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01910: loss did not improve from 1.65887\n",
      "Epoch 1911/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 1.6596 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01911: loss did not improve from 1.65887\n",
      "Epoch 1912/2000\n",
      "240/240 [==============================] - 0s 739us/step - loss: 1.6593 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01912: loss did not improve from 1.65887\n",
      "Epoch 1913/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.6596 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01913: loss did not improve from 1.65887\n",
      "Epoch 1914/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6594 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01914: loss did not improve from 1.65887\n",
      "Epoch 1915/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 1.6593 - accuracy: 0.4000\n",
      "\n",
      "Epoch 01915: loss did not improve from 1.65887\n",
      "Epoch 1916/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.6597 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01916: loss did not improve from 1.65887\n",
      "Epoch 1917/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.6596 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01917: loss did not improve from 1.65887\n",
      "Epoch 1918/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.6594 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01918: loss did not improve from 1.65887\n",
      "Epoch 1919/2000\n",
      "240/240 [==============================] - 0s 811us/step - loss: 1.6602 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01919: loss did not improve from 1.65887\n",
      "Epoch 1920/2000\n",
      "240/240 [==============================] - 0s 913us/step - loss: 1.6598 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01920: loss did not improve from 1.65887\n",
      "Epoch 1921/2000\n",
      "240/240 [==============================] - 0s 871us/step - loss: 1.6601 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01921: loss did not improve from 1.65887\n",
      "Epoch 1922/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 1.6595 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01922: loss did not improve from 1.65887\n",
      "Epoch 1923/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 1.6594 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01923: loss did not improve from 1.65887\n",
      "Epoch 1924/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 1.6592 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01924: loss did not improve from 1.65887\n",
      "Epoch 1925/2000\n",
      "240/240 [==============================] - 0s 644us/step - loss: 1.6591 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01925: loss did not improve from 1.65887\n",
      "Epoch 1926/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.6590 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01926: loss did not improve from 1.65887\n",
      "Epoch 1927/2000\n",
      "240/240 [==============================] - 0s 662us/step - loss: 1.6590 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01927: loss did not improve from 1.65887\n",
      "Epoch 1928/2000\n",
      "240/240 [==============================] - 0s 642us/step - loss: 1.6591 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01928: loss did not improve from 1.65887\n",
      "Epoch 1929/2000\n",
      "240/240 [==============================] - 0s 638us/step - loss: 1.6590 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01929: loss did not improve from 1.65887\n",
      "Epoch 1930/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.6591 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01930: loss did not improve from 1.65887\n",
      "Epoch 1931/2000\n",
      "240/240 [==============================] - 0s 624us/step - loss: 1.6591 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01931: loss did not improve from 1.65887\n",
      "Epoch 1932/2000\n",
      "240/240 [==============================] - 0s 631us/step - loss: 1.6591 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01932: loss did not improve from 1.65887\n",
      "Epoch 1933/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 632us/step - loss: 1.6592 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01933: loss did not improve from 1.65887\n",
      "Epoch 1934/2000\n",
      "240/240 [==============================] - 0s 613us/step - loss: 1.6597 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01934: loss did not improve from 1.65887\n",
      "Epoch 1935/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.6611 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01935: loss did not improve from 1.65887\n",
      "Epoch 1936/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.6638 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01936: loss did not improve from 1.65887\n",
      "Epoch 1937/2000\n",
      "240/240 [==============================] - 0s 629us/step - loss: 1.6604 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01937: loss did not improve from 1.65887\n",
      "Epoch 1938/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.6599 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01938: loss did not improve from 1.65887\n",
      "Epoch 1939/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 1.6595 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01939: loss did not improve from 1.65887\n",
      "Epoch 1940/2000\n",
      "240/240 [==============================] - 0s 655us/step - loss: 1.6595 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01940: loss did not improve from 1.65887\n",
      "Epoch 1941/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6592 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01941: loss did not improve from 1.65887\n",
      "Epoch 1942/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 1.6593 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01942: loss did not improve from 1.65887\n",
      "Epoch 1943/2000\n",
      "240/240 [==============================] - 0s 644us/step - loss: 1.6593 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01943: loss did not improve from 1.65887\n",
      "Epoch 1944/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 1.6595 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01944: loss did not improve from 1.65887\n",
      "Epoch 1945/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 1.6593 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01945: loss did not improve from 1.65887\n",
      "Epoch 1946/2000\n",
      "240/240 [==============================] - 0s 732us/step - loss: 1.6592 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01946: loss did not improve from 1.65887\n",
      "Epoch 1947/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6591 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01947: loss did not improve from 1.65887\n",
      "Epoch 1948/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.6593 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01948: loss did not improve from 1.65887\n",
      "Epoch 1949/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.6591 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01949: loss did not improve from 1.65887\n",
      "Epoch 1950/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6591 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01950: loss did not improve from 1.65887\n",
      "Epoch 1951/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 1.6591 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01951: loss did not improve from 1.65887\n",
      "Epoch 1952/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6591 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01952: loss did not improve from 1.65887\n",
      "Epoch 1953/2000\n",
      "240/240 [==============================] - 0s 733us/step - loss: 1.6590 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01953: loss did not improve from 1.65887\n",
      "Epoch 1954/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 1.6591 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01954: loss did not improve from 1.65887\n",
      "Epoch 1955/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 1.6595 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01955: loss did not improve from 1.65887\n",
      "Epoch 1956/2000\n",
      "240/240 [==============================] - 0s 768us/step - loss: 1.6614 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01956: loss did not improve from 1.65887\n",
      "Epoch 1957/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 1.6607 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01957: loss did not improve from 1.65887\n",
      "Epoch 1958/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6620 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01958: loss did not improve from 1.65887\n",
      "Epoch 1959/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.6612 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01959: loss did not improve from 1.65887\n",
      "Epoch 1960/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 1.6614 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01960: loss did not improve from 1.65887\n",
      "Epoch 1961/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6596 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01961: loss did not improve from 1.65887\n",
      "Epoch 1962/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.6593 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01962: loss did not improve from 1.65887\n",
      "Epoch 1963/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6590 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01963: loss did not improve from 1.65887\n",
      "Epoch 1964/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6590 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01964: loss did not improve from 1.65887\n",
      "Epoch 1965/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.6589 - accuracy: 0.4000\n",
      "\n",
      "Epoch 01965: loss did not improve from 1.65887\n",
      "Epoch 1966/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 1.6589 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01966: loss did not improve from 1.65887\n",
      "Epoch 1967/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.6590 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01967: loss did not improve from 1.65887\n",
      "Epoch 1968/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.6591 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01968: loss did not improve from 1.65887\n",
      "Epoch 1969/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6591 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01969: loss did not improve from 1.65887\n",
      "Epoch 1970/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6590 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01970: loss did not improve from 1.65887\n",
      "Epoch 1971/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6591 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01971: loss did not improve from 1.65887\n",
      "Epoch 1972/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.6590 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01972: loss did not improve from 1.65887\n",
      "Epoch 1973/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6589 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01973: loss improved from 1.65887 to 1.65885, saving model to ./model/1000001000100010001.hdf5\n",
      "Epoch 1974/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.6592 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01974: loss did not improve from 1.65885\n",
      "Epoch 1975/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 1.6592 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01975: loss did not improve from 1.65885\n",
      "Epoch 1976/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.6592 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01976: loss did not improve from 1.65885\n",
      "Epoch 1977/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.6594 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01977: loss did not improve from 1.65885\n",
      "Epoch 1978/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.6590 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01978: loss did not improve from 1.65885\n",
      "Epoch 1979/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.6590 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01979: loss did not improve from 1.65885\n",
      "Epoch 1980/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.6596 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01980: loss did not improve from 1.65885\n",
      "Epoch 1981/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.6668 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01981: loss did not improve from 1.65885\n",
      "Epoch 1982/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.6609 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01982: loss did not improve from 1.65885\n",
      "Epoch 1983/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.7753 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01983: loss did not improve from 1.65885\n",
      "Epoch 1984/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.8244 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01984: loss did not improve from 1.65885\n",
      "Epoch 1985/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.6604 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01985: loss did not improve from 1.65885\n",
      "Epoch 1986/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6602 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01986: loss did not improve from 1.65885\n",
      "Epoch 1987/2000\n",
      "240/240 [==============================] - 0s 697us/step - loss: 1.6598 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01987: loss did not improve from 1.65885\n",
      "Epoch 1988/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.6593 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01988: loss did not improve from 1.65885\n",
      "Epoch 1989/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.6591 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01989: loss did not improve from 1.65885\n",
      "Epoch 1990/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.6591 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01990: loss did not improve from 1.65885\n",
      "Epoch 1991/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.6590 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01991: loss did not improve from 1.65885\n",
      "Epoch 1992/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 1.6592 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01992: loss did not improve from 1.65885\n",
      "Epoch 1993/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 1.6590 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01993: loss did not improve from 1.65885\n",
      "Epoch 1994/2000\n",
      "240/240 [==============================] - 0s 753us/step - loss: 1.6590 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01994: loss did not improve from 1.65885\n",
      "Epoch 1995/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.6589 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01995: loss did not improve from 1.65885\n",
      "Epoch 1996/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 1.6590 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01996: loss did not improve from 1.65885\n",
      "Epoch 1997/2000\n",
      "240/240 [==============================] - 0s 632us/step - loss: 1.6589 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01997: loss did not improve from 1.65885\n",
      "Epoch 1998/2000\n",
      "240/240 [==============================] - 0s 739us/step - loss: 1.6590 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01998: loss did not improve from 1.65885\n",
      "Epoch 1999/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.6590 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01999: loss did not improve from 1.65885\n",
      "Epoch 2000/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 1.6589 - accuracy: 0.4292\n",
      "\n",
      "Epoch 02000: loss did not improve from 1.65885\n"
     ]
    }
   ],
   "source": [
    "readdata_and_savemodel(\"1000001000100010001.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 4.6620 - accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00001: loss improved from inf to 4.66197, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 2/2000\n",
      "93/93 [==============================] - 0s 754us/step - loss: 4.3926 - accuracy: 0.0430\n",
      "\n",
      "Epoch 00002: loss improved from 4.66197 to 4.39259, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 3/2000\n",
      "93/93 [==============================] - 0s 751us/step - loss: 4.0592 - accuracy: 0.1075\n",
      "\n",
      "Epoch 00003: loss improved from 4.39259 to 4.05924, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 4/2000\n",
      "93/93 [==============================] - 0s 718us/step - loss: 3.5953 - accuracy: 0.2043\n",
      "\n",
      "Epoch 00004: loss improved from 4.05924 to 3.59526, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 5/2000\n",
      "93/93 [==============================] - 0s 807us/step - loss: 3.1324 - accuracy: 0.2796\n",
      "\n",
      "Epoch 00005: loss improved from 3.59526 to 3.13244, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 6/2000\n",
      "93/93 [==============================] - 0s 836us/step - loss: 2.7508 - accuracy: 0.3656\n",
      "\n",
      "Epoch 00006: loss improved from 3.13244 to 2.75085, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 7/2000\n",
      "93/93 [==============================] - 0s 740us/step - loss: 2.4655 - accuracy: 0.4409\n",
      "\n",
      "Epoch 00007: loss improved from 2.75085 to 2.46550, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 8/2000\n",
      "93/93 [==============================] - 0s 730us/step - loss: 2.2941 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00008: loss improved from 2.46550 to 2.29407, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 9/2000\n",
      "93/93 [==============================] - 0s 761us/step - loss: 2.1534 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00009: loss improved from 2.29407 to 2.15343, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 10/2000\n",
      "93/93 [==============================] - 0s 750us/step - loss: 2.0747 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00010: loss improved from 2.15343 to 2.07473, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 11/2000\n",
      "93/93 [==============================] - 0s 726us/step - loss: 2.0127 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00011: loss improved from 2.07473 to 2.01267, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 12/2000\n",
      "93/93 [==============================] - 0s 704us/step - loss: 1.9775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00012: loss improved from 2.01267 to 1.97748, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 13/2000\n",
      "93/93 [==============================] - 0s 697us/step - loss: 1.9425 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00013: loss improved from 1.97748 to 1.94249, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 14/2000\n",
      "93/93 [==============================] - 0s 762us/step - loss: 1.9215 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00014: loss improved from 1.94249 to 1.92147, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 15/2000\n",
      "93/93 [==============================] - 0s 705us/step - loss: 1.8925 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00015: loss improved from 1.92147 to 1.89246, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 16/2000\n",
      "93/93 [==============================] - 0s 730us/step - loss: 1.8857 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00016: loss improved from 1.89246 to 1.88565, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 17/2000\n",
      "93/93 [==============================] - 0s 760us/step - loss: 1.8852 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00017: loss improved from 1.88565 to 1.88519, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 18/2000\n",
      "93/93 [==============================] - 0s 757us/step - loss: 1.8638 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00018: loss improved from 1.88519 to 1.86381, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 19/2000\n",
      "93/93 [==============================] - 0s 728us/step - loss: 1.8476 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00019: loss improved from 1.86381 to 1.84763, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 20/2000\n",
      "93/93 [==============================] - 0s 804us/step - loss: 1.8412 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00020: loss improved from 1.84763 to 1.84124, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 21/2000\n",
      "93/93 [==============================] - 0s 840us/step - loss: 1.8415 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00021: loss did not improve from 1.84124\n",
      "Epoch 22/2000\n",
      "93/93 [==============================] - 0s 761us/step - loss: 1.8350 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00022: loss improved from 1.84124 to 1.83504, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 23/2000\n",
      "93/93 [==============================] - 0s 872us/step - loss: 1.8271 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00023: loss improved from 1.83504 to 1.82709, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 24/2000\n",
      "93/93 [==============================] - 0s 819us/step - loss: 1.8177 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00024: loss improved from 1.82709 to 1.81771, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 25/2000\n",
      "93/93 [==============================] - 0s 847us/step - loss: 1.8139 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00025: loss improved from 1.81771 to 1.81387, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 26/2000\n",
      "93/93 [==============================] - 0s 869us/step - loss: 1.8145 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00026: loss did not improve from 1.81387\n",
      "Epoch 27/2000\n",
      "93/93 [==============================] - 0s 744us/step - loss: 1.8050 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00027: loss improved from 1.81387 to 1.80499, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 28/2000\n",
      "93/93 [==============================] - 0s 718us/step - loss: 1.7945 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00028: loss improved from 1.80499 to 1.79447, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 29/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.7955 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00029: loss did not improve from 1.79447\n",
      "Epoch 30/2000\n",
      "93/93 [==============================] - 0s 672us/step - loss: 1.7922 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00030: loss improved from 1.79447 to 1.79224, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 31/2000\n",
      "93/93 [==============================] - 0s 613us/step - loss: 1.7910 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00031: loss improved from 1.79224 to 1.79099, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 32/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.7884 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00032: loss improved from 1.79099 to 1.78845, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 33/2000\n",
      "93/93 [==============================] - 0s 682us/step - loss: 1.7840 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00033: loss improved from 1.78845 to 1.78402, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 34/2000\n",
      "93/93 [==============================] - 0s 701us/step - loss: 1.7828 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00034: loss improved from 1.78402 to 1.78282, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 35/2000\n",
      "93/93 [==============================] - 0s 690us/step - loss: 1.7847 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00035: loss did not improve from 1.78282\n",
      "Epoch 36/2000\n",
      "93/93 [==============================] - 0s 697us/step - loss: 1.7768 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00036: loss improved from 1.78282 to 1.77676, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 37/2000\n",
      "93/93 [==============================] - 0s 698us/step - loss: 1.7803 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00037: loss did not improve from 1.77676\n",
      "Epoch 38/2000\n",
      "93/93 [==============================] - 0s 683us/step - loss: 1.7710 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00038: loss improved from 1.77676 to 1.77096, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 39/2000\n",
      "93/93 [==============================] - 0s 687us/step - loss: 1.7719 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00039: loss did not improve from 1.77096\n",
      "Epoch 40/2000\n",
      "93/93 [==============================] - 0s 699us/step - loss: 1.7693 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00040: loss improved from 1.77096 to 1.76933, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 41/2000\n",
      "93/93 [==============================] - 0s 708us/step - loss: 1.7683 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00041: loss improved from 1.76933 to 1.76834, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 42/2000\n",
      "93/93 [==============================] - 0s 772us/step - loss: 1.7601 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00042: loss improved from 1.76834 to 1.76008, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 43/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 0s 732us/step - loss: 1.7644 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00043: loss did not improve from 1.76008\n",
      "Epoch 44/2000\n",
      "93/93 [==============================] - 0s 758us/step - loss: 1.7576 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00044: loss improved from 1.76008 to 1.75765, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 45/2000\n",
      "93/93 [==============================] - 0s 710us/step - loss: 1.7621 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00045: loss did not improve from 1.75765\n",
      "Epoch 46/2000\n",
      "93/93 [==============================] - 0s 729us/step - loss: 1.7665 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00046: loss did not improve from 1.75765\n",
      "Epoch 47/2000\n",
      "93/93 [==============================] - 0s 712us/step - loss: 1.7563 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00047: loss improved from 1.75765 to 1.75628, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 48/2000\n",
      "93/93 [==============================] - 0s 718us/step - loss: 1.7522 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00048: loss improved from 1.75628 to 1.75216, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 49/2000\n",
      "93/93 [==============================] - 0s 777us/step - loss: 1.7537 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00049: loss did not improve from 1.75216\n",
      "Epoch 50/2000\n",
      "93/93 [==============================] - 0s 738us/step - loss: 1.7535 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00050: loss did not improve from 1.75216\n",
      "Epoch 51/2000\n",
      "93/93 [==============================] - 0s 745us/step - loss: 1.7576 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00051: loss did not improve from 1.75216\n",
      "Epoch 52/2000\n",
      "93/93 [==============================] - 0s 780us/step - loss: 1.7430 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00052: loss improved from 1.75216 to 1.74298, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 53/2000\n",
      "93/93 [==============================] - 0s 791us/step - loss: 1.7462 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00053: loss did not improve from 1.74298\n",
      "Epoch 54/2000\n",
      "93/93 [==============================] - 0s 722us/step - loss: 1.7495 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00054: loss did not improve from 1.74298\n",
      "Epoch 55/2000\n",
      "93/93 [==============================] - 0s 772us/step - loss: 1.7469 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00055: loss did not improve from 1.74298\n",
      "Epoch 56/2000\n",
      "93/93 [==============================] - 0s 760us/step - loss: 1.7407 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00056: loss improved from 1.74298 to 1.74067, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 57/2000\n",
      "93/93 [==============================] - 0s 787us/step - loss: 1.7388 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00057: loss improved from 1.74067 to 1.73884, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 58/2000\n",
      "93/93 [==============================] - 0s 758us/step - loss: 1.7377 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00058: loss improved from 1.73884 to 1.73767, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 59/2000\n",
      "93/93 [==============================] - 0s 766us/step - loss: 1.7382 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00059: loss did not improve from 1.73767\n",
      "Epoch 60/2000\n",
      "93/93 [==============================] - 0s 807us/step - loss: 1.7419 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00060: loss did not improve from 1.73767\n",
      "Epoch 61/2000\n",
      "93/93 [==============================] - 0s 795us/step - loss: 1.7385 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00061: loss did not improve from 1.73767\n",
      "Epoch 62/2000\n",
      "93/93 [==============================] - 0s 760us/step - loss: 1.7346 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00062: loss improved from 1.73767 to 1.73460, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 63/2000\n",
      "93/93 [==============================] - 0s 761us/step - loss: 1.7381 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00063: loss did not improve from 1.73460\n",
      "Epoch 64/2000\n",
      "93/93 [==============================] - 0s 794us/step - loss: 1.7317 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00064: loss improved from 1.73460 to 1.73166, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 65/2000\n",
      "93/93 [==============================] - 0s 766us/step - loss: 1.7345 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00065: loss did not improve from 1.73166\n",
      "Epoch 66/2000\n",
      "93/93 [==============================] - 0s 725us/step - loss: 1.7348 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00066: loss did not improve from 1.73166\n",
      "Epoch 67/2000\n",
      "93/93 [==============================] - 0s 740us/step - loss: 1.7350 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00067: loss did not improve from 1.73166\n",
      "Epoch 68/2000\n",
      "93/93 [==============================] - 0s 760us/step - loss: 1.7303 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00068: loss improved from 1.73166 to 1.73032, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 69/2000\n",
      "93/93 [==============================] - 0s 644us/step - loss: 1.7344 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00069: loss did not improve from 1.73032\n",
      "Epoch 70/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.7242 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00070: loss improved from 1.73032 to 1.72421, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 71/2000\n",
      "93/93 [==============================] - 0s 695us/step - loss: 1.7313 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00071: loss did not improve from 1.72421\n",
      "Epoch 72/2000\n",
      "93/93 [==============================] - 0s 708us/step - loss: 1.7336 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00072: loss did not improve from 1.72421\n",
      "Epoch 73/2000\n",
      "93/93 [==============================] - 0s 752us/step - loss: 1.7319 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00073: loss did not improve from 1.72421\n",
      "Epoch 74/2000\n",
      "93/93 [==============================] - 0s 751us/step - loss: 1.7273 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00074: loss did not improve from 1.72421\n",
      "Epoch 75/2000\n",
      "93/93 [==============================] - 0s 696us/step - loss: 1.7290 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00075: loss did not improve from 1.72421\n",
      "Epoch 76/2000\n",
      "93/93 [==============================] - 0s 656us/step - loss: 1.7250 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00076: loss did not improve from 1.72421\n",
      "Epoch 77/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.7268 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00077: loss did not improve from 1.72421\n",
      "Epoch 78/2000\n",
      "93/93 [==============================] - 0s 688us/step - loss: 1.7272 - accuracy: 0.4946\n",
      "\n",
      "Epoch 00078: loss did not improve from 1.72421\n",
      "Epoch 79/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.7227 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00079: loss improved from 1.72421 to 1.72272, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 80/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.7204 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00080: loss improved from 1.72272 to 1.72038, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 81/2000\n",
      "93/93 [==============================] - 0s 697us/step - loss: 1.7251 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00081: loss did not improve from 1.72038\n",
      "Epoch 82/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.7195 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00082: loss improved from 1.72038 to 1.71951, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 83/2000\n",
      "93/93 [==============================] - 0s 746us/step - loss: 1.7197 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00083: loss did not improve from 1.71951\n",
      "Epoch 84/2000\n",
      "93/93 [==============================] - 0s 700us/step - loss: 1.7174 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00084: loss improved from 1.71951 to 1.71737, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 85/2000\n",
      "93/93 [==============================] - 0s 699us/step - loss: 1.7157 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00085: loss improved from 1.71737 to 1.71574, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 86/2000\n",
      "93/93 [==============================] - 0s 690us/step - loss: 1.7171 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00086: loss did not improve from 1.71574\n",
      "Epoch 87/2000\n",
      "93/93 [==============================] - 0s 698us/step - loss: 1.7186 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00087: loss did not improve from 1.71574\n",
      "Epoch 88/2000\n",
      "93/93 [==============================] - 0s 669us/step - loss: 1.7164 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00088: loss did not improve from 1.71574\n",
      "Epoch 89/2000\n",
      "93/93 [==============================] - 0s 692us/step - loss: 1.7142 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00089: loss improved from 1.71574 to 1.71424, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 90/2000\n",
      "93/93 [==============================] - 0s 683us/step - loss: 1.7147 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00090: loss did not improve from 1.71424\n",
      "Epoch 91/2000\n",
      "93/93 [==============================] - 0s 677us/step - loss: 1.7158 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00091: loss did not improve from 1.71424\n",
      "Epoch 92/2000\n",
      "93/93 [==============================] - 0s 694us/step - loss: 1.7139 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00092: loss improved from 1.71424 to 1.71389, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 93/2000\n",
      "93/93 [==============================] - 0s 706us/step - loss: 1.7125 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00093: loss improved from 1.71389 to 1.71247, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 94/2000\n",
      "93/93 [==============================] - 0s 702us/step - loss: 1.7108 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00094: loss improved from 1.71247 to 1.71082, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 95/2000\n",
      "93/93 [==============================] - 0s 699us/step - loss: 1.7102 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00095: loss improved from 1.71082 to 1.71022, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 96/2000\n",
      "93/93 [==============================] - 0s 680us/step - loss: 1.7131 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00096: loss did not improve from 1.71022\n",
      "Epoch 97/2000\n",
      "93/93 [==============================] - 0s 688us/step - loss: 1.7127 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00097: loss did not improve from 1.71022\n",
      "Epoch 98/2000\n",
      "93/93 [==============================] - 0s 690us/step - loss: 1.7114 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00098: loss did not improve from 1.71022\n",
      "Epoch 99/2000\n",
      "93/93 [==============================] - 0s 698us/step - loss: 1.7075 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00099: loss improved from 1.71022 to 1.70754, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 100/2000\n",
      "93/93 [==============================] - 0s 705us/step - loss: 1.7090 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00100: loss did not improve from 1.70754\n",
      "Epoch 101/2000\n",
      "93/93 [==============================] - 0s 696us/step - loss: 1.7079 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00101: loss did not improve from 1.70754\n",
      "Epoch 102/2000\n",
      "93/93 [==============================] - 0s 674us/step - loss: 1.7064 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00102: loss improved from 1.70754 to 1.70635, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 103/2000\n",
      "93/93 [==============================] - 0s 672us/step - loss: 1.7080 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00103: loss did not improve from 1.70635\n",
      "Epoch 104/2000\n",
      "93/93 [==============================] - 0s 680us/step - loss: 1.7057 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00104: loss improved from 1.70635 to 1.70571, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 105/2000\n",
      "93/93 [==============================] - 0s 610us/step - loss: 1.7053 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00105: loss improved from 1.70571 to 1.70535, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 106/2000\n",
      "93/93 [==============================] - 0s 690us/step - loss: 1.7065 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00106: loss did not improve from 1.70535\n",
      "Epoch 107/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.7036 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00107: loss improved from 1.70535 to 1.70359, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 108/2000\n",
      "93/93 [==============================] - 0s 696us/step - loss: 1.7046 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00108: loss did not improve from 1.70359\n",
      "Epoch 109/2000\n",
      "93/93 [==============================] - 0s 674us/step - loss: 1.7052 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00109: loss did not improve from 1.70359\n",
      "Epoch 110/2000\n",
      "93/93 [==============================] - 0s 664us/step - loss: 1.7036 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00110: loss improved from 1.70359 to 1.70359, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 111/2000\n",
      "93/93 [==============================] - 0s 679us/step - loss: 1.7009 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00111: loss improved from 1.70359 to 1.70088, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 112/2000\n",
      "93/93 [==============================] - 0s 698us/step - loss: 1.7064 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00112: loss did not improve from 1.70088\n",
      "Epoch 113/2000\n",
      "93/93 [==============================] - 0s 684us/step - loss: 1.7037 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00113: loss did not improve from 1.70088\n",
      "Epoch 114/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.7043 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00114: loss did not improve from 1.70088\n",
      "Epoch 115/2000\n",
      "93/93 [==============================] - 0s 695us/step - loss: 1.7024 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00115: loss did not improve from 1.70088\n",
      "Epoch 116/2000\n",
      "93/93 [==============================] - 0s 685us/step - loss: 1.7031 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00116: loss did not improve from 1.70088\n",
      "Epoch 117/2000\n",
      "93/93 [==============================] - 0s 672us/step - loss: 1.7010 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00117: loss did not improve from 1.70088\n",
      "Epoch 118/2000\n",
      "93/93 [==============================] - 0s 675us/step - loss: 1.7005 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00118: loss improved from 1.70088 to 1.70047, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 119/2000\n",
      "93/93 [==============================] - 0s 678us/step - loss: 1.7037 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00119: loss did not improve from 1.70047\n",
      "Epoch 120/2000\n",
      "93/93 [==============================] - 0s 698us/step - loss: 1.7006 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00120: loss did not improve from 1.70047\n",
      "Epoch 121/2000\n",
      "93/93 [==============================] - 0s 705us/step - loss: 1.7002 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00121: loss improved from 1.70047 to 1.70024, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 122/2000\n",
      "93/93 [==============================] - 0s 758us/step - loss: 1.6998 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00122: loss improved from 1.70024 to 1.69980, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 123/2000\n",
      "93/93 [==============================] - 0s 707us/step - loss: 1.6992 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00123: loss improved from 1.69980 to 1.69923, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 124/2000\n",
      "93/93 [==============================] - 0s 721us/step - loss: 1.6999 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00124: loss did not improve from 1.69923\n",
      "Epoch 125/2000\n",
      "93/93 [==============================] - 0s 723us/step - loss: 1.6993 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00125: loss did not improve from 1.69923\n",
      "Epoch 126/2000\n",
      "93/93 [==============================] - 0s 715us/step - loss: 1.6987 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00126: loss improved from 1.69923 to 1.69867, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 127/2000\n",
      "93/93 [==============================] - 0s 717us/step - loss: 1.6986 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00127: loss improved from 1.69867 to 1.69858, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 128/2000\n",
      "93/93 [==============================] - 0s 735us/step - loss: 1.6980 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00128: loss improved from 1.69858 to 1.69797, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 129/2000\n",
      "93/93 [==============================] - 0s 724us/step - loss: 1.6971 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00129: loss improved from 1.69797 to 1.69712, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 130/2000\n",
      "93/93 [==============================] - 0s 714us/step - loss: 1.6964 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00130: loss improved from 1.69712 to 1.69643, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 131/2000\n",
      "93/93 [==============================] - 0s 709us/step - loss: 1.6966 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00131: loss did not improve from 1.69643\n",
      "Epoch 132/2000\n",
      "93/93 [==============================] - 0s 728us/step - loss: 1.6978 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00132: loss did not improve from 1.69643\n",
      "Epoch 133/2000\n",
      "93/93 [==============================] - 0s 719us/step - loss: 1.6962 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00133: loss improved from 1.69643 to 1.69625, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 134/2000\n",
      "93/93 [==============================] - 0s 719us/step - loss: 1.6983 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00134: loss did not improve from 1.69625\n",
      "Epoch 135/2000\n",
      "93/93 [==============================] - 0s 719us/step - loss: 1.6960 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00135: loss improved from 1.69625 to 1.69603, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 136/2000\n",
      "93/93 [==============================] - 0s 755us/step - loss: 1.6969 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00136: loss did not improve from 1.69603\n",
      "Epoch 137/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 0s 754us/step - loss: 1.7053 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00137: loss did not improve from 1.69603\n",
      "Epoch 138/2000\n",
      "93/93 [==============================] - 0s 760us/step - loss: 1.6984 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00138: loss did not improve from 1.69603\n",
      "Epoch 139/2000\n",
      "93/93 [==============================] - 0s 718us/step - loss: 1.6938 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00139: loss improved from 1.69603 to 1.69375, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 140/2000\n",
      "93/93 [==============================] - 0s 722us/step - loss: 1.6936 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00140: loss improved from 1.69375 to 1.69362, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 141/2000\n",
      "93/93 [==============================] - 0s 722us/step - loss: 1.6937 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00141: loss did not improve from 1.69362\n",
      "Epoch 142/2000\n",
      "93/93 [==============================] - 0s 715us/step - loss: 1.6938 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00142: loss did not improve from 1.69362\n",
      "Epoch 143/2000\n",
      "93/93 [==============================] - 0s 702us/step - loss: 1.6949 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00143: loss did not improve from 1.69362\n",
      "Epoch 144/2000\n",
      "93/93 [==============================] - 0s 715us/step - loss: 1.6972 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00144: loss did not improve from 1.69362\n",
      "Epoch 145/2000\n",
      "93/93 [==============================] - 0s 736us/step - loss: 1.6960 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00145: loss did not improve from 1.69362\n",
      "Epoch 146/2000\n",
      "93/93 [==============================] - 0s 717us/step - loss: 1.7005 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00146: loss did not improve from 1.69362\n",
      "Epoch 147/2000\n",
      "93/93 [==============================] - 0s 741us/step - loss: 1.6946 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00147: loss did not improve from 1.69362\n",
      "Epoch 148/2000\n",
      "93/93 [==============================] - 0s 731us/step - loss: 1.6937 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00148: loss did not improve from 1.69362\n",
      "Epoch 149/2000\n",
      "93/93 [==============================] - 0s 735us/step - loss: 1.6987 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00149: loss did not improve from 1.69362\n",
      "Epoch 150/2000\n",
      "93/93 [==============================] - 0s 720us/step - loss: 1.6942 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00150: loss did not improve from 1.69362\n",
      "Epoch 151/2000\n",
      "93/93 [==============================] - 0s 719us/step - loss: 1.6924 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00151: loss improved from 1.69362 to 1.69244, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 152/2000\n",
      "93/93 [==============================] - 0s 739us/step - loss: 1.6926 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00152: loss did not improve from 1.69244\n",
      "Epoch 153/2000\n",
      "93/93 [==============================] - 0s 707us/step - loss: 1.6915 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00153: loss improved from 1.69244 to 1.69150, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 154/2000\n",
      "93/93 [==============================] - 0s 713us/step - loss: 1.6931 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00154: loss did not improve from 1.69150\n",
      "Epoch 155/2000\n",
      "93/93 [==============================] - 0s 734us/step - loss: 1.6914 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00155: loss improved from 1.69150 to 1.69144, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 156/2000\n",
      "93/93 [==============================] - 0s 714us/step - loss: 1.6922 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00156: loss did not improve from 1.69144\n",
      "Epoch 157/2000\n",
      "93/93 [==============================] - 0s 725us/step - loss: 1.6919 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00157: loss did not improve from 1.69144\n",
      "Epoch 158/2000\n",
      "93/93 [==============================] - 0s 712us/step - loss: 1.6909 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00158: loss improved from 1.69144 to 1.69087, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 159/2000\n",
      "93/93 [==============================] - 0s 740us/step - loss: 1.6918 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00159: loss did not improve from 1.69087\n",
      "Epoch 160/2000\n",
      "93/93 [==============================] - 0s 730us/step - loss: 1.6912 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00160: loss did not improve from 1.69087\n",
      "Epoch 161/2000\n",
      "93/93 [==============================] - 0s 739us/step - loss: 1.6913 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00161: loss did not improve from 1.69087\n",
      "Epoch 162/2000\n",
      "93/93 [==============================] - 0s 735us/step - loss: 1.6899 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00162: loss improved from 1.69087 to 1.68994, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 163/2000\n",
      "93/93 [==============================] - 0s 691us/step - loss: 1.6900 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00163: loss did not improve from 1.68994\n",
      "Epoch 164/2000\n",
      "93/93 [==============================] - 0s 724us/step - loss: 1.6907 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00164: loss did not improve from 1.68994\n",
      "Epoch 165/2000\n",
      "93/93 [==============================] - 0s 715us/step - loss: 1.6895 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00165: loss improved from 1.68994 to 1.68953, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 166/2000\n",
      "93/93 [==============================] - 0s 745us/step - loss: 1.6900 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00166: loss did not improve from 1.68953\n",
      "Epoch 167/2000\n",
      "93/93 [==============================] - 0s 718us/step - loss: 1.6902 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00167: loss did not improve from 1.68953\n",
      "Epoch 168/2000\n",
      "93/93 [==============================] - 0s 711us/step - loss: 1.6895 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00168: loss improved from 1.68953 to 1.68949, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 169/2000\n",
      "93/93 [==============================] - 0s 715us/step - loss: 1.6931 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00169: loss did not improve from 1.68949\n",
      "Epoch 170/2000\n",
      "93/93 [==============================] - 0s 695us/step - loss: 1.6914 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00170: loss did not improve from 1.68949\n",
      "Epoch 171/2000\n",
      "93/93 [==============================] - 0s 716us/step - loss: 1.6901 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00171: loss did not improve from 1.68949\n",
      "Epoch 172/2000\n",
      "93/93 [==============================] - 0s 719us/step - loss: 1.6918 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00172: loss did not improve from 1.68949\n",
      "Epoch 173/2000\n",
      "93/93 [==============================] - 0s 726us/step - loss: 1.6897 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00173: loss did not improve from 1.68949\n",
      "Epoch 174/2000\n",
      "93/93 [==============================] - 0s 724us/step - loss: 1.6892 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00174: loss improved from 1.68949 to 1.68920, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 175/2000\n",
      "93/93 [==============================] - 0s 725us/step - loss: 1.6895 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00175: loss did not improve from 1.68920\n",
      "Epoch 176/2000\n",
      "93/93 [==============================] - 0s 739us/step - loss: 1.6885 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00176: loss improved from 1.68920 to 1.68852, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 177/2000\n",
      "93/93 [==============================] - 0s 721us/step - loss: 1.6900 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00177: loss did not improve from 1.68852\n",
      "Epoch 178/2000\n",
      "93/93 [==============================] - 0s 740us/step - loss: 1.6889 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00178: loss did not improve from 1.68852\n",
      "Epoch 179/2000\n",
      "93/93 [==============================] - 0s 728us/step - loss: 1.6881 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00179: loss improved from 1.68852 to 1.68814, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 180/2000\n",
      "93/93 [==============================] - 0s 725us/step - loss: 1.6889 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00180: loss did not improve from 1.68814\n",
      "Epoch 181/2000\n",
      "93/93 [==============================] - 0s 714us/step - loss: 1.6880 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00181: loss improved from 1.68814 to 1.68796, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 182/2000\n",
      "93/93 [==============================] - 0s 741us/step - loss: 1.6906 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00182: loss did not improve from 1.68796\n",
      "Epoch 183/2000\n",
      "93/93 [==============================] - 0s 723us/step - loss: 1.6902 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00183: loss did not improve from 1.68796\n",
      "Epoch 184/2000\n",
      "93/93 [==============================] - 0s 718us/step - loss: 1.6884 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00184: loss did not improve from 1.68796\n",
      "Epoch 185/2000\n",
      "93/93 [==============================] - 0s 773us/step - loss: 1.6889 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00185: loss did not improve from 1.68796\n",
      "Epoch 186/2000\n",
      "93/93 [==============================] - 0s 737us/step - loss: 1.6885 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00186: loss did not improve from 1.68796\n",
      "Epoch 187/2000\n",
      "93/93 [==============================] - 0s 747us/step - loss: 1.6884 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00187: loss did not improve from 1.68796\n",
      "Epoch 188/2000\n",
      "93/93 [==============================] - 0s 706us/step - loss: 1.6887 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00188: loss did not improve from 1.68796\n",
      "Epoch 189/2000\n",
      "93/93 [==============================] - 0s 718us/step - loss: 1.6868 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00189: loss improved from 1.68796 to 1.68678, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 190/2000\n",
      "93/93 [==============================] - 0s 718us/step - loss: 1.6883 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00190: loss did not improve from 1.68678\n",
      "Epoch 191/2000\n",
      "93/93 [==============================] - 0s 697us/step - loss: 1.6878 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00191: loss did not improve from 1.68678\n",
      "Epoch 192/2000\n",
      "93/93 [==============================] - 0s 696us/step - loss: 1.6876 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00192: loss did not improve from 1.68678\n",
      "Epoch 193/2000\n",
      "93/93 [==============================] - 0s 807us/step - loss: 1.6874 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00193: loss did not improve from 1.68678\n",
      "Epoch 194/2000\n",
      "93/93 [==============================] - 0s 752us/step - loss: 1.6869 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00194: loss did not improve from 1.68678\n",
      "Epoch 195/2000\n",
      "93/93 [==============================] - 0s 680us/step - loss: 1.6874 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00195: loss did not improve from 1.68678\n",
      "Epoch 196/2000\n",
      "93/93 [==============================] - 0s 681us/step - loss: 1.6869 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00196: loss did not improve from 1.68678\n",
      "Epoch 197/2000\n",
      "93/93 [==============================] - 0s 697us/step - loss: 1.6885 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00197: loss did not improve from 1.68678\n",
      "Epoch 198/2000\n",
      "93/93 [==============================] - 0s 718us/step - loss: 1.6871 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00198: loss did not improve from 1.68678\n",
      "Epoch 199/2000\n",
      "93/93 [==============================] - 0s 707us/step - loss: 1.6870 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00199: loss did not improve from 1.68678\n",
      "Epoch 200/2000\n",
      "93/93 [==============================] - 0s 764us/step - loss: 1.6867 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00200: loss improved from 1.68678 to 1.68674, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 201/2000\n",
      "93/93 [==============================] - 0s 717us/step - loss: 1.6869 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00201: loss did not improve from 1.68674\n",
      "Epoch 202/2000\n",
      "93/93 [==============================] - 0s 730us/step - loss: 1.6900 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00202: loss did not improve from 1.68674\n",
      "Epoch 203/2000\n",
      "93/93 [==============================] - 0s 726us/step - loss: 1.6863 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00203: loss improved from 1.68674 to 1.68635, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 204/2000\n",
      "93/93 [==============================] - 0s 700us/step - loss: 1.6857 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00204: loss improved from 1.68635 to 1.68575, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 205/2000\n",
      "93/93 [==============================] - 0s 771us/step - loss: 1.6866 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00205: loss did not improve from 1.68575\n",
      "Epoch 206/2000\n",
      "93/93 [==============================] - 0s 718us/step - loss: 1.6862 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00206: loss did not improve from 1.68575\n",
      "Epoch 207/2000\n",
      "93/93 [==============================] - 0s 791us/step - loss: 1.6865 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00207: loss did not improve from 1.68575\n",
      "Epoch 208/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6860 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00208: loss did not improve from 1.68575\n",
      "Epoch 209/2000\n",
      "93/93 [==============================] - 0s 669us/step - loss: 1.6863 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00209: loss did not improve from 1.68575\n",
      "Epoch 210/2000\n",
      "93/93 [==============================] - 0s 783us/step - loss: 1.6857 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00210: loss improved from 1.68575 to 1.68574, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 211/2000\n",
      "93/93 [==============================] - 0s 794us/step - loss: 1.6857 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00211: loss improved from 1.68574 to 1.68573, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 212/2000\n",
      "93/93 [==============================] - 0s 708us/step - loss: 1.6859 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00212: loss did not improve from 1.68573\n",
      "Epoch 213/2000\n",
      "93/93 [==============================] - 0s 740us/step - loss: 1.6903 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00213: loss did not improve from 1.68573\n",
      "Epoch 214/2000\n",
      "93/93 [==============================] - 0s 718us/step - loss: 1.6866 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00214: loss did not improve from 1.68573\n",
      "Epoch 215/2000\n",
      "93/93 [==============================] - 0s 722us/step - loss: 1.6852 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00215: loss improved from 1.68573 to 1.68516, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 216/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6870 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00216: loss did not improve from 1.68516\n",
      "Epoch 217/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6862 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00217: loss did not improve from 1.68516\n",
      "Epoch 218/2000\n",
      "93/93 [==============================] - 0s 729us/step - loss: 1.6854 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00218: loss did not improve from 1.68516\n",
      "Epoch 219/2000\n",
      "93/93 [==============================] - 0s 675us/step - loss: 1.6855 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00219: loss did not improve from 1.68516\n",
      "Epoch 220/2000\n",
      "93/93 [==============================] - 0s 696us/step - loss: 1.6851 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00220: loss improved from 1.68516 to 1.68511, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 221/2000\n",
      "93/93 [==============================] - 0s 662us/step - loss: 1.6850 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00221: loss improved from 1.68511 to 1.68495, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 222/2000\n",
      "93/93 [==============================] - 0s 632us/step - loss: 1.6850 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00222: loss did not improve from 1.68495\n",
      "Epoch 223/2000\n",
      "93/93 [==============================] - 0s 719us/step - loss: 1.6850 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00223: loss did not improve from 1.68495\n",
      "Epoch 224/2000\n",
      "93/93 [==============================] - 0s 667us/step - loss: 1.6887 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00224: loss did not improve from 1.68495\n",
      "Epoch 225/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6900 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00225: loss did not improve from 1.68495\n",
      "Epoch 226/2000\n",
      "93/93 [==============================] - 0s 729us/step - loss: 1.6911 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00226: loss did not improve from 1.68495\n",
      "Epoch 227/2000\n",
      "93/93 [==============================] - 0s 707us/step - loss: 1.6852 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00227: loss did not improve from 1.68495\n",
      "Epoch 228/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6852 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00228: loss did not improve from 1.68495\n",
      "Epoch 229/2000\n",
      "93/93 [==============================] - 0s 736us/step - loss: 1.6851 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00229: loss did not improve from 1.68495\n",
      "Epoch 230/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6842 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00230: loss improved from 1.68495 to 1.68419, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 231/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6870 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00231: loss did not improve from 1.68419\n",
      "Epoch 232/2000\n",
      "93/93 [==============================] - 0s 702us/step - loss: 1.6853 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00232: loss did not improve from 1.68419\n",
      "Epoch 233/2000\n",
      "93/93 [==============================] - 0s 729us/step - loss: 1.6841 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00233: loss improved from 1.68419 to 1.68415, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 234/2000\n",
      "93/93 [==============================] - 0s 729us/step - loss: 1.6849 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00234: loss did not improve from 1.68415\n",
      "Epoch 235/2000\n",
      "93/93 [==============================] - 0s 965us/step - loss: 1.6845 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00235: loss did not improve from 1.68415\n",
      "Epoch 236/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 0s 831us/step - loss: 1.6849 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00236: loss did not improve from 1.68415\n",
      "Epoch 237/2000\n",
      "93/93 [==============================] - 0s 783us/step - loss: 1.6852 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00237: loss did not improve from 1.68415\n",
      "Epoch 238/2000\n",
      "93/93 [==============================] - 0s 772us/step - loss: 1.6843 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00238: loss did not improve from 1.68415\n",
      "Epoch 239/2000\n",
      "93/93 [==============================] - 0s 755us/step - loss: 1.6843 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00239: loss did not improve from 1.68415\n",
      "Epoch 240/2000\n",
      "93/93 [==============================] - 0s 722us/step - loss: 1.6846 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00240: loss did not improve from 1.68415\n",
      "Epoch 241/2000\n",
      "93/93 [==============================] - 0s 729us/step - loss: 1.6841 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00241: loss improved from 1.68415 to 1.68414, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 242/2000\n",
      "93/93 [==============================] - 0s 761us/step - loss: 1.6840 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00242: loss improved from 1.68414 to 1.68400, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 243/2000\n",
      "93/93 [==============================] - 0s 629us/step - loss: 1.6843 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00243: loss did not improve from 1.68400\n",
      "Epoch 244/2000\n",
      "93/93 [==============================] - 0s 678us/step - loss: 1.6835 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00244: loss improved from 1.68400 to 1.68353, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 245/2000\n",
      "93/93 [==============================] - 0s 700us/step - loss: 1.6879 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00245: loss did not improve from 1.68353\n",
      "Epoch 246/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6838 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00246: loss did not improve from 1.68353\n",
      "Epoch 247/2000\n",
      "93/93 [==============================] - 0s 584us/step - loss: 1.6856 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00247: loss did not improve from 1.68353\n",
      "Epoch 248/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6854 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00248: loss did not improve from 1.68353\n",
      "Epoch 249/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6843 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00249: loss did not improve from 1.68353\n",
      "Epoch 250/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6834 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00250: loss improved from 1.68353 to 1.68338, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 251/2000\n",
      "93/93 [==============================] - 0s 729us/step - loss: 1.6841 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00251: loss did not improve from 1.68338\n",
      "Epoch 252/2000\n",
      "93/93 [==============================] - 0s 697us/step - loss: 1.6836 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00252: loss did not improve from 1.68338\n",
      "Epoch 253/2000\n",
      "93/93 [==============================] - 0s 674us/step - loss: 1.6836 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00253: loss did not improve from 1.68338\n",
      "Epoch 254/2000\n",
      "93/93 [==============================] - 0s 674us/step - loss: 1.6833 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00254: loss improved from 1.68338 to 1.68330, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 255/2000\n",
      "93/93 [==============================] - 0s 707us/step - loss: 1.6833 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00255: loss improved from 1.68330 to 1.68328, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 256/2000\n",
      "93/93 [==============================] - 0s 732us/step - loss: 1.6837 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00256: loss did not improve from 1.68328\n",
      "Epoch 257/2000\n",
      "93/93 [==============================] - 0s 723us/step - loss: 1.6847 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00257: loss did not improve from 1.68328\n",
      "Epoch 258/2000\n",
      "93/93 [==============================] - 0s 719us/step - loss: 1.6832 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00258: loss improved from 1.68328 to 1.68319, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 259/2000\n",
      "93/93 [==============================] - 0s 746us/step - loss: 1.6846 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00259: loss did not improve from 1.68319\n",
      "Epoch 260/2000\n",
      "93/93 [==============================] - 0s 725us/step - loss: 1.6835 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00260: loss did not improve from 1.68319\n",
      "Epoch 261/2000\n",
      "93/93 [==============================] - 0s 773us/step - loss: 1.6832 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00261: loss improved from 1.68319 to 1.68316, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 262/2000\n",
      "93/93 [==============================] - 0s 736us/step - loss: 1.6830 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00262: loss improved from 1.68316 to 1.68301, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 263/2000\n",
      "93/93 [==============================] - 0s 711us/step - loss: 1.6835 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00263: loss did not improve from 1.68301\n",
      "Epoch 264/2000\n",
      "93/93 [==============================] - 0s 694us/step - loss: 1.6834 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00264: loss did not improve from 1.68301\n",
      "Epoch 265/2000\n",
      "93/93 [==============================] - 0s 705us/step - loss: 1.6831 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00265: loss did not improve from 1.68301\n",
      "Epoch 266/2000\n",
      "93/93 [==============================] - 0s 710us/step - loss: 1.6835 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00266: loss did not improve from 1.68301\n",
      "Epoch 267/2000\n",
      "93/93 [==============================] - 0s 785us/step - loss: 1.6829 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00267: loss improved from 1.68301 to 1.68289, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 268/2000\n",
      "93/93 [==============================] - 0s 706us/step - loss: 1.6828 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00268: loss improved from 1.68289 to 1.68283, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 269/2000\n",
      "93/93 [==============================] - 0s 744us/step - loss: 1.6832 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00269: loss did not improve from 1.68283\n",
      "Epoch 270/2000\n",
      "93/93 [==============================] - 0s 720us/step - loss: 1.6830 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00270: loss did not improve from 1.68283\n",
      "Epoch 271/2000\n",
      "93/93 [==============================] - 0s 704us/step - loss: 1.6831 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00271: loss did not improve from 1.68283\n",
      "Epoch 272/2000\n",
      "93/93 [==============================] - 0s 706us/step - loss: 1.6832 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00272: loss did not improve from 1.68283\n",
      "Epoch 273/2000\n",
      "93/93 [==============================] - 0s 716us/step - loss: 1.6831 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00273: loss did not improve from 1.68283\n",
      "Epoch 274/2000\n",
      "93/93 [==============================] - 0s 720us/step - loss: 1.6832 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00274: loss did not improve from 1.68283\n",
      "Epoch 275/2000\n",
      "93/93 [==============================] - 0s 717us/step - loss: 1.6831 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00275: loss did not improve from 1.68283\n",
      "Epoch 276/2000\n",
      "93/93 [==============================] - 0s 732us/step - loss: 1.6828 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00276: loss improved from 1.68283 to 1.68278, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 277/2000\n",
      "93/93 [==============================] - 0s 727us/step - loss: 1.6827 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00277: loss improved from 1.68278 to 1.68275, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 278/2000\n",
      "93/93 [==============================] - 0s 727us/step - loss: 1.6827 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00278: loss improved from 1.68275 to 1.68273, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 279/2000\n",
      "93/93 [==============================] - 0s 736us/step - loss: 1.6826 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00279: loss improved from 1.68273 to 1.68265, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 280/2000\n",
      "93/93 [==============================] - 0s 711us/step - loss: 1.6823 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00280: loss improved from 1.68265 to 1.68231, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 281/2000\n",
      "93/93 [==============================] - 0s 722us/step - loss: 1.6827 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00281: loss did not improve from 1.68231\n",
      "Epoch 282/2000\n",
      "93/93 [==============================] - 0s 708us/step - loss: 1.6825 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00282: loss did not improve from 1.68231\n",
      "Epoch 283/2000\n",
      "93/93 [==============================] - 0s 717us/step - loss: 1.6829 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00283: loss did not improve from 1.68231\n",
      "Epoch 284/2000\n",
      "93/93 [==============================] - 0s 719us/step - loss: 1.6830 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00284: loss did not improve from 1.68231\n",
      "Epoch 285/2000\n",
      "93/93 [==============================] - 0s 728us/step - loss: 1.6825 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00285: loss did not improve from 1.68231\n",
      "Epoch 286/2000\n",
      "93/93 [==============================] - 0s 728us/step - loss: 1.6835 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00286: loss did not improve from 1.68231\n",
      "Epoch 287/2000\n",
      "93/93 [==============================] - 0s 716us/step - loss: 1.6862 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00287: loss did not improve from 1.68231\n",
      "Epoch 288/2000\n",
      "93/93 [==============================] - 0s 684us/step - loss: 1.6824 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00288: loss did not improve from 1.68231\n",
      "Epoch 289/2000\n",
      "93/93 [==============================] - 0s 696us/step - loss: 1.6822 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00289: loss improved from 1.68231 to 1.68220, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 290/2000\n",
      "93/93 [==============================] - 0s 726us/step - loss: 1.6843 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00290: loss did not improve from 1.68220\n",
      "Epoch 291/2000\n",
      "93/93 [==============================] - 0s 788us/step - loss: 1.6988 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00291: loss did not improve from 1.68220\n",
      "Epoch 292/2000\n",
      "93/93 [==============================] - 0s 723us/step - loss: 1.6833 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00292: loss did not improve from 1.68220\n",
      "Epoch 293/2000\n",
      "93/93 [==============================] - 0s 745us/step - loss: 1.6865 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00293: loss did not improve from 1.68220\n",
      "Epoch 294/2000\n",
      "93/93 [==============================] - 0s 716us/step - loss: 1.9183 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00294: loss did not improve from 1.68220\n",
      "Epoch 295/2000\n",
      "93/93 [==============================] - 0s 794us/step - loss: 1.6848 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00295: loss did not improve from 1.68220\n",
      "Epoch 296/2000\n",
      "93/93 [==============================] - 0s 743us/step - loss: 1.6820 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00296: loss improved from 1.68220 to 1.68198, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 297/2000\n",
      "93/93 [==============================] - 0s 769us/step - loss: 1.6822 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00297: loss did not improve from 1.68198\n",
      "Epoch 298/2000\n",
      "93/93 [==============================] - 0s 708us/step - loss: 1.6816 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00298: loss improved from 1.68198 to 1.68157, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 299/2000\n",
      "93/93 [==============================] - 0s 713us/step - loss: 1.6819 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00299: loss did not improve from 1.68157\n",
      "Epoch 300/2000\n",
      "93/93 [==============================] - 0s 718us/step - loss: 1.6817 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00300: loss did not improve from 1.68157\n",
      "Epoch 301/2000\n",
      "93/93 [==============================] - 0s 728us/step - loss: 1.6818 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00301: loss did not improve from 1.68157\n",
      "Epoch 302/2000\n",
      "93/93 [==============================] - 0s 738us/step - loss: 1.6818 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00302: loss did not improve from 1.68157\n",
      "Epoch 303/2000\n",
      "93/93 [==============================] - 0s 719us/step - loss: 1.6819 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00303: loss did not improve from 1.68157\n",
      "Epoch 304/2000\n",
      "93/93 [==============================] - 0s 869us/step - loss: 1.6817 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00304: loss did not improve from 1.68157\n",
      "Epoch 305/2000\n",
      "93/93 [==============================] - 0s 921us/step - loss: 1.6815 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00305: loss improved from 1.68157 to 1.68152, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 306/2000\n",
      "93/93 [==============================] - 0s 880us/step - loss: 1.6818 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00306: loss did not improve from 1.68152\n",
      "Epoch 307/2000\n",
      "93/93 [==============================] - 0s 701us/step - loss: 1.6818 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00307: loss did not improve from 1.68152\n",
      "Epoch 308/2000\n",
      "93/93 [==============================] - 0s 706us/step - loss: 1.6818 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00308: loss did not improve from 1.68152\n",
      "Epoch 309/2000\n",
      "93/93 [==============================] - 0s 703us/step - loss: 1.6815 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00309: loss did not improve from 1.68152\n",
      "Epoch 310/2000\n",
      "93/93 [==============================] - 0s 713us/step - loss: 1.6819 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00310: loss did not improve from 1.68152\n",
      "Epoch 311/2000\n",
      "93/93 [==============================] - 0s 727us/step - loss: 1.6819 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00311: loss did not improve from 1.68152\n",
      "Epoch 312/2000\n",
      "93/93 [==============================] - 0s 697us/step - loss: 1.6817 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00312: loss did not improve from 1.68152\n",
      "Epoch 313/2000\n",
      "93/93 [==============================] - 0s 690us/step - loss: 1.6819 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00313: loss did not improve from 1.68152\n",
      "Epoch 314/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6817 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00314: loss did not improve from 1.68152\n",
      "Epoch 315/2000\n",
      "93/93 [==============================] - 0s 702us/step - loss: 1.6817 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00315: loss did not improve from 1.68152\n",
      "Epoch 316/2000\n",
      "93/93 [==============================] - 0s 670us/step - loss: 1.6817 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00316: loss did not improve from 1.68152\n",
      "Epoch 317/2000\n",
      "93/93 [==============================] - 0s 712us/step - loss: 1.6817 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00317: loss did not improve from 1.68152\n",
      "Epoch 318/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6818 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00318: loss did not improve from 1.68152\n",
      "Epoch 319/2000\n",
      "93/93 [==============================] - 0s 714us/step - loss: 1.6814 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00319: loss improved from 1.68152 to 1.68141, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 320/2000\n",
      "93/93 [==============================] - 0s 699us/step - loss: 1.6816 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00320: loss did not improve from 1.68141\n",
      "Epoch 321/2000\n",
      "93/93 [==============================] - 0s 704us/step - loss: 1.6814 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00321: loss improved from 1.68141 to 1.68137, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 322/2000\n",
      "93/93 [==============================] - 0s 736us/step - loss: 1.6820 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00322: loss did not improve from 1.68137\n",
      "Epoch 323/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6816 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00323: loss did not improve from 1.68137\n",
      "Epoch 324/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6815 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00324: loss did not improve from 1.68137\n",
      "Epoch 325/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6816 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00325: loss did not improve from 1.68137\n",
      "Epoch 326/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6814 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00326: loss did not improve from 1.68137\n",
      "Epoch 327/2000\n",
      "93/93 [==============================] - 0s 579us/step - loss: 1.6816 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00327: loss did not improve from 1.68137\n",
      "Epoch 328/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6813 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00328: loss improved from 1.68137 to 1.68126, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 329/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6816 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00329: loss did not improve from 1.68126\n",
      "Epoch 330/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6815 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00330: loss did not improve from 1.68126\n",
      "Epoch 331/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6817 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00331: loss did not improve from 1.68126\n",
      "Epoch 332/2000\n",
      "93/93 [==============================] - 0s 660us/step - loss: 1.6815 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00332: loss did not improve from 1.68126\n",
      "Epoch 333/2000\n",
      "93/93 [==============================] - 0s 632us/step - loss: 1.6817 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00333: loss did not improve from 1.68126\n",
      "Epoch 334/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6813 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00334: loss did not improve from 1.68126\n",
      "Epoch 335/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 0s 645us/step - loss: 1.6815 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00335: loss did not improve from 1.68126\n",
      "Epoch 336/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6814 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00336: loss did not improve from 1.68126\n",
      "Epoch 337/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6813 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00337: loss did not improve from 1.68126\n",
      "Epoch 338/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6813 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00338: loss did not improve from 1.68126\n",
      "Epoch 339/2000\n",
      "93/93 [==============================] - 0s 645us/step - loss: 1.6816 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00339: loss did not improve from 1.68126\n",
      "Epoch 340/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6814 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00340: loss did not improve from 1.68126\n",
      "Epoch 341/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6811 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00341: loss improved from 1.68126 to 1.68106, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 342/2000\n",
      "93/93 [==============================] - 0s 656us/step - loss: 1.6816 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00342: loss did not improve from 1.68106\n",
      "Epoch 343/2000\n",
      "93/93 [==============================] - 0s 695us/step - loss: 1.6826 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00343: loss did not improve from 1.68106\n",
      "Epoch 344/2000\n",
      "93/93 [==============================] - 0s 709us/step - loss: 1.6813 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00344: loss did not improve from 1.68106\n",
      "Epoch 345/2000\n",
      "93/93 [==============================] - 0s 695us/step - loss: 1.6812 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00345: loss did not improve from 1.68106\n",
      "Epoch 346/2000\n",
      "93/93 [==============================] - 0s 685us/step - loss: 1.6813 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00346: loss did not improve from 1.68106\n",
      "Epoch 347/2000\n",
      "93/93 [==============================] - 0s 670us/step - loss: 1.6814 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00347: loss did not improve from 1.68106\n",
      "Epoch 348/2000\n",
      "93/93 [==============================] - 0s 692us/step - loss: 1.6816 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00348: loss did not improve from 1.68106\n",
      "Epoch 349/2000\n",
      "93/93 [==============================] - 0s 679us/step - loss: 1.6816 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00349: loss did not improve from 1.68106\n",
      "Epoch 350/2000\n",
      "93/93 [==============================] - 0s 706us/step - loss: 1.6816 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00350: loss did not improve from 1.68106\n",
      "Epoch 351/2000\n",
      "93/93 [==============================] - 0s 692us/step - loss: 1.6811 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00351: loss did not improve from 1.68106\n",
      "Epoch 352/2000\n",
      "93/93 [==============================] - 0s 653us/step - loss: 1.6815 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00352: loss did not improve from 1.68106\n",
      "Epoch 353/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6811 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00353: loss did not improve from 1.68106\n",
      "Epoch 354/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6815 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00354: loss did not improve from 1.68106\n",
      "Epoch 355/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6812 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00355: loss did not improve from 1.68106\n",
      "Epoch 356/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6811 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00356: loss did not improve from 1.68106\n",
      "Epoch 357/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6819 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00357: loss did not improve from 1.68106\n",
      "Epoch 358/2000\n",
      "93/93 [==============================] - 0s 655us/step - loss: 1.6809 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00358: loss improved from 1.68106 to 1.68090, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 359/2000\n",
      "93/93 [==============================] - 0s 641us/step - loss: 1.6814 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00359: loss did not improve from 1.68090\n",
      "Epoch 360/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6817 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00360: loss did not improve from 1.68090\n",
      "Epoch 361/2000\n",
      "93/93 [==============================] - 0s 655us/step - loss: 1.6815 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00361: loss did not improve from 1.68090\n",
      "Epoch 362/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6812 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00362: loss did not improve from 1.68090\n",
      "Epoch 363/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6814 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00363: loss did not improve from 1.68090\n",
      "Epoch 364/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6811 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00364: loss did not improve from 1.68090\n",
      "Epoch 365/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6810 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00365: loss did not improve from 1.68090\n",
      "Epoch 366/2000\n",
      "93/93 [==============================] - 0s 641us/step - loss: 1.6814 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00366: loss did not improve from 1.68090\n",
      "Epoch 367/2000\n",
      "93/93 [==============================] - 0s 634us/step - loss: 1.6825 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00367: loss did not improve from 1.68090\n",
      "Epoch 368/2000\n",
      "93/93 [==============================] - 0s 644us/step - loss: 1.6810 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00368: loss did not improve from 1.68090\n",
      "Epoch 369/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6809 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00369: loss improved from 1.68090 to 1.68086, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 370/2000\n",
      "93/93 [==============================] - 0s 652us/step - loss: 1.6813 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00370: loss did not improve from 1.68086\n",
      "Epoch 371/2000\n",
      "93/93 [==============================] - 0s 699us/step - loss: 1.6811 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00371: loss did not improve from 1.68086\n",
      "Epoch 372/2000\n",
      "93/93 [==============================] - 0s 687us/step - loss: 1.6809 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00372: loss did not improve from 1.68086\n",
      "Epoch 373/2000\n",
      "93/93 [==============================] - 0s 674us/step - loss: 1.6810 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00373: loss did not improve from 1.68086\n",
      "Epoch 374/2000\n",
      "93/93 [==============================] - 0s 717us/step - loss: 1.6808 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00374: loss improved from 1.68086 to 1.68079, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 375/2000\n",
      "93/93 [==============================] - 0s 677us/step - loss: 1.6809 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00375: loss did not improve from 1.68079\n",
      "Epoch 376/2000\n",
      "93/93 [==============================] - 0s 693us/step - loss: 1.6813 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00376: loss did not improve from 1.68079\n",
      "Epoch 377/2000\n",
      "93/93 [==============================] - 0s 705us/step - loss: 1.6807 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00377: loss improved from 1.68079 to 1.68074, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 378/2000\n",
      "93/93 [==============================] - 0s 697us/step - loss: 1.6811 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00378: loss did not improve from 1.68074\n",
      "Epoch 379/2000\n",
      "93/93 [==============================] - 0s 718us/step - loss: 1.6809 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00379: loss did not improve from 1.68074\n",
      "Epoch 380/2000\n",
      "93/93 [==============================] - 0s 761us/step - loss: 1.6805 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00380: loss improved from 1.68074 to 1.68051, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 381/2000\n",
      "93/93 [==============================] - 0s 708us/step - loss: 1.6805 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00381: loss did not improve from 1.68051\n",
      "Epoch 382/2000\n",
      "93/93 [==============================] - 0s 778us/step - loss: 1.6808 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00382: loss did not improve from 1.68051\n",
      "Epoch 383/2000\n",
      "93/93 [==============================] - 0s 721us/step - loss: 1.6806 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00383: loss did not improve from 1.68051\n",
      "Epoch 384/2000\n",
      "93/93 [==============================] - 0s 717us/step - loss: 1.6808 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00384: loss did not improve from 1.68051\n",
      "Epoch 385/2000\n",
      "93/93 [==============================] - 0s 728us/step - loss: 1.6857 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00385: loss did not improve from 1.68051\n",
      "Epoch 386/2000\n",
      "93/93 [==============================] - 0s 688us/step - loss: 1.6805 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00386: loss did not improve from 1.68051\n",
      "Epoch 387/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 0s 671us/step - loss: 1.6807 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00387: loss did not improve from 1.68051\n",
      "Epoch 388/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6807 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00388: loss did not improve from 1.68051\n",
      "Epoch 389/2000\n",
      "93/93 [==============================] - 0s 664us/step - loss: 1.6805 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00389: loss did not improve from 1.68051\n",
      "Epoch 390/2000\n",
      "93/93 [==============================] - 0s 696us/step - loss: 1.6806 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00390: loss did not improve from 1.68051\n",
      "Epoch 391/2000\n",
      "93/93 [==============================] - 0s 679us/step - loss: 1.6811 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00391: loss did not improve from 1.68051\n",
      "Epoch 392/2000\n",
      "93/93 [==============================] - 0s 651us/step - loss: 1.6805 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00392: loss did not improve from 1.68051\n",
      "Epoch 393/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6810 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00393: loss did not improve from 1.68051\n",
      "Epoch 394/2000\n",
      "93/93 [==============================] - 0s 644us/step - loss: 1.6803 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00394: loss improved from 1.68051 to 1.68033, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 395/2000\n",
      "93/93 [==============================] - 0s 699us/step - loss: 1.6806 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00395: loss did not improve from 1.68033\n",
      "Epoch 396/2000\n",
      "93/93 [==============================] - 0s 805us/step - loss: 1.6803 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00396: loss improved from 1.68033 to 1.68032, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 397/2000\n",
      "93/93 [==============================] - 0s 743us/step - loss: 1.6805 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00397: loss did not improve from 1.68032\n",
      "Epoch 398/2000\n",
      "93/93 [==============================] - 0s 753us/step - loss: 1.6803 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00398: loss improved from 1.68032 to 1.68030, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 399/2000\n",
      "93/93 [==============================] - 0s 774us/step - loss: 1.6808 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00399: loss did not improve from 1.68030\n",
      "Epoch 400/2000\n",
      "93/93 [==============================] - 0s 710us/step - loss: 1.6811 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00400: loss did not improve from 1.68030\n",
      "Epoch 401/2000\n",
      "93/93 [==============================] - 0s 751us/step - loss: 1.6805 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00401: loss did not improve from 1.68030\n",
      "Epoch 402/2000\n",
      "93/93 [==============================] - 0s 708us/step - loss: 1.6803 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00402: loss improved from 1.68030 to 1.68028, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 403/2000\n",
      "93/93 [==============================] - 0s 684us/step - loss: 1.6835 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00403: loss did not improve from 1.68028\n",
      "Epoch 404/2000\n",
      "93/93 [==============================] - 0s 673us/step - loss: 1.6818 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00404: loss did not improve from 1.68028\n",
      "Epoch 405/2000\n",
      "93/93 [==============================] - 0s 671us/step - loss: 1.6806 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00405: loss did not improve from 1.68028\n",
      "Epoch 406/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6808 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00406: loss did not improve from 1.68028\n",
      "Epoch 407/2000\n",
      "93/93 [==============================] - 0s 675us/step - loss: 1.6803 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00407: loss did not improve from 1.68028\n",
      "Epoch 408/2000\n",
      "93/93 [==============================] - 0s 695us/step - loss: 1.6806 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00408: loss did not improve from 1.68028\n",
      "Epoch 409/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6802 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00409: loss improved from 1.68028 to 1.68025, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 410/2000\n",
      "93/93 [==============================] - 0s 687us/step - loss: 1.6803 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00410: loss did not improve from 1.68025\n",
      "Epoch 411/2000\n",
      "93/93 [==============================] - 0s 685us/step - loss: 1.6802 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00411: loss improved from 1.68025 to 1.68021, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 412/2000\n",
      "93/93 [==============================] - 0s 687us/step - loss: 1.6806 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00412: loss did not improve from 1.68021\n",
      "Epoch 413/2000\n",
      "93/93 [==============================] - 0s 694us/step - loss: 1.6805 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00413: loss did not improve from 1.68021\n",
      "Epoch 414/2000\n",
      "93/93 [==============================] - 0s 673us/step - loss: 1.6803 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00414: loss did not improve from 1.68021\n",
      "Epoch 415/2000\n",
      "93/93 [==============================] - 0s 674us/step - loss: 1.6803 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00415: loss did not improve from 1.68021\n",
      "Epoch 416/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6801 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00416: loss improved from 1.68021 to 1.68015, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 417/2000\n",
      "93/93 [==============================] - 0s 677us/step - loss: 1.6802 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00417: loss did not improve from 1.68015\n",
      "Epoch 418/2000\n",
      "93/93 [==============================] - 0s 677us/step - loss: 1.6855 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00418: loss did not improve from 1.68015\n",
      "Epoch 419/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6808 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00419: loss did not improve from 1.68015\n",
      "Epoch 420/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6805 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00420: loss did not improve from 1.68015\n",
      "Epoch 421/2000\n",
      "93/93 [==============================] - 0s 669us/step - loss: 1.6815 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00421: loss did not improve from 1.68015\n",
      "Epoch 422/2000\n",
      "93/93 [==============================] - 0s 688us/step - loss: 1.6807 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00422: loss did not improve from 1.68015\n",
      "Epoch 423/2000\n",
      "93/93 [==============================] - 0s 679us/step - loss: 1.6840 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00423: loss did not improve from 1.68015\n",
      "Epoch 424/2000\n",
      "93/93 [==============================] - 0s 706us/step - loss: 1.6803 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00424: loss did not improve from 1.68015\n",
      "Epoch 425/2000\n",
      "93/93 [==============================] - 0s 697us/step - loss: 1.6804 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00425: loss did not improve from 1.68015\n",
      "Epoch 426/2000\n",
      "93/93 [==============================] - 0s 735us/step - loss: 1.6805 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00426: loss did not improve from 1.68015\n",
      "Epoch 427/2000\n",
      "93/93 [==============================] - 0s 703us/step - loss: 1.6804 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00427: loss did not improve from 1.68015\n",
      "Epoch 428/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6802 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00428: loss did not improve from 1.68015\n",
      "Epoch 429/2000\n",
      "93/93 [==============================] - 0s 670us/step - loss: 1.6804 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00429: loss did not improve from 1.68015\n",
      "Epoch 430/2000\n",
      "93/93 [==============================] - 0s 664us/step - loss: 1.6852 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00430: loss did not improve from 1.68015\n",
      "Epoch 431/2000\n",
      "93/93 [==============================] - 0s 750us/step - loss: 1.6810 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00431: loss did not improve from 1.68015\n",
      "Epoch 432/2000\n",
      "93/93 [==============================] - 0s 728us/step - loss: 1.6814 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00432: loss did not improve from 1.68015\n",
      "Epoch 433/2000\n",
      "93/93 [==============================] - 0s 717us/step - loss: 1.6806 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00433: loss did not improve from 1.68015\n",
      "Epoch 434/2000\n",
      "93/93 [==============================] - 0s 697us/step - loss: 1.6805 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00434: loss did not improve from 1.68015\n",
      "Epoch 435/2000\n",
      "93/93 [==============================] - 0s 740us/step - loss: 1.6815 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00435: loss did not improve from 1.68015\n",
      "Epoch 436/2000\n",
      "93/93 [==============================] - 0s 717us/step - loss: 1.6803 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00436: loss did not improve from 1.68015\n",
      "Epoch 437/2000\n",
      "93/93 [==============================] - 0s 719us/step - loss: 1.6801 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00437: loss improved from 1.68015 to 1.68011, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 438/2000\n",
      "93/93 [==============================] - 0s 713us/step - loss: 1.6798 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00438: loss improved from 1.68011 to 1.67977, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 439/2000\n",
      "93/93 [==============================] - 0s 743us/step - loss: 1.6797 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00439: loss improved from 1.67977 to 1.67971, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 440/2000\n",
      "93/93 [==============================] - 0s 704us/step - loss: 1.6815 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00440: loss did not improve from 1.67971\n",
      "Epoch 441/2000\n",
      "93/93 [==============================] - 0s 675us/step - loss: 1.6807 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00441: loss did not improve from 1.67971\n",
      "Epoch 442/2000\n",
      "93/93 [==============================] - 0s 677us/step - loss: 1.6800 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00442: loss did not improve from 1.67971\n",
      "Epoch 443/2000\n",
      "93/93 [==============================] - 0s 697us/step - loss: 1.6800 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00443: loss did not improve from 1.67971\n",
      "Epoch 444/2000\n",
      "93/93 [==============================] - 0s 685us/step - loss: 1.6800 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00444: loss did not improve from 1.67971\n",
      "Epoch 445/2000\n",
      "93/93 [==============================] - 0s 764us/step - loss: 1.6801 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00445: loss did not improve from 1.67971\n",
      "Epoch 446/2000\n",
      "93/93 [==============================] - 0s 692us/step - loss: 1.6798 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00446: loss did not improve from 1.67971\n",
      "Epoch 447/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6799 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00447: loss did not improve from 1.67971\n",
      "Epoch 448/2000\n",
      "93/93 [==============================] - 0s 704us/step - loss: 1.6796 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00448: loss improved from 1.67971 to 1.67964, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 449/2000\n",
      "93/93 [==============================] - 0s 685us/step - loss: 1.6799 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00449: loss did not improve from 1.67964\n",
      "Epoch 450/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6799 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00450: loss did not improve from 1.67964\n",
      "Epoch 451/2000\n",
      "93/93 [==============================] - 0s 685us/step - loss: 1.6799 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00451: loss did not improve from 1.67964\n",
      "Epoch 452/2000\n",
      "93/93 [==============================] - 0s 707us/step - loss: 1.6799 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00452: loss did not improve from 1.67964\n",
      "Epoch 453/2000\n",
      "93/93 [==============================] - 0s 718us/step - loss: 1.6825 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00453: loss did not improve from 1.67964\n",
      "Epoch 454/2000\n",
      "93/93 [==============================] - 0s 743us/step - loss: 1.6798 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00454: loss did not improve from 1.67964\n",
      "Epoch 455/2000\n",
      "93/93 [==============================] - 0s 719us/step - loss: 1.6795 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00455: loss improved from 1.67964 to 1.67950, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 456/2000\n",
      "93/93 [==============================] - 0s 699us/step - loss: 1.6802 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00456: loss did not improve from 1.67950\n",
      "Epoch 457/2000\n",
      "93/93 [==============================] - 0s 717us/step - loss: 1.6799 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00457: loss did not improve from 1.67950\n",
      "Epoch 458/2000\n",
      "93/93 [==============================] - 0s 685us/step - loss: 1.6798 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00458: loss did not improve from 1.67950\n",
      "Epoch 459/2000\n",
      "93/93 [==============================] - 0s 696us/step - loss: 1.6798 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00459: loss did not improve from 1.67950\n",
      "Epoch 460/2000\n",
      "93/93 [==============================] - 0s 706us/step - loss: 1.6799 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00460: loss did not improve from 1.67950\n",
      "Epoch 461/2000\n",
      "93/93 [==============================] - 0s 675us/step - loss: 1.6797 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00461: loss did not improve from 1.67950\n",
      "Epoch 462/2000\n",
      "93/93 [==============================] - 0s 675us/step - loss: 1.6797 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00462: loss did not improve from 1.67950\n",
      "Epoch 463/2000\n",
      "93/93 [==============================] - 0s 672us/step - loss: 1.6796 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00463: loss did not improve from 1.67950\n",
      "Epoch 464/2000\n",
      "93/93 [==============================] - 0s 741us/step - loss: 1.6796 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00464: loss did not improve from 1.67950\n",
      "Epoch 465/2000\n",
      "93/93 [==============================] - 0s 858us/step - loss: 1.6800 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00465: loss did not improve from 1.67950\n",
      "Epoch 466/2000\n",
      "93/93 [==============================] - 0s 789us/step - loss: 1.6796 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00466: loss did not improve from 1.67950\n",
      "Epoch 467/2000\n",
      "93/93 [==============================] - 0s 740us/step - loss: 1.6795 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00467: loss did not improve from 1.67950\n",
      "Epoch 468/2000\n",
      "93/93 [==============================] - 0s 772us/step - loss: 1.6795 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00468: loss did not improve from 1.67950\n",
      "Epoch 469/2000\n",
      "93/93 [==============================] - 0s 826us/step - loss: 1.6801 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00469: loss did not improve from 1.67950\n",
      "Epoch 470/2000\n",
      "93/93 [==============================] - 0s 761us/step - loss: 1.6796 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00470: loss did not improve from 1.67950\n",
      "Epoch 471/2000\n",
      "93/93 [==============================] - 0s 762us/step - loss: 1.6796 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00471: loss did not improve from 1.67950\n",
      "Epoch 472/2000\n",
      "93/93 [==============================] - 0s 728us/step - loss: 1.6808 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00472: loss did not improve from 1.67950\n",
      "Epoch 473/2000\n",
      "93/93 [==============================] - 0s 707us/step - loss: 1.6804 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00473: loss did not improve from 1.67950\n",
      "Epoch 474/2000\n",
      "93/93 [==============================] - 0s 653us/step - loss: 1.6799 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00474: loss did not improve from 1.67950\n",
      "Epoch 475/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6796 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00475: loss did not improve from 1.67950\n",
      "Epoch 476/2000\n",
      "93/93 [==============================] - 0s 587us/step - loss: 1.6796 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00476: loss did not improve from 1.67950\n",
      "Epoch 477/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6795 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00477: loss did not improve from 1.67950\n",
      "Epoch 478/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6795 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00478: loss did not improve from 1.67950\n",
      "Epoch 479/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6797 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00479: loss did not improve from 1.67950\n",
      "Epoch 480/2000\n",
      "93/93 [==============================] - 0s 584us/step - loss: 1.6798 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00480: loss did not improve from 1.67950\n",
      "Epoch 481/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6856 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00481: loss did not improve from 1.67950\n",
      "Epoch 482/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6803 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00482: loss did not improve from 1.67950\n",
      "Epoch 483/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6799 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00483: loss did not improve from 1.67950\n",
      "Epoch 484/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6797 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00484: loss did not improve from 1.67950\n",
      "Epoch 485/2000\n",
      "93/93 [==============================] - 0s 578us/step - loss: 1.6794 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00485: loss improved from 1.67950 to 1.67943, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 486/2000\n",
      "93/93 [==============================] - 0s 639us/step - loss: 1.6792 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00486: loss improved from 1.67943 to 1.67921, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 487/2000\n",
      "93/93 [==============================] - 0s 630us/step - loss: 1.6796 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00487: loss did not improve from 1.67921\n",
      "Epoch 488/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6794 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00488: loss did not improve from 1.67921\n",
      "Epoch 489/2000\n",
      "93/93 [==============================] - 0s 642us/step - loss: 1.6794 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00489: loss did not improve from 1.67921\n",
      "Epoch 490/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 0s 655us/step - loss: 1.6795 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00490: loss did not improve from 1.67921\n",
      "Epoch 491/2000\n",
      "93/93 [==============================] - 0s 740us/step - loss: 1.6793 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00491: loss did not improve from 1.67921\n",
      "Epoch 492/2000\n",
      "93/93 [==============================] - 0s 674us/step - loss: 1.6796 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00492: loss did not improve from 1.67921\n",
      "Epoch 493/2000\n",
      "93/93 [==============================] - 0s 664us/step - loss: 1.6798 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00493: loss did not improve from 1.67921\n",
      "Epoch 494/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6806 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00494: loss did not improve from 1.67921\n",
      "Epoch 495/2000\n",
      "93/93 [==============================] - 0s 707us/step - loss: 1.6794 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00495: loss did not improve from 1.67921\n",
      "Epoch 496/2000\n",
      "93/93 [==============================] - 0s 677us/step - loss: 1.6795 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00496: loss did not improve from 1.67921\n",
      "Epoch 497/2000\n",
      "93/93 [==============================] - 0s 729us/step - loss: 1.6793 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00497: loss did not improve from 1.67921\n",
      "Epoch 498/2000\n",
      "93/93 [==============================] - 0s 664us/step - loss: 1.6793 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00498: loss did not improve from 1.67921\n",
      "Epoch 499/2000\n",
      "93/93 [==============================] - 0s 652us/step - loss: 1.6795 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00499: loss did not improve from 1.67921\n",
      "Epoch 500/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6792 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00500: loss did not improve from 1.67921\n",
      "Epoch 501/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6791 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00501: loss improved from 1.67921 to 1.67914, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 502/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6793 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00502: loss did not improve from 1.67914\n",
      "Epoch 503/2000\n",
      "93/93 [==============================] - 0s 691us/step - loss: 1.6789 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00503: loss improved from 1.67914 to 1.67893, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 504/2000\n",
      "93/93 [==============================] - 0s 735us/step - loss: 1.6805 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00504: loss did not improve from 1.67893\n",
      "Epoch 505/2000\n",
      "93/93 [==============================] - 0s 725us/step - loss: 1.6798 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00505: loss did not improve from 1.67893\n",
      "Epoch 506/2000\n",
      "93/93 [==============================] - 0s 737us/step - loss: 1.6799 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00506: loss did not improve from 1.67893\n",
      "Epoch 507/2000\n",
      "93/93 [==============================] - 0s 714us/step - loss: 1.6795 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00507: loss did not improve from 1.67893\n",
      "Epoch 508/2000\n",
      "93/93 [==============================] - 0s 704us/step - loss: 1.6801 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00508: loss did not improve from 1.67893\n",
      "Epoch 509/2000\n",
      "93/93 [==============================] - 0s 706us/step - loss: 1.6795 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00509: loss did not improve from 1.67893\n",
      "Epoch 510/2000\n",
      "93/93 [==============================] - 0s 719us/step - loss: 1.6793 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00510: loss did not improve from 1.67893\n",
      "Epoch 511/2000\n",
      "93/93 [==============================] - 0s 714us/step - loss: 1.6795 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00511: loss did not improve from 1.67893\n",
      "Epoch 512/2000\n",
      "93/93 [==============================] - 0s 720us/step - loss: 1.6792 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00512: loss did not improve from 1.67893\n",
      "Epoch 513/2000\n",
      "93/93 [==============================] - 0s 707us/step - loss: 1.6791 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00513: loss did not improve from 1.67893\n",
      "Epoch 514/2000\n",
      "93/93 [==============================] - 0s 714us/step - loss: 1.6793 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00514: loss did not improve from 1.67893\n",
      "Epoch 515/2000\n",
      "93/93 [==============================] - 0s 704us/step - loss: 1.6793 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00515: loss did not improve from 1.67893\n",
      "Epoch 516/2000\n",
      "93/93 [==============================] - 0s 698us/step - loss: 1.6791 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00516: loss did not improve from 1.67893\n",
      "Epoch 517/2000\n",
      "93/93 [==============================] - 0s 702us/step - loss: 1.6793 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00517: loss did not improve from 1.67893\n",
      "Epoch 518/2000\n",
      "93/93 [==============================] - 0s 775us/step - loss: 1.6790 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00518: loss did not improve from 1.67893\n",
      "Epoch 519/2000\n",
      "93/93 [==============================] - 0s 846us/step - loss: 1.6792 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00519: loss did not improve from 1.67893\n",
      "Epoch 520/2000\n",
      "93/93 [==============================] - 0s 742us/step - loss: 1.6792 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00520: loss did not improve from 1.67893\n",
      "Epoch 521/2000\n",
      "93/93 [==============================] - 0s 718us/step - loss: 1.6792 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00521: loss did not improve from 1.67893\n",
      "Epoch 522/2000\n",
      "93/93 [==============================] - 0s 728us/step - loss: 1.6795 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00522: loss did not improve from 1.67893\n",
      "Epoch 523/2000\n",
      "93/93 [==============================] - 0s 718us/step - loss: 1.6791 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00523: loss did not improve from 1.67893\n",
      "Epoch 524/2000\n",
      "93/93 [==============================] - 0s 707us/step - loss: 1.6793 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00524: loss did not improve from 1.67893\n",
      "Epoch 525/2000\n",
      "93/93 [==============================] - 0s 718us/step - loss: 1.6792 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00525: loss did not improve from 1.67893\n",
      "Epoch 526/2000\n",
      "93/93 [==============================] - 0s 720us/step - loss: 1.6791 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00526: loss did not improve from 1.67893\n",
      "Epoch 527/2000\n",
      "93/93 [==============================] - 0s 726us/step - loss: 1.6790 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00527: loss did not improve from 1.67893\n",
      "Epoch 528/2000\n",
      "93/93 [==============================] - 0s 730us/step - loss: 1.6790 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00528: loss did not improve from 1.67893\n",
      "Epoch 529/2000\n",
      "93/93 [==============================] - 0s 712us/step - loss: 1.6788 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00529: loss improved from 1.67893 to 1.67878, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 530/2000\n",
      "93/93 [==============================] - 0s 734us/step - loss: 1.6790 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00530: loss did not improve from 1.67878\n",
      "Epoch 531/2000\n",
      "93/93 [==============================] - 0s 708us/step - loss: 1.6793 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00531: loss did not improve from 1.67878\n",
      "Epoch 532/2000\n",
      "93/93 [==============================] - 0s 717us/step - loss: 1.6793 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00532: loss did not improve from 1.67878\n",
      "Epoch 533/2000\n",
      "93/93 [==============================] - 0s 740us/step - loss: 1.6792 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00533: loss did not improve from 1.67878\n",
      "Epoch 534/2000\n",
      "93/93 [==============================] - 0s 738us/step - loss: 1.6792 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00534: loss did not improve from 1.67878\n",
      "Epoch 535/2000\n",
      "93/93 [==============================] - 0s 739us/step - loss: 1.6794 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00535: loss did not improve from 1.67878\n",
      "Epoch 536/2000\n",
      "93/93 [==============================] - 0s 716us/step - loss: 1.6791 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00536: loss did not improve from 1.67878\n",
      "Epoch 537/2000\n",
      "93/93 [==============================] - 0s 713us/step - loss: 1.6790 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00537: loss did not improve from 1.67878\n",
      "Epoch 538/2000\n",
      "93/93 [==============================] - 0s 734us/step - loss: 1.6791 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00538: loss did not improve from 1.67878\n",
      "Epoch 539/2000\n",
      "93/93 [==============================] - 0s 717us/step - loss: 1.6794 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00539: loss did not improve from 1.67878\n",
      "Epoch 540/2000\n",
      "93/93 [==============================] - 0s 711us/step - loss: 1.6794 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00540: loss did not improve from 1.67878\n",
      "Epoch 541/2000\n",
      "93/93 [==============================] - 0s 720us/step - loss: 1.6832 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00541: loss did not improve from 1.67878\n",
      "Epoch 542/2000\n",
      "93/93 [==============================] - 0s 727us/step - loss: 1.6807 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00542: loss did not improve from 1.67878\n",
      "Epoch 543/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 0s 707us/step - loss: 1.6790 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00543: loss did not improve from 1.67878\n",
      "Epoch 544/2000\n",
      "93/93 [==============================] - 0s 718us/step - loss: 1.6791 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00544: loss did not improve from 1.67878\n",
      "Epoch 545/2000\n",
      "93/93 [==============================] - 0s 733us/step - loss: 1.6791 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00545: loss did not improve from 1.67878\n",
      "Epoch 546/2000\n",
      "93/93 [==============================] - 0s 755us/step - loss: 1.6789 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00546: loss did not improve from 1.67878\n",
      "Epoch 547/2000\n",
      "93/93 [==============================] - 0s 727us/step - loss: 1.6787 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00547: loss improved from 1.67878 to 1.67873, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 548/2000\n",
      "93/93 [==============================] - 0s 950us/step - loss: 1.6790 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00548: loss did not improve from 1.67873\n",
      "Epoch 549/2000\n",
      "93/93 [==============================] - 0s 829us/step - loss: 1.6790 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00549: loss did not improve from 1.67873\n",
      "Epoch 550/2000\n",
      "93/93 [==============================] - 0s 710us/step - loss: 1.6788 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00550: loss did not improve from 1.67873\n",
      "Epoch 551/2000\n",
      "93/93 [==============================] - 0s 681us/step - loss: 1.6793 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00551: loss did not improve from 1.67873\n",
      "Epoch 552/2000\n",
      "93/93 [==============================] - 0s 691us/step - loss: 1.6792 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00552: loss did not improve from 1.67873\n",
      "Epoch 553/2000\n",
      "93/93 [==============================] - 0s 697us/step - loss: 1.6805 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00553: loss did not improve from 1.67873\n",
      "Epoch 554/2000\n",
      "93/93 [==============================] - 0s 674us/step - loss: 1.6802 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00554: loss did not improve from 1.67873\n",
      "Epoch 555/2000\n",
      "93/93 [==============================] - 0s 691us/step - loss: 1.6790 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00555: loss did not improve from 1.67873\n",
      "Epoch 556/2000\n",
      "93/93 [==============================] - 0s 855us/step - loss: 1.6789 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00556: loss did not improve from 1.67873\n",
      "Epoch 557/2000\n",
      "93/93 [==============================] - 0s 879us/step - loss: 1.6790 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00557: loss did not improve from 1.67873\n",
      "Epoch 558/2000\n",
      "93/93 [==============================] - 0s 752us/step - loss: 1.6789 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00558: loss did not improve from 1.67873\n",
      "Epoch 559/2000\n",
      "93/93 [==============================] - 0s 689us/step - loss: 1.6789 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00559: loss did not improve from 1.67873\n",
      "Epoch 560/2000\n",
      "93/93 [==============================] - 0s 703us/step - loss: 1.6788 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00560: loss did not improve from 1.67873\n",
      "Epoch 561/2000\n",
      "93/93 [==============================] - 0s 690us/step - loss: 1.6797 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00561: loss did not improve from 1.67873\n",
      "Epoch 562/2000\n",
      "93/93 [==============================] - 0s 663us/step - loss: 2.7123 - accuracy: 0.4194\n",
      "\n",
      "Epoch 00562: loss did not improve from 1.67873\n",
      "Epoch 563/2000\n",
      "93/93 [==============================] - 0s 703us/step - loss: 2.0479 - accuracy: 0.4409\n",
      "\n",
      "Epoch 00563: loss did not improve from 1.67873\n",
      "Epoch 564/2000\n",
      "93/93 [==============================] - 0s 772us/step - loss: 1.7784 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00564: loss did not improve from 1.67873\n",
      "Epoch 565/2000\n",
      "93/93 [==============================] - 0s 921us/step - loss: 1.6800 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00565: loss did not improve from 1.67873\n",
      "Epoch 566/2000\n",
      "93/93 [==============================] - 0s 818us/step - loss: 1.6796 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00566: loss did not improve from 1.67873\n",
      "Epoch 567/2000\n",
      "93/93 [==============================] - 0s 733us/step - loss: 1.6793 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00567: loss did not improve from 1.67873\n",
      "Epoch 568/2000\n",
      "93/93 [==============================] - 0s 706us/step - loss: 1.6791 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00568: loss did not improve from 1.67873\n",
      "Epoch 569/2000\n",
      "93/93 [==============================] - 0s 696us/step - loss: 1.6789 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00569: loss did not improve from 1.67873\n",
      "Epoch 570/2000\n",
      "93/93 [==============================] - 0s 707us/step - loss: 1.6790 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00570: loss did not improve from 1.67873\n",
      "Epoch 571/2000\n",
      "93/93 [==============================] - 0s 707us/step - loss: 1.6792 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00571: loss did not improve from 1.67873\n",
      "Epoch 572/2000\n",
      "93/93 [==============================] - 0s 706us/step - loss: 1.6789 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00572: loss did not improve from 1.67873\n",
      "Epoch 573/2000\n",
      "93/93 [==============================] - 0s 706us/step - loss: 1.6790 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00573: loss did not improve from 1.67873\n",
      "Epoch 574/2000\n",
      "93/93 [==============================] - 0s 687us/step - loss: 1.6790 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00574: loss did not improve from 1.67873\n",
      "Epoch 575/2000\n",
      "93/93 [==============================] - 0s 730us/step - loss: 1.6791 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00575: loss did not improve from 1.67873\n",
      "Epoch 576/2000\n",
      "93/93 [==============================] - 0s 731us/step - loss: 1.6788 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00576: loss did not improve from 1.67873\n",
      "Epoch 577/2000\n",
      "93/93 [==============================] - 0s 710us/step - loss: 1.6784 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00577: loss improved from 1.67873 to 1.67842, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 578/2000\n",
      "93/93 [==============================] - 0s 712us/step - loss: 1.6788 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00578: loss did not improve from 1.67842\n",
      "Epoch 579/2000\n",
      "93/93 [==============================] - 0s 712us/step - loss: 1.6787 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00579: loss did not improve from 1.67842\n",
      "Epoch 580/2000\n",
      "93/93 [==============================] - 0s 750us/step - loss: 1.6786 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00580: loss did not improve from 1.67842\n",
      "Epoch 581/2000\n",
      "93/93 [==============================] - 0s 704us/step - loss: 1.6788 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00581: loss did not improve from 1.67842\n",
      "Epoch 582/2000\n",
      "93/93 [==============================] - 0s 725us/step - loss: 1.6786 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00582: loss did not improve from 1.67842\n",
      "Epoch 583/2000\n",
      "93/93 [==============================] - 0s 901us/step - loss: 1.6786 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00583: loss did not improve from 1.67842\n",
      "Epoch 584/2000\n",
      "93/93 [==============================] - 0s 922us/step - loss: 1.6790 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00584: loss did not improve from 1.67842\n",
      "Epoch 585/2000\n",
      "93/93 [==============================] - 0s 834us/step - loss: 1.6789 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00585: loss did not improve from 1.67842\n",
      "Epoch 586/2000\n",
      "93/93 [==============================] - 0s 709us/step - loss: 1.6789 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00586: loss did not improve from 1.67842\n",
      "Epoch 587/2000\n",
      "93/93 [==============================] - 0s 696us/step - loss: 1.6789 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00587: loss did not improve from 1.67842\n",
      "Epoch 588/2000\n",
      "93/93 [==============================] - 0s 696us/step - loss: 1.6790 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00588: loss did not improve from 1.67842\n",
      "Epoch 589/2000\n",
      "93/93 [==============================] - 0s 706us/step - loss: 1.6788 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00589: loss did not improve from 1.67842\n",
      "Epoch 590/2000\n",
      "93/93 [==============================] - 0s 725us/step - loss: 1.6787 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00590: loss did not improve from 1.67842\n",
      "Epoch 591/2000\n",
      "93/93 [==============================] - 0s 701us/step - loss: 1.6789 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00591: loss did not improve from 1.67842\n",
      "Epoch 592/2000\n",
      "93/93 [==============================] - 0s 707us/step - loss: 1.6789 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00592: loss did not improve from 1.67842\n",
      "Epoch 593/2000\n",
      "93/93 [==============================] - 0s 709us/step - loss: 1.6786 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00593: loss did not improve from 1.67842\n",
      "Epoch 594/2000\n",
      "93/93 [==============================] - 0s 691us/step - loss: 1.6788 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00594: loss did not improve from 1.67842\n",
      "Epoch 595/2000\n",
      "93/93 [==============================] - 0s 707us/step - loss: 1.6785 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00595: loss did not improve from 1.67842\n",
      "Epoch 596/2000\n",
      "93/93 [==============================] - 0s 707us/step - loss: 1.6789 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00596: loss did not improve from 1.67842\n",
      "Epoch 597/2000\n",
      "93/93 [==============================] - 0s 728us/step - loss: 1.6788 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00597: loss did not improve from 1.67842\n",
      "Epoch 598/2000\n",
      "93/93 [==============================] - 0s 708us/step - loss: 1.6785 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00598: loss did not improve from 1.67842\n",
      "Epoch 599/2000\n",
      "93/93 [==============================] - 0s 739us/step - loss: 1.6787 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00599: loss did not improve from 1.67842\n",
      "Epoch 600/2000\n",
      "93/93 [==============================] - 0s 719us/step - loss: 1.6788 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00600: loss did not improve from 1.67842\n",
      "Epoch 601/2000\n",
      "93/93 [==============================] - 0s 908us/step - loss: 1.6786 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00601: loss did not improve from 1.67842\n",
      "Epoch 602/2000\n",
      "93/93 [==============================] - 0s 894us/step - loss: 1.6787 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00602: loss did not improve from 1.67842\n",
      "Epoch 603/2000\n",
      "93/93 [==============================] - 0s 803us/step - loss: 1.6785 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00603: loss did not improve from 1.67842\n",
      "Epoch 604/2000\n",
      "93/93 [==============================] - 0s 695us/step - loss: 1.6786 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00604: loss did not improve from 1.67842\n",
      "Epoch 605/2000\n",
      "93/93 [==============================] - 0s 685us/step - loss: 1.6785 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00605: loss did not improve from 1.67842\n",
      "Epoch 606/2000\n",
      "93/93 [==============================] - 0s 689us/step - loss: 1.6784 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00606: loss did not improve from 1.67842\n",
      "Epoch 607/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6787 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00607: loss did not improve from 1.67842\n",
      "Epoch 608/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6808 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00608: loss did not improve from 1.67842\n",
      "Epoch 609/2000\n",
      "93/93 [==============================] - 0s 719us/step - loss: 1.6813 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00609: loss did not improve from 1.67842\n",
      "Epoch 610/2000\n",
      "93/93 [==============================] - 0s 697us/step - loss: 1.6786 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00610: loss did not improve from 1.67842\n",
      "Epoch 611/2000\n",
      "93/93 [==============================] - 0s 718us/step - loss: 1.6786 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00611: loss did not improve from 1.67842\n",
      "Epoch 612/2000\n",
      "93/93 [==============================] - 0s 718us/step - loss: 1.6788 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00612: loss did not improve from 1.67842\n",
      "Epoch 613/2000\n",
      "93/93 [==============================] - 0s 719us/step - loss: 1.6786 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00613: loss did not improve from 1.67842\n",
      "Epoch 614/2000\n",
      "93/93 [==============================] - 0s 706us/step - loss: 1.6789 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00614: loss did not improve from 1.67842\n",
      "Epoch 615/2000\n",
      "93/93 [==============================] - 0s 709us/step - loss: 1.6786 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00615: loss did not improve from 1.67842\n",
      "Epoch 616/2000\n",
      "93/93 [==============================] - 0s 711us/step - loss: 1.6787 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00616: loss did not improve from 1.67842\n",
      "Epoch 617/2000\n",
      "93/93 [==============================] - 0s 705us/step - loss: 1.6786 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00617: loss did not improve from 1.67842\n",
      "Epoch 618/2000\n",
      "93/93 [==============================] - 0s 707us/step - loss: 1.6787 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00618: loss did not improve from 1.67842\n",
      "Epoch 619/2000\n",
      "93/93 [==============================] - 0s 673us/step - loss: 1.6786 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00619: loss did not improve from 1.67842\n",
      "Epoch 620/2000\n",
      "93/93 [==============================] - 0s 675us/step - loss: 1.6786 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00620: loss did not improve from 1.67842\n",
      "Epoch 621/2000\n",
      "93/93 [==============================] - 0s 699us/step - loss: 1.6787 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00621: loss did not improve from 1.67842\n",
      "Epoch 622/2000\n",
      "93/93 [==============================] - 0s 757us/step - loss: 1.6787 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00622: loss did not improve from 1.67842\n",
      "Epoch 623/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6785 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00623: loss did not improve from 1.67842\n",
      "Epoch 624/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6786 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00624: loss did not improve from 1.67842\n",
      "Epoch 625/2000\n",
      "93/93 [==============================] - 0s 634us/step - loss: 1.6786 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00625: loss did not improve from 1.67842\n",
      "Epoch 626/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6785 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00626: loss did not improve from 1.67842\n",
      "Epoch 627/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6786 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00627: loss did not improve from 1.67842\n",
      "Epoch 628/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6786 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00628: loss did not improve from 1.67842\n",
      "Epoch 629/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6787 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00629: loss did not improve from 1.67842\n",
      "Epoch 630/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6784 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00630: loss improved from 1.67842 to 1.67838, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 631/2000\n",
      "93/93 [==============================] - 0s 714us/step - loss: 1.6786 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00631: loss did not improve from 1.67838\n",
      "Epoch 632/2000\n",
      "93/93 [==============================] - 0s 716us/step - loss: 1.6787 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00632: loss did not improve from 1.67838\n",
      "Epoch 633/2000\n",
      "93/93 [==============================] - 0s 759us/step - loss: 1.6786 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00633: loss did not improve from 1.67838\n",
      "Epoch 634/2000\n",
      "93/93 [==============================] - 0s 815us/step - loss: 1.6786 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00634: loss did not improve from 1.67838\n",
      "Epoch 635/2000\n",
      "93/93 [==============================] - 0s 810us/step - loss: 1.6786 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00635: loss did not improve from 1.67838\n",
      "Epoch 636/2000\n",
      "93/93 [==============================] - 0s 814us/step - loss: 1.6783 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00636: loss improved from 1.67838 to 1.67832, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 637/2000\n",
      "93/93 [==============================] - 0s 837us/step - loss: 1.6787 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00637: loss did not improve from 1.67832\n",
      "Epoch 638/2000\n",
      "93/93 [==============================] - 0s 867us/step - loss: 1.6785 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00638: loss did not improve from 1.67832\n",
      "Epoch 639/2000\n",
      "93/93 [==============================] - ETA: 0s - loss: 1.7899 - accuracy: 0.4237   - 0s 854us/step - loss: 1.6785 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00639: loss did not improve from 1.67832\n",
      "Epoch 640/2000\n",
      "93/93 [==============================] - 0s 735us/step - loss: 1.6788 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00640: loss did not improve from 1.67832\n",
      "Epoch 641/2000\n",
      "93/93 [==============================] - 0s 673us/step - loss: 1.6787 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00641: loss did not improve from 1.67832\n",
      "Epoch 642/2000\n",
      "93/93 [==============================] - 0s 718us/step - loss: 1.6787 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00642: loss did not improve from 1.67832\n",
      "Epoch 643/2000\n",
      "93/93 [==============================] - 0s 711us/step - loss: 1.6787 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00643: loss did not improve from 1.67832\n",
      "Epoch 644/2000\n",
      "93/93 [==============================] - 0s 717us/step - loss: 1.6785 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00644: loss did not improve from 1.67832\n",
      "Epoch 645/2000\n",
      "93/93 [==============================] - 0s 908us/step - loss: 1.6785 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00645: loss did not improve from 1.67832\n",
      "Epoch 646/2000\n",
      "93/93 [==============================] - 0s 869us/step - loss: 1.6784 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00646: loss did not improve from 1.67832\n",
      "Epoch 647/2000\n",
      "93/93 [==============================] - 0s 772us/step - loss: 1.6786 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00647: loss did not improve from 1.67832\n",
      "Epoch 648/2000\n",
      "93/93 [==============================] - 0s 718us/step - loss: 1.6783 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00648: loss did not improve from 1.67832\n",
      "Epoch 649/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 0s 665us/step - loss: 1.6783 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00649: loss did not improve from 1.67832\n",
      "Epoch 650/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6786 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00650: loss did not improve from 1.67832\n",
      "Epoch 651/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6783 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00651: loss improved from 1.67832 to 1.67827, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 652/2000\n",
      "93/93 [==============================] - 0s 729us/step - loss: 1.6786 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00652: loss did not improve from 1.67827\n",
      "Epoch 653/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6785 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00653: loss did not improve from 1.67827\n",
      "Epoch 654/2000\n",
      "93/93 [==============================] - 0s 719us/step - loss: 1.6784 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00654: loss did not improve from 1.67827\n",
      "Epoch 655/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6785 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00655: loss did not improve from 1.67827\n",
      "Epoch 656/2000\n",
      "93/93 [==============================] - 0s 657us/step - loss: 1.6784 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00656: loss did not improve from 1.67827\n",
      "Epoch 657/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6785 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00657: loss did not improve from 1.67827\n",
      "Epoch 658/2000\n",
      "93/93 [==============================] - 0s 729us/step - loss: 1.6785 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00658: loss did not improve from 1.67827\n",
      "Epoch 659/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6784 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00659: loss did not improve from 1.67827\n",
      "Epoch 660/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6784 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00660: loss did not improve from 1.67827\n",
      "Epoch 661/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6785 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00661: loss did not improve from 1.67827\n",
      "Epoch 662/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6785 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00662: loss did not improve from 1.67827\n",
      "Epoch 663/2000\n",
      "93/93 [==============================] - 0s 740us/step - loss: 1.6788 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00663: loss did not improve from 1.67827\n",
      "Epoch 664/2000\n",
      "93/93 [==============================] - 0s 825us/step - loss: 1.6786 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00664: loss did not improve from 1.67827\n",
      "Epoch 665/2000\n",
      "93/93 [==============================] - 0s 804us/step - loss: 1.6789 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00665: loss did not improve from 1.67827\n",
      "Epoch 666/2000\n",
      "93/93 [==============================] - 0s 815us/step - loss: 1.6785 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00666: loss did not improve from 1.67827\n",
      "Epoch 667/2000\n",
      "93/93 [==============================] - 0s 815us/step - loss: 1.6785 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00667: loss did not improve from 1.67827\n",
      "Epoch 668/2000\n",
      "93/93 [==============================] - 0s 772us/step - loss: 1.6785 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00668: loss did not improve from 1.67827\n",
      "Epoch 669/2000\n",
      "93/93 [==============================] - 0s 794us/step - loss: 1.6786 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00669: loss did not improve from 1.67827\n",
      "Epoch 670/2000\n",
      "93/93 [==============================] - 0s 719us/step - loss: 1.6785 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00670: loss did not improve from 1.67827\n",
      "Epoch 671/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6784 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00671: loss did not improve from 1.67827\n",
      "Epoch 672/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6785 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00672: loss did not improve from 1.67827\n",
      "Epoch 673/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6790 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00673: loss did not improve from 1.67827\n",
      "Epoch 674/2000\n",
      "93/93 [==============================] - 0s 675us/step - loss: 1.6786 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00674: loss did not improve from 1.67827\n",
      "Epoch 675/2000\n",
      "93/93 [==============================] - 0s 661us/step - loss: 1.6785 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00675: loss did not improve from 1.67827\n",
      "Epoch 676/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6784 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00676: loss did not improve from 1.67827\n",
      "Epoch 677/2000\n",
      "93/93 [==============================] - 0s 655us/step - loss: 1.6785 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00677: loss did not improve from 1.67827\n",
      "Epoch 678/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6783 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00678: loss did not improve from 1.67827\n",
      "Epoch 679/2000\n",
      "93/93 [==============================] - 0s 685us/step - loss: 1.6783 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00679: loss did not improve from 1.67827\n",
      "Epoch 680/2000\n",
      "93/93 [==============================] - 0s 663us/step - loss: 1.6784 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00680: loss did not improve from 1.67827\n",
      "Epoch 681/2000\n",
      "93/93 [==============================] - 0s 656us/step - loss: 1.6785 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00681: loss did not improve from 1.67827\n",
      "Epoch 682/2000\n",
      "93/93 [==============================] - 0s 653us/step - loss: 1.6786 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00682: loss did not improve from 1.67827\n",
      "Epoch 683/2000\n",
      "93/93 [==============================] - 0s 675us/step - loss: 1.6782 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00683: loss improved from 1.67827 to 1.67823, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 684/2000\n",
      "93/93 [==============================] - 0s 702us/step - loss: 1.6781 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00684: loss improved from 1.67823 to 1.67813, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 685/2000\n",
      "93/93 [==============================] - 0s 742us/step - loss: 1.6783 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00685: loss did not improve from 1.67813\n",
      "Epoch 686/2000\n",
      "93/93 [==============================] - 0s 738us/step - loss: 1.6784 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00686: loss did not improve from 1.67813\n",
      "Epoch 687/2000\n",
      "93/93 [==============================] - 0s 719us/step - loss: 1.6785 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00687: loss did not improve from 1.67813\n",
      "Epoch 688/2000\n",
      "93/93 [==============================] - 0s 731us/step - loss: 1.6782 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00688: loss did not improve from 1.67813\n",
      "Epoch 689/2000\n",
      "93/93 [==============================] - 0s 718us/step - loss: 1.6785 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00689: loss did not improve from 1.67813\n",
      "Epoch 690/2000\n",
      "93/93 [==============================] - 0s 680us/step - loss: 1.6782 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00690: loss did not improve from 1.67813\n",
      "Epoch 691/2000\n",
      "93/93 [==============================] - 0s 683us/step - loss: 1.6780 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00691: loss improved from 1.67813 to 1.67804, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 692/2000\n",
      "93/93 [==============================] - 0s 732us/step - loss: 1.6784 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00692: loss did not improve from 1.67804\n",
      "Epoch 693/2000\n",
      "93/93 [==============================] - 0s 755us/step - loss: 1.6784 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00693: loss did not improve from 1.67804\n",
      "Epoch 694/2000\n",
      "93/93 [==============================] - 0s 740us/step - loss: 1.6783 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00694: loss did not improve from 1.67804\n",
      "Epoch 695/2000\n",
      "93/93 [==============================] - 0s 727us/step - loss: 1.6782 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00695: loss did not improve from 1.67804\n",
      "Epoch 696/2000\n",
      "93/93 [==============================] - 0s 793us/step - loss: 1.6785 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00696: loss did not improve from 1.67804\n",
      "Epoch 697/2000\n",
      "93/93 [==============================] - 0s 843us/step - loss: 1.6783 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00697: loss did not improve from 1.67804\n",
      "Epoch 698/2000\n",
      "93/93 [==============================] - 0s 760us/step - loss: 1.6782 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00698: loss did not improve from 1.67804\n",
      "Epoch 699/2000\n",
      "93/93 [==============================] - 0s 726us/step - loss: 1.6784 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00699: loss did not improve from 1.67804\n",
      "Epoch 700/2000\n",
      "93/93 [==============================] - 0s 761us/step - loss: 1.6782 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00700: loss did not improve from 1.67804\n",
      "Epoch 701/2000\n",
      "93/93 [==============================] - 0s 795us/step - loss: 1.6781 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00701: loss did not improve from 1.67804\n",
      "Epoch 702/2000\n",
      "93/93 [==============================] - 0s 852us/step - loss: 1.6781 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00702: loss did not improve from 1.67804\n",
      "Epoch 703/2000\n",
      "93/93 [==============================] - 0s 768us/step - loss: 1.6783 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00703: loss did not improve from 1.67804\n",
      "Epoch 704/2000\n",
      "93/93 [==============================] - 0s 751us/step - loss: 1.6783 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00704: loss did not improve from 1.67804\n",
      "Epoch 705/2000\n",
      "93/93 [==============================] - 0s 847us/step - loss: 1.6785 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00705: loss did not improve from 1.67804\n",
      "Epoch 706/2000\n",
      "93/93 [==============================] - 0s 708us/step - loss: 1.6782 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00706: loss did not improve from 1.67804\n",
      "Epoch 707/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6782 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00707: loss did not improve from 1.67804\n",
      "Epoch 708/2000\n",
      "93/93 [==============================] - 0s 626us/step - loss: 1.6782 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00708: loss did not improve from 1.67804\n",
      "Epoch 709/2000\n",
      "93/93 [==============================] - 0s 599us/step - loss: 1.6783 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00709: loss did not improve from 1.67804\n",
      "Epoch 710/2000\n",
      "93/93 [==============================] - 0s 646us/step - loss: 1.6841 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00710: loss did not improve from 1.67804\n",
      "Epoch 711/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6793 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00711: loss did not improve from 1.67804\n",
      "Epoch 712/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6787 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00712: loss did not improve from 1.67804\n",
      "Epoch 713/2000\n",
      "93/93 [==============================] - 0s 600us/step - loss: 1.6784 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00713: loss did not improve from 1.67804\n",
      "Epoch 714/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6781 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00714: loss did not improve from 1.67804\n",
      "Epoch 715/2000\n",
      "93/93 [==============================] - 0s 602us/step - loss: 1.6784 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00715: loss did not improve from 1.67804\n",
      "Epoch 716/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6783 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00716: loss did not improve from 1.67804\n",
      "Epoch 717/2000\n",
      "93/93 [==============================] - 0s 600us/step - loss: 1.6781 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00717: loss did not improve from 1.67804\n",
      "Epoch 718/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6783 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00718: loss did not improve from 1.67804\n",
      "Epoch 719/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6781 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00719: loss did not improve from 1.67804\n",
      "Epoch 720/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6782 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00720: loss did not improve from 1.67804\n",
      "Epoch 721/2000\n",
      "93/93 [==============================] - 0s 602us/step - loss: 1.6781 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00721: loss did not improve from 1.67804\n",
      "Epoch 722/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6801 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00722: loss did not improve from 1.67804\n",
      "Epoch 723/2000\n",
      "93/93 [==============================] - 0s 600us/step - loss: 1.6784 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00723: loss did not improve from 1.67804\n",
      "Epoch 724/2000\n",
      "93/93 [==============================] - 0s 624us/step - loss: 1.6781 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00724: loss did not improve from 1.67804\n",
      "Epoch 725/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6782 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00725: loss did not improve from 1.67804\n",
      "Epoch 726/2000\n",
      "93/93 [==============================] - 0s 624us/step - loss: 1.6780 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00726: loss improved from 1.67804 to 1.67799, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 727/2000\n",
      "93/93 [==============================] - 0s 663us/step - loss: 1.6782 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00727: loss did not improve from 1.67799\n",
      "Epoch 728/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6781 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00728: loss did not improve from 1.67799\n",
      "Epoch 729/2000\n",
      "93/93 [==============================] - 0s 675us/step - loss: 1.6782 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00729: loss did not improve from 1.67799\n",
      "Epoch 730/2000\n",
      "93/93 [==============================] - 0s 708us/step - loss: 1.6780 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00730: loss improved from 1.67799 to 1.67795, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 731/2000\n",
      "93/93 [==============================] - 0s 699us/step - loss: 1.6782 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00731: loss did not improve from 1.67795\n",
      "Epoch 732/2000\n",
      "93/93 [==============================] - 0s 719us/step - loss: 1.6781 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00732: loss did not improve from 1.67795\n",
      "Epoch 733/2000\n",
      "93/93 [==============================] - 0s 707us/step - loss: 1.6780 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00733: loss did not improve from 1.67795\n",
      "Epoch 734/2000\n",
      "93/93 [==============================] - 0s 679us/step - loss: 1.6788 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00734: loss did not improve from 1.67795\n",
      "Epoch 735/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6782 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00735: loss did not improve from 1.67795\n",
      "Epoch 736/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6781 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00736: loss did not improve from 1.67795\n",
      "Epoch 737/2000\n",
      "93/93 [==============================] - 0s 655us/step - loss: 1.6781 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00737: loss did not improve from 1.67795\n",
      "Epoch 738/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6782 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00738: loss did not improve from 1.67795\n",
      "Epoch 739/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6780 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00739: loss did not improve from 1.67795\n",
      "Epoch 740/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6782 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00740: loss did not improve from 1.67795\n",
      "Epoch 741/2000\n",
      "93/93 [==============================] - 0s 729us/step - loss: 1.6782 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00741: loss did not improve from 1.67795\n",
      "Epoch 742/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6784 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00742: loss did not improve from 1.67795\n",
      "Epoch 743/2000\n",
      "93/93 [==============================] - 0s 675us/step - loss: 1.6782 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00743: loss did not improve from 1.67795\n",
      "Epoch 744/2000\n",
      "93/93 [==============================] - 0s 761us/step - loss: 1.6779 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00744: loss improved from 1.67795 to 1.67794, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 745/2000\n",
      "93/93 [==============================] - 0s 901us/step - loss: 1.6781 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00745: loss did not improve from 1.67794\n",
      "Epoch 746/2000\n",
      "93/93 [==============================] - 0s 879us/step - loss: 1.6781 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00746: loss did not improve from 1.67794\n",
      "Epoch 747/2000\n",
      "93/93 [==============================] - 0s 880us/step - loss: 1.6781 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00747: loss did not improve from 1.67794\n",
      "Epoch 748/2000\n",
      "93/93 [==============================] - 0s 858us/step - loss: 1.6799 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00748: loss did not improve from 1.67794\n",
      "Epoch 749/2000\n",
      "93/93 [==============================] - 0s 869us/step - loss: 1.6782 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00749: loss did not improve from 1.67794\n",
      "Epoch 750/2000\n",
      "93/93 [==============================] - 0s 707us/step - loss: 1.6781 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00750: loss did not improve from 1.67794\n",
      "Epoch 751/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6780 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00751: loss did not improve from 1.67794\n",
      "Epoch 752/2000\n",
      "93/93 [==============================] - 0s 675us/step - loss: 1.6791 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00752: loss did not improve from 1.67794\n",
      "Epoch 753/2000\n",
      "93/93 [==============================] - 0s 697us/step - loss: 1.6794 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00753: loss did not improve from 1.67794\n",
      "Epoch 754/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 0s 697us/step - loss: 1.6785 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00754: loss did not improve from 1.67794\n",
      "Epoch 755/2000\n",
      "93/93 [==============================] - 0s 708us/step - loss: 1.6783 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00755: loss did not improve from 1.67794\n",
      "Epoch 756/2000\n",
      "93/93 [==============================] - 0s 697us/step - loss: 1.6784 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00756: loss did not improve from 1.67794\n",
      "Epoch 757/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6782 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00757: loss did not improve from 1.67794\n",
      "Epoch 758/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6782 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00758: loss did not improve from 1.67794\n",
      "Epoch 759/2000\n",
      "93/93 [==============================] - 0s 697us/step - loss: 1.6781 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00759: loss did not improve from 1.67794\n",
      "Epoch 760/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6782 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00760: loss did not improve from 1.67794\n",
      "Epoch 761/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6769 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00761: loss improved from 1.67794 to 1.67686, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 762/2000\n",
      "93/93 [==============================] - 0s 752us/step - loss: 1.7316 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00762: loss did not improve from 1.67686\n",
      "Epoch 763/2000\n",
      "93/93 [==============================] - 0s 761us/step - loss: 2.1668 - accuracy: 0.4409\n",
      "\n",
      "Epoch 00763: loss did not improve from 1.67686\n",
      "Epoch 764/2000\n",
      "93/93 [==============================] - 0s 709us/step - loss: 1.8038 - accuracy: 0.4409\n",
      "\n",
      "Epoch 00764: loss did not improve from 1.67686\n",
      "Epoch 765/2000\n",
      "93/93 [==============================] - 0s 709us/step - loss: 1.6852 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00765: loss did not improve from 1.67686\n",
      "Epoch 766/2000\n",
      "93/93 [==============================] - 0s 694us/step - loss: 1.6817 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00766: loss did not improve from 1.67686\n",
      "Epoch 767/2000\n",
      "93/93 [==============================] - 0s 707us/step - loss: 1.6814 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00767: loss did not improve from 1.67686\n",
      "Epoch 768/2000\n",
      "93/93 [==============================] - 0s 700us/step - loss: 1.6807 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00768: loss did not improve from 1.67686\n",
      "Epoch 769/2000\n",
      "93/93 [==============================] - 0s 702us/step - loss: 1.6799 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00769: loss did not improve from 1.67686\n",
      "Epoch 770/2000\n",
      "93/93 [==============================] - 0s 696us/step - loss: 1.6802 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00770: loss did not improve from 1.67686\n",
      "Epoch 771/2000\n",
      "93/93 [==============================] - 0s 702us/step - loss: 1.6793 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00771: loss did not improve from 1.67686\n",
      "Epoch 772/2000\n",
      "93/93 [==============================] - 0s 713us/step - loss: 1.6804 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00772: loss did not improve from 1.67686\n",
      "Epoch 773/2000\n",
      "93/93 [==============================] - 0s 695us/step - loss: 1.6800 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00773: loss did not improve from 1.67686\n",
      "Epoch 774/2000\n",
      "93/93 [==============================] - 0s 681us/step - loss: 1.6798 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00774: loss did not improve from 1.67686\n",
      "Epoch 775/2000\n",
      "93/93 [==============================] - 0s 753us/step - loss: 1.6787 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00775: loss did not improve from 1.67686\n",
      "Epoch 776/2000\n",
      "93/93 [==============================] - 0s 732us/step - loss: 1.6800 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00776: loss did not improve from 1.67686\n",
      "Epoch 777/2000\n",
      "93/93 [==============================] - 0s 814us/step - loss: 1.6800 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00777: loss did not improve from 1.67686\n",
      "Epoch 778/2000\n",
      "93/93 [==============================] - 0s 844us/step - loss: 1.6799 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00778: loss did not improve from 1.67686\n",
      "Epoch 779/2000\n",
      "93/93 [==============================] - 0s 849us/step - loss: 1.6791 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00779: loss did not improve from 1.67686\n",
      "Epoch 780/2000\n",
      "93/93 [==============================] - 0s 858us/step - loss: 1.6794 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00780: loss did not improve from 1.67686\n",
      "Epoch 781/2000\n",
      "93/93 [==============================] - 0s 790us/step - loss: 1.6786 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00781: loss did not improve from 1.67686\n",
      "Epoch 782/2000\n",
      "93/93 [==============================] - 0s 751us/step - loss: 1.6794 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00782: loss did not improve from 1.67686\n",
      "Epoch 783/2000\n",
      "93/93 [==============================] - 0s 783us/step - loss: 1.6793 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00783: loss did not improve from 1.67686\n",
      "Epoch 784/2000\n",
      "93/93 [==============================] - 0s 815us/step - loss: 1.6789 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00784: loss did not improve from 1.67686\n",
      "Epoch 785/2000\n",
      "93/93 [==============================] - 0s 815us/step - loss: 1.6786 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00785: loss did not improve from 1.67686\n",
      "Epoch 786/2000\n",
      "93/93 [==============================] - 0s 772us/step - loss: 1.6795 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00786: loss did not improve from 1.67686\n",
      "Epoch 787/2000\n",
      "93/93 [==============================] - 0s 708us/step - loss: 1.6795 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00787: loss did not improve from 1.67686\n",
      "Epoch 788/2000\n",
      "93/93 [==============================] - 0s 718us/step - loss: 1.6796 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00788: loss did not improve from 1.67686\n",
      "Epoch 789/2000\n",
      "93/93 [==============================] - 0s 697us/step - loss: 1.6787 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00789: loss did not improve from 1.67686\n",
      "Epoch 790/2000\n",
      "93/93 [==============================] - 0s 675us/step - loss: 1.6787 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00790: loss did not improve from 1.67686\n",
      "Epoch 791/2000\n",
      "93/93 [==============================] - 0s 751us/step - loss: 1.6788 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00791: loss did not improve from 1.67686\n",
      "Epoch 792/2000\n",
      "93/93 [==============================] - 0s 719us/step - loss: 1.6785 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00792: loss did not improve from 1.67686\n",
      "Epoch 793/2000\n",
      "93/93 [==============================] - 0s 751us/step - loss: 1.6791 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00793: loss did not improve from 1.67686\n",
      "Epoch 794/2000\n",
      "93/93 [==============================] - 0s 750us/step - loss: 1.6787 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00794: loss did not improve from 1.67686\n",
      "Epoch 795/2000\n",
      "93/93 [==============================] - 0s 762us/step - loss: 1.6792 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00795: loss did not improve from 1.67686\n",
      "Epoch 796/2000\n",
      "93/93 [==============================] - 0s 783us/step - loss: 1.6791 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00796: loss did not improve from 1.67686\n",
      "Epoch 797/2000\n",
      "93/93 [==============================] - 0s 750us/step - loss: 1.6791 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00797: loss did not improve from 1.67686\n",
      "Epoch 798/2000\n",
      "93/93 [==============================] - 0s 761us/step - loss: 1.6792 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00798: loss did not improve from 1.67686\n",
      "Epoch 799/2000\n",
      "93/93 [==============================] - 0s 718us/step - loss: 1.6787 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00799: loss did not improve from 1.67686\n",
      "Epoch 800/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6788 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00800: loss did not improve from 1.67686\n",
      "Epoch 801/2000\n",
      "93/93 [==============================] - 0s 697us/step - loss: 1.6784 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00801: loss did not improve from 1.67686\n",
      "Epoch 802/2000\n",
      "93/93 [==============================] - 0s 740us/step - loss: 1.6789 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00802: loss did not improve from 1.67686\n",
      "Epoch 803/2000\n",
      "93/93 [==============================] - 0s 761us/step - loss: 1.6785 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00803: loss did not improve from 1.67686\n",
      "Epoch 804/2000\n",
      "93/93 [==============================] - 0s 751us/step - loss: 1.6786 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00804: loss did not improve from 1.67686\n",
      "Epoch 805/2000\n",
      "93/93 [==============================] - 0s 794us/step - loss: 1.6781 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00805: loss did not improve from 1.67686\n",
      "Epoch 806/2000\n",
      "93/93 [==============================] - 0s 718us/step - loss: 1.6786 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00806: loss did not improve from 1.67686\n",
      "Epoch 807/2000\n",
      "93/93 [==============================] - 0s 765us/step - loss: 1.6786 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00807: loss did not improve from 1.67686\n",
      "Epoch 808/2000\n",
      "93/93 [==============================] - 0s 750us/step - loss: 1.6787 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00808: loss did not improve from 1.67686\n",
      "Epoch 809/2000\n",
      "93/93 [==============================] - 0s 751us/step - loss: 1.6787 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00809: loss did not improve from 1.67686\n",
      "Epoch 810/2000\n",
      "93/93 [==============================] - 0s 707us/step - loss: 1.6784 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00810: loss did not improve from 1.67686\n",
      "Epoch 811/2000\n",
      "93/93 [==============================] - 0s 675us/step - loss: 1.6788 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00811: loss did not improve from 1.67686\n",
      "Epoch 812/2000\n",
      "93/93 [==============================] - 0s 708us/step - loss: 1.6786 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00812: loss did not improve from 1.67686\n",
      "Epoch 813/2000\n",
      "93/93 [==============================] - 0s 751us/step - loss: 1.6781 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00813: loss did not improve from 1.67686\n",
      "Epoch 814/2000\n",
      "93/93 [==============================] - 0s 750us/step - loss: 1.6785 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00814: loss did not improve from 1.67686\n",
      "Epoch 815/2000\n",
      "93/93 [==============================] - 0s 674us/step - loss: 1.6785 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00815: loss did not improve from 1.67686\n",
      "Epoch 816/2000\n",
      "93/93 [==============================] - 0s 712us/step - loss: 1.6786 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00816: loss did not improve from 1.67686\n",
      "Epoch 817/2000\n",
      "93/93 [==============================] - 0s 708us/step - loss: 1.6785 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00817: loss did not improve from 1.67686\n",
      "Epoch 818/2000\n",
      "93/93 [==============================] - 0s 729us/step - loss: 1.6786 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00818: loss did not improve from 1.67686\n",
      "Epoch 819/2000\n",
      "93/93 [==============================] - 0s 708us/step - loss: 1.6780 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00819: loss did not improve from 1.67686\n",
      "Epoch 820/2000\n",
      "93/93 [==============================] - 0s 708us/step - loss: 1.6784 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00820: loss did not improve from 1.67686\n",
      "Epoch 821/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6783 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00821: loss did not improve from 1.67686\n",
      "Epoch 822/2000\n",
      "93/93 [==============================] - 0s 708us/step - loss: 1.6785 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00822: loss did not improve from 1.67686\n",
      "Epoch 823/2000\n",
      "93/93 [==============================] - 0s 680us/step - loss: 1.6783 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00823: loss did not improve from 1.67686\n",
      "Epoch 824/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6783 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00824: loss did not improve from 1.67686\n",
      "Epoch 825/2000\n",
      "93/93 [==============================] - 0s 847us/step - loss: 1.6786 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00825: loss did not improve from 1.67686\n",
      "Epoch 826/2000\n",
      "93/93 [==============================] - 0s 879us/step - loss: 1.6783 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00826: loss did not improve from 1.67686\n",
      "Epoch 827/2000\n",
      "93/93 [==============================] - 0s 783us/step - loss: 1.6786 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00827: loss did not improve from 1.67686\n",
      "Epoch 828/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6787 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00828: loss did not improve from 1.67686\n",
      "Epoch 829/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6784 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00829: loss did not improve from 1.67686\n",
      "Epoch 830/2000\n",
      "93/93 [==============================] - 0s 663us/step - loss: 1.6782 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00830: loss did not improve from 1.67686\n",
      "Epoch 831/2000\n",
      "93/93 [==============================] - 0s 836us/step - loss: 1.6784 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00831: loss did not improve from 1.67686\n",
      "Epoch 832/2000\n",
      "93/93 [==============================] - 0s 815us/step - loss: 1.6779 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00832: loss did not improve from 1.67686\n",
      "Epoch 833/2000\n",
      "93/93 [==============================] - 0s 826us/step - loss: 1.6783 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00833: loss did not improve from 1.67686\n",
      "Epoch 834/2000\n",
      "93/93 [==============================] - 0s 708us/step - loss: 1.6779 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00834: loss did not improve from 1.67686\n",
      "Epoch 835/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6782 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00835: loss did not improve from 1.67686\n",
      "Epoch 836/2000\n",
      "93/93 [==============================] - 0s 635us/step - loss: 1.6784 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00836: loss did not improve from 1.67686\n",
      "Epoch 837/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6784 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00837: loss did not improve from 1.67686\n",
      "Epoch 838/2000\n",
      "93/93 [==============================] - 0s 738us/step - loss: 1.6782 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00838: loss did not improve from 1.67686\n",
      "Epoch 839/2000\n",
      "93/93 [==============================] - 0s 847us/step - loss: 1.6783 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00839: loss did not improve from 1.67686\n",
      "Epoch 840/2000\n",
      "93/93 [==============================] - 0s 804us/step - loss: 1.6781 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00840: loss did not improve from 1.67686\n",
      "Epoch 841/2000\n",
      "93/93 [==============================] - 0s 718us/step - loss: 1.6785 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00841: loss did not improve from 1.67686\n",
      "Epoch 842/2000\n",
      "93/93 [==============================] - 0s 657us/step - loss: 1.6783 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00842: loss did not improve from 1.67686\n",
      "Epoch 843/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6783 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00843: loss did not improve from 1.67686\n",
      "Epoch 844/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6779 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00844: loss did not improve from 1.67686\n",
      "Epoch 845/2000\n",
      "93/93 [==============================] - 0s 675us/step - loss: 1.6784 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00845: loss did not improve from 1.67686\n",
      "Epoch 846/2000\n",
      "93/93 [==============================] - 0s 772us/step - loss: 1.6782 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00846: loss did not improve from 1.67686\n",
      "Epoch 847/2000\n",
      "93/93 [==============================] - 0s 815us/step - loss: 1.6780 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00847: loss did not improve from 1.67686\n",
      "Epoch 848/2000\n",
      "93/93 [==============================] - 0s 729us/step - loss: 1.6784 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00848: loss did not improve from 1.67686\n",
      "Epoch 849/2000\n",
      "93/93 [==============================] - 0s 675us/step - loss: 1.6783 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00849: loss did not improve from 1.67686\n",
      "Epoch 850/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6787 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00850: loss did not improve from 1.67686\n",
      "Epoch 851/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6781 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00851: loss did not improve from 1.67686\n",
      "Epoch 852/2000\n",
      "93/93 [==============================] - 0s 675us/step - loss: 1.6783 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00852: loss did not improve from 1.67686\n",
      "Epoch 853/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6837 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00853: loss did not improve from 1.67686\n",
      "Epoch 854/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6779 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00854: loss did not improve from 1.67686\n",
      "Epoch 855/2000\n",
      "93/93 [==============================] - 0s 658us/step - loss: 1.6782 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00855: loss did not improve from 1.67686\n",
      "Epoch 856/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6784 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00856: loss did not improve from 1.67686\n",
      "Epoch 857/2000\n",
      "93/93 [==============================] - 0s 644us/step - loss: 1.6782 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00857: loss did not improve from 1.67686\n",
      "Epoch 858/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6783 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00858: loss did not improve from 1.67686\n",
      "Epoch 859/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6783 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00859: loss did not improve from 1.67686\n",
      "Epoch 860/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6784 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00860: loss did not improve from 1.67686\n",
      "Epoch 861/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 0s 643us/step - loss: 1.6790 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00861: loss did not improve from 1.67686\n",
      "Epoch 862/2000\n",
      "93/93 [==============================] - 0s 644us/step - loss: 1.6791 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00862: loss did not improve from 1.67686\n",
      "Epoch 863/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6780 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00863: loss did not improve from 1.67686\n",
      "Epoch 864/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6782 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00864: loss did not improve from 1.67686\n",
      "Epoch 865/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6782 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00865: loss did not improve from 1.67686\n",
      "Epoch 866/2000\n",
      "93/93 [==============================] - 0s 641us/step - loss: 1.6783 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00866: loss did not improve from 1.67686\n",
      "Epoch 867/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6782 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00867: loss did not improve from 1.67686\n",
      "Epoch 868/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6781 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00868: loss did not improve from 1.67686\n",
      "Epoch 869/2000\n",
      "93/93 [==============================] - 0s 648us/step - loss: 1.6781 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00869: loss did not improve from 1.67686\n",
      "Epoch 870/2000\n",
      "93/93 [==============================] - 0s 639us/step - loss: 1.6781 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00870: loss did not improve from 1.67686\n",
      "Epoch 871/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6782 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00871: loss did not improve from 1.67686\n",
      "Epoch 872/2000\n",
      "93/93 [==============================] - 0s 631us/step - loss: 1.6786 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00872: loss did not improve from 1.67686\n",
      "Epoch 873/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6781 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00873: loss did not improve from 1.67686\n",
      "Epoch 874/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6779 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00874: loss did not improve from 1.67686\n",
      "Epoch 875/2000\n",
      "93/93 [==============================] - 0s 634us/step - loss: 1.6782 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00875: loss did not improve from 1.67686\n",
      "Epoch 876/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6783 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00876: loss did not improve from 1.67686\n",
      "Epoch 877/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6780 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00877: loss did not improve from 1.67686\n",
      "Epoch 878/2000\n",
      "93/93 [==============================] - 0s 664us/step - loss: 1.6783 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00878: loss did not improve from 1.67686\n",
      "Epoch 879/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6782 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00879: loss did not improve from 1.67686\n",
      "Epoch 880/2000\n",
      "93/93 [==============================] - 0s 644us/step - loss: 1.6782 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00880: loss did not improve from 1.67686\n",
      "Epoch 881/2000\n",
      "93/93 [==============================] - 0s 657us/step - loss: 1.6782 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00881: loss did not improve from 1.67686\n",
      "Epoch 882/2000\n",
      "93/93 [==============================] - 0s 675us/step - loss: 1.6780 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00882: loss did not improve from 1.67686\n",
      "Epoch 883/2000\n",
      "93/93 [==============================] - 0s 652us/step - loss: 1.6781 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00883: loss did not improve from 1.67686\n",
      "Epoch 884/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6780 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00884: loss did not improve from 1.67686\n",
      "Epoch 885/2000\n",
      "93/93 [==============================] - 0s 826us/step - loss: 1.6780 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00885: loss did not improve from 1.67686\n",
      "Epoch 886/2000\n",
      "93/93 [==============================] - 0s 804us/step - loss: 1.6782 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00886: loss did not improve from 1.67686\n",
      "Epoch 887/2000\n",
      "93/93 [==============================] - 0s 804us/step - loss: 1.6783 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00887: loss did not improve from 1.67686\n",
      "Epoch 888/2000\n",
      "93/93 [==============================] - 0s 697us/step - loss: 1.6786 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00888: loss did not improve from 1.67686\n",
      "Epoch 889/2000\n",
      "93/93 [==============================] - 0s 632us/step - loss: 1.6782 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00889: loss did not improve from 1.67686\n",
      "Epoch 890/2000\n",
      "93/93 [==============================] - 0s 645us/step - loss: 1.6783 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00890: loss did not improve from 1.67686\n",
      "Epoch 891/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6781 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00891: loss did not improve from 1.67686\n",
      "Epoch 892/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6780 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00892: loss did not improve from 1.67686\n",
      "Epoch 893/2000\n",
      "93/93 [==============================] - 0s 751us/step - loss: 1.6780 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00893: loss did not improve from 1.67686\n",
      "Epoch 894/2000\n",
      "93/93 [==============================] - 0s 825us/step - loss: 1.6780 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00894: loss did not improve from 1.67686\n",
      "Epoch 895/2000\n",
      "93/93 [==============================] - 0s 793us/step - loss: 1.6780 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00895: loss did not improve from 1.67686\n",
      "Epoch 896/2000\n",
      "93/93 [==============================] - 0s 804us/step - loss: 1.6780 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00896: loss did not improve from 1.67686\n",
      "Epoch 897/2000\n",
      "93/93 [==============================] - 0s 718us/step - loss: 1.6781 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00897: loss did not improve from 1.67686\n",
      "Epoch 898/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6780 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00898: loss did not improve from 1.67686\n",
      "Epoch 899/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6780 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00899: loss did not improve from 1.67686\n",
      "Epoch 900/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6780 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00900: loss did not improve from 1.67686\n",
      "Epoch 901/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6779 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00901: loss did not improve from 1.67686\n",
      "Epoch 902/2000\n",
      "93/93 [==============================] - 0s 740us/step - loss: 1.6780 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00902: loss did not improve from 1.67686\n",
      "Epoch 903/2000\n",
      "93/93 [==============================] - 0s 815us/step - loss: 1.6780 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00903: loss did not improve from 1.67686\n",
      "Epoch 904/2000\n",
      "93/93 [==============================] - 0s 815us/step - loss: 1.6780 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00904: loss did not improve from 1.67686\n",
      "Epoch 905/2000\n",
      "93/93 [==============================] - 0s 697us/step - loss: 1.6780 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00905: loss did not improve from 1.67686\n",
      "Epoch 906/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6778 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00906: loss did not improve from 1.67686\n",
      "Epoch 907/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6781 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00907: loss did not improve from 1.67686\n",
      "Epoch 908/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6780 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00908: loss did not improve from 1.67686\n",
      "Epoch 909/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6780 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00909: loss did not improve from 1.67686\n",
      "Epoch 910/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6780 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00910: loss did not improve from 1.67686\n",
      "Epoch 911/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6779 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00911: loss did not improve from 1.67686\n",
      "Epoch 912/2000\n",
      "93/93 [==============================] - 0s 772us/step - loss: 1.6780 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00912: loss did not improve from 1.67686\n",
      "Epoch 913/2000\n",
      "93/93 [==============================] - 0s 815us/step - loss: 1.6781 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00913: loss did not improve from 1.67686\n",
      "Epoch 914/2000\n",
      "93/93 [==============================] - 0s 815us/step - loss: 1.6780 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00914: loss did not improve from 1.67686\n",
      "Epoch 915/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 0s 729us/step - loss: 1.6779 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00915: loss did not improve from 1.67686\n",
      "Epoch 916/2000\n",
      "93/93 [==============================] - 0s 804us/step - loss: 1.6793 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00916: loss did not improve from 1.67686\n",
      "Epoch 917/2000\n",
      "93/93 [==============================] - 0s 815us/step - loss: 1.6785 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00917: loss did not improve from 1.67686\n",
      "Epoch 918/2000\n",
      "93/93 [==============================] - 0s 740us/step - loss: 1.6780 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00918: loss did not improve from 1.67686\n",
      "Epoch 919/2000\n",
      "93/93 [==============================] - 0s 805us/step - loss: 1.6779 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00919: loss did not improve from 1.67686\n",
      "Epoch 920/2000\n",
      "93/93 [==============================] - 0s 826us/step - loss: 1.6779 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00920: loss did not improve from 1.67686\n",
      "Epoch 921/2000\n",
      "93/93 [==============================] - 0s 804us/step - loss: 1.6780 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00921: loss did not improve from 1.67686\n",
      "Epoch 922/2000\n",
      "93/93 [==============================] - 0s 815us/step - loss: 1.6778 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00922: loss did not improve from 1.67686\n",
      "Epoch 923/2000\n",
      "93/93 [==============================] - 0s 662us/step - loss: 1.6779 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00923: loss did not improve from 1.67686\n",
      "Epoch 924/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6778 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00924: loss did not improve from 1.67686\n",
      "Epoch 925/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6780 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00925: loss did not improve from 1.67686\n",
      "Epoch 926/2000\n",
      "93/93 [==============================] - 0s 634us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00926: loss did not improve from 1.67686\n",
      "Epoch 927/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6779 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00927: loss did not improve from 1.67686\n",
      "Epoch 928/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6779 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00928: loss did not improve from 1.67686\n",
      "Epoch 929/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6777 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00929: loss did not improve from 1.67686\n",
      "Epoch 930/2000\n",
      "93/93 [==============================] - 0s 653us/step - loss: 1.6779 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00930: loss did not improve from 1.67686\n",
      "Epoch 931/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00931: loss did not improve from 1.67686\n",
      "Epoch 932/2000\n",
      "93/93 [==============================] - 0s 729us/step - loss: 1.6778 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00932: loss did not improve from 1.67686\n",
      "Epoch 933/2000\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00933: loss did not improve from 1.67686\n",
      "Epoch 934/2000\n",
      "93/93 [==============================] - 0s 933us/step - loss: 1.6778 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00934: loss did not improve from 1.67686\n",
      "Epoch 935/2000\n",
      "93/93 [==============================] - 0s 761us/step - loss: 1.6780 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00935: loss did not improve from 1.67686\n",
      "Epoch 936/2000\n",
      "93/93 [==============================] - 0s 748us/step - loss: 1.6780 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00936: loss did not improve from 1.67686\n",
      "Epoch 937/2000\n",
      "93/93 [==============================] - 0s 954us/step - loss: 1.6779 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00937: loss did not improve from 1.67686\n",
      "Epoch 938/2000\n",
      "93/93 [==============================] - 0s 955us/step - loss: 1.6780 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00938: loss did not improve from 1.67686\n",
      "Epoch 939/2000\n",
      "93/93 [==============================] - 0s 912us/step - loss: 1.6778 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00939: loss did not improve from 1.67686\n",
      "Epoch 940/2000\n",
      "93/93 [==============================] - 0s 740us/step - loss: 1.6780 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00940: loss did not improve from 1.67686\n",
      "Epoch 941/2000\n",
      "93/93 [==============================] - 0s 728us/step - loss: 1.6779 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00941: loss did not improve from 1.67686\n",
      "Epoch 942/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00942: loss did not improve from 1.67686\n",
      "Epoch 943/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6796 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00943: loss did not improve from 1.67686\n",
      "Epoch 944/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6778 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00944: loss did not improve from 1.67686\n",
      "Epoch 945/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6779 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00945: loss did not improve from 1.67686\n",
      "Epoch 946/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6778 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00946: loss did not improve from 1.67686\n",
      "Epoch 947/2000\n",
      "93/93 [==============================] - 0s 594us/step - loss: 1.6779 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00947: loss did not improve from 1.67686\n",
      "Epoch 948/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6779 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00948: loss did not improve from 1.67686\n",
      "Epoch 949/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00949: loss did not improve from 1.67686\n",
      "Epoch 950/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6777 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00950: loss did not improve from 1.67686\n",
      "Epoch 951/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00951: loss did not improve from 1.67686\n",
      "Epoch 952/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00952: loss did not improve from 1.67686\n",
      "Epoch 953/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6779 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00953: loss did not improve from 1.67686\n",
      "Epoch 954/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00954: loss did not improve from 1.67686\n",
      "Epoch 955/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6778 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00955: loss did not improve from 1.67686\n",
      "Epoch 956/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6779 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00956: loss did not improve from 1.67686\n",
      "Epoch 957/2000\n",
      "93/93 [==============================] - 0s 594us/step - loss: 1.6778 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00957: loss did not improve from 1.67686\n",
      "Epoch 958/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00958: loss did not improve from 1.67686\n",
      "Epoch 959/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6779 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00959: loss did not improve from 1.67686\n",
      "Epoch 960/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00960: loss did not improve from 1.67686\n",
      "Epoch 961/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00961: loss did not improve from 1.67686\n",
      "Epoch 962/2000\n",
      "93/93 [==============================] - 0s 596us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00962: loss did not improve from 1.67686\n",
      "Epoch 963/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00963: loss did not improve from 1.67686\n",
      "Epoch 964/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6778 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00964: loss did not improve from 1.67686\n",
      "Epoch 965/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00965: loss did not improve from 1.67686\n",
      "Epoch 966/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6778 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00966: loss did not improve from 1.67686\n",
      "Epoch 967/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00967: loss did not improve from 1.67686\n",
      "Epoch 968/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00968: loss did not improve from 1.67686\n",
      "Epoch 969/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 0s 667us/step - loss: 1.6778 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00969: loss did not improve from 1.67686\n",
      "Epoch 970/2000\n",
      "93/93 [==============================] - 0s 815us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00970: loss did not improve from 1.67686\n",
      "Epoch 971/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6778 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00971: loss did not improve from 1.67686\n",
      "Epoch 972/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00972: loss did not improve from 1.67686\n",
      "Epoch 973/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6779 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00973: loss did not improve from 1.67686\n",
      "Epoch 974/2000\n",
      "93/93 [==============================] - 0s 588us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00974: loss did not improve from 1.67686\n",
      "Epoch 975/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6779 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00975: loss did not improve from 1.67686\n",
      "Epoch 976/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6777 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00976: loss did not improve from 1.67686\n",
      "Epoch 977/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6839 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00977: loss did not improve from 1.67686\n",
      "Epoch 978/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6779 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00978: loss did not improve from 1.67686\n",
      "Epoch 979/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6784 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00979: loss did not improve from 1.67686\n",
      "Epoch 980/2000\n",
      "93/93 [==============================] - 0s 603us/step - loss: 1.6782 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00980: loss did not improve from 1.67686\n",
      "Epoch 981/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6781 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00981: loss did not improve from 1.67686\n",
      "Epoch 982/2000\n",
      "93/93 [==============================] - 0s 609us/step - loss: 1.6781 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00982: loss did not improve from 1.67686\n",
      "Epoch 983/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6781 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00983: loss did not improve from 1.67686\n",
      "Epoch 984/2000\n",
      "93/93 [==============================] - 0s 583us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00984: loss did not improve from 1.67686\n",
      "Epoch 985/2000\n",
      "93/93 [==============================] - 0s 613us/step - loss: 1.6783 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00985: loss did not improve from 1.67686\n",
      "Epoch 986/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6845 - accuracy: 0.4731\n",
      "\n",
      "Epoch 00986: loss did not improve from 1.67686\n",
      "Epoch 987/2000\n",
      "93/93 [==============================] - 0s 581us/step - loss: 1.6811 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00987: loss did not improve from 1.67686\n",
      "Epoch 988/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 2.0055 - accuracy: 0.4409\n",
      "\n",
      "Epoch 00988: loss did not improve from 1.67686\n",
      "Epoch 989/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 2.9784 - accuracy: 0.4301\n",
      "\n",
      "Epoch 00989: loss did not improve from 1.67686\n",
      "Epoch 990/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.9174 - accuracy: 0.4409\n",
      "\n",
      "Epoch 00990: loss did not improve from 1.67686\n",
      "Epoch 991/2000\n",
      "93/93 [==============================] - 0s 578us/step - loss: 1.6836 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00991: loss did not improve from 1.67686\n",
      "Epoch 992/2000\n",
      "93/93 [==============================] - 0s 583us/step - loss: 1.6797 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00992: loss did not improve from 1.67686\n",
      "Epoch 993/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6782 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00993: loss did not improve from 1.67686\n",
      "Epoch 994/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6782 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00994: loss did not improve from 1.67686\n",
      "Epoch 995/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6784 - accuracy: 0.4839\n",
      "\n",
      "Epoch 00995: loss did not improve from 1.67686\n",
      "Epoch 996/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6783 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00996: loss did not improve from 1.67686\n",
      "Epoch 997/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6779 - accuracy: 0.4516\n",
      "\n",
      "Epoch 00997: loss did not improve from 1.67686\n",
      "Epoch 998/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6781 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00998: loss did not improve from 1.67686\n",
      "Epoch 999/2000\n",
      "93/93 [==============================] - 0s 585us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 00999: loss did not improve from 1.67686\n",
      "Epoch 1000/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01000: loss did not improve from 1.67686\n",
      "Epoch 1001/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6783 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01001: loss did not improve from 1.67686\n",
      "Epoch 1002/2000\n",
      "93/93 [==============================] - 0s 603us/step - loss: 1.6784 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01002: loss did not improve from 1.67686\n",
      "Epoch 1003/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6785 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01003: loss did not improve from 1.67686\n",
      "Epoch 1004/2000\n",
      "93/93 [==============================] - 0s 594us/step - loss: 1.6783 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01004: loss did not improve from 1.67686\n",
      "Epoch 1005/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6782 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01005: loss did not improve from 1.67686\n",
      "Epoch 1006/2000\n",
      "93/93 [==============================] - 0s 600us/step - loss: 1.6777 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01006: loss did not improve from 1.67686\n",
      "Epoch 1007/2000\n",
      "93/93 [==============================] - 0s 608us/step - loss: 1.6782 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01007: loss did not improve from 1.67686\n",
      "Epoch 1008/2000\n",
      "93/93 [==============================] - 0s 593us/step - loss: 1.6782 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01008: loss did not improve from 1.67686\n",
      "Epoch 1009/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6783 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01009: loss did not improve from 1.67686\n",
      "Epoch 1010/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01010: loss did not improve from 1.67686\n",
      "Epoch 1011/2000\n",
      "93/93 [==============================] - 0s 581us/step - loss: 1.6780 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01011: loss did not improve from 1.67686\n",
      "Epoch 1012/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6781 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01012: loss did not improve from 1.67686\n",
      "Epoch 1013/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6782 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01013: loss did not improve from 1.67686\n",
      "Epoch 1014/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6780 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01014: loss did not improve from 1.67686\n",
      "Epoch 1015/2000\n",
      "93/93 [==============================] - 0s 579us/step - loss: 1.6782 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01015: loss did not improve from 1.67686\n",
      "Epoch 1016/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6780 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01016: loss did not improve from 1.67686\n",
      "Epoch 1017/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6779 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01017: loss did not improve from 1.67686\n",
      "Epoch 1018/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01018: loss did not improve from 1.67686\n",
      "Epoch 1019/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6778 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01019: loss did not improve from 1.67686\n",
      "Epoch 1020/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6778 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01020: loss did not improve from 1.67686\n",
      "Epoch 1021/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6779 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01021: loss did not improve from 1.67686\n",
      "Epoch 1022/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6782 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01022: loss did not improve from 1.67686\n",
      "Epoch 1023/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 0s 613us/step - loss: 1.6783 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01023: loss did not improve from 1.67686\n",
      "Epoch 1024/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01024: loss did not improve from 1.67686\n",
      "Epoch 1025/2000\n",
      "93/93 [==============================] - 0s 624us/step - loss: 1.6778 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01025: loss did not improve from 1.67686\n",
      "Epoch 1026/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01026: loss did not improve from 1.67686\n",
      "Epoch 1027/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6778 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01027: loss did not improve from 1.67686\n",
      "Epoch 1028/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01028: loss did not improve from 1.67686\n",
      "Epoch 1029/2000\n",
      "93/93 [==============================] - 0s 614us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01029: loss did not improve from 1.67686\n",
      "Epoch 1030/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6776 - accuracy: 0.4946\n",
      "\n",
      "Epoch 01030: loss did not improve from 1.67686\n",
      "Epoch 1031/2000\n",
      "93/93 [==============================] - 0s 610us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01031: loss did not improve from 1.67686\n",
      "Epoch 1032/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01032: loss did not improve from 1.67686\n",
      "Epoch 1033/2000\n",
      "93/93 [==============================] - 0s 581us/step - loss: 1.6777 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01033: loss did not improve from 1.67686\n",
      "Epoch 1034/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6778 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01034: loss did not improve from 1.67686\n",
      "Epoch 1035/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01035: loss did not improve from 1.67686\n",
      "Epoch 1036/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01036: loss did not improve from 1.67686\n",
      "Epoch 1037/2000\n",
      "93/93 [==============================] - 0s 624us/step - loss: 1.6778 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01037: loss did not improve from 1.67686\n",
      "Epoch 1038/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01038: loss did not improve from 1.67686\n",
      "Epoch 1039/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6778 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01039: loss did not improve from 1.67686\n",
      "Epoch 1040/2000\n",
      "93/93 [==============================] - 0s 624us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01040: loss did not improve from 1.67686\n",
      "Epoch 1041/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6778 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01041: loss did not improve from 1.67686\n",
      "Epoch 1042/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6778 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01042: loss did not improve from 1.67686\n",
      "Epoch 1043/2000\n",
      "93/93 [==============================] - 0s 645us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01043: loss did not improve from 1.67686\n",
      "Epoch 1044/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6777 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01044: loss did not improve from 1.67686\n",
      "Epoch 1045/2000\n",
      "93/93 [==============================] - 0s 602us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01045: loss did not improve from 1.67686\n",
      "Epoch 1046/2000\n",
      "93/93 [==============================] - 0s 588us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01046: loss did not improve from 1.67686\n",
      "Epoch 1047/2000\n",
      "93/93 [==============================] - 0s 581us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01047: loss did not improve from 1.67686\n",
      "Epoch 1048/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6777 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01048: loss did not improve from 1.67686\n",
      "Epoch 1049/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01049: loss did not improve from 1.67686\n",
      "Epoch 1050/2000\n",
      "93/93 [==============================] - 0s 603us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01050: loss did not improve from 1.67686\n",
      "Epoch 1051/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01051: loss did not improve from 1.67686\n",
      "Epoch 1052/2000\n",
      "93/93 [==============================] - 0s 581us/step - loss: 1.6777 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01052: loss did not improve from 1.67686\n",
      "Epoch 1053/2000\n",
      "93/93 [==============================] - 0s 614us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01053: loss did not improve from 1.67686\n",
      "Epoch 1054/2000\n",
      "93/93 [==============================] - 0s 578us/step - loss: 1.6778 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01054: loss did not improve from 1.67686\n",
      "Epoch 1055/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6777 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01055: loss did not improve from 1.67686\n",
      "Epoch 1056/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6778 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01056: loss did not improve from 1.67686\n",
      "Epoch 1057/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6777 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01057: loss did not improve from 1.67686\n",
      "Epoch 1058/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01058: loss did not improve from 1.67686\n",
      "Epoch 1059/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6776 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01059: loss did not improve from 1.67686\n",
      "Epoch 1060/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6779 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01060: loss did not improve from 1.67686\n",
      "Epoch 1061/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6778 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01061: loss did not improve from 1.67686\n",
      "Epoch 1062/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6780 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01062: loss did not improve from 1.67686\n",
      "Epoch 1063/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6776 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01063: loss did not improve from 1.67686\n",
      "Epoch 1064/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6777 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01064: loss did not improve from 1.67686\n",
      "Epoch 1065/2000\n",
      "93/93 [==============================] - 0s 613us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01065: loss did not improve from 1.67686\n",
      "Epoch 1066/2000\n",
      "93/93 [==============================] - 0s 585us/step - loss: 1.6777 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01066: loss did not improve from 1.67686\n",
      "Epoch 1067/2000\n",
      "93/93 [==============================] - 0s 584us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01067: loss did not improve from 1.67686\n",
      "Epoch 1068/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01068: loss did not improve from 1.67686\n",
      "Epoch 1069/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01069: loss did not improve from 1.67686\n",
      "Epoch 1070/2000\n",
      "93/93 [==============================] - 0s 635us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01070: loss did not improve from 1.67686\n",
      "Epoch 1071/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01071: loss did not improve from 1.67686\n",
      "Epoch 1072/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6776 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01072: loss did not improve from 1.67686\n",
      "Epoch 1073/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6776 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01073: loss did not improve from 1.67686\n",
      "Epoch 1074/2000\n",
      "93/93 [==============================] - 0s 602us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01074: loss did not improve from 1.67686\n",
      "Epoch 1075/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6778 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01075: loss did not improve from 1.67686\n",
      "Epoch 1076/2000\n",
      "93/93 [==============================] - 0s 632us/step - loss: 1.6777 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01076: loss did not improve from 1.67686\n",
      "Epoch 1077/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01077: loss did not improve from 1.67686\n",
      "Epoch 1078/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6778 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01078: loss did not improve from 1.67686\n",
      "Epoch 1079/2000\n",
      "93/93 [==============================] - 0s 603us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01079: loss did not improve from 1.67686\n",
      "Epoch 1080/2000\n",
      "93/93 [==============================] - 0s 578us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01080: loss did not improve from 1.67686\n",
      "Epoch 1081/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01081: loss did not improve from 1.67686\n",
      "Epoch 1082/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6779 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01082: loss did not improve from 1.67686\n",
      "Epoch 1083/2000\n",
      "93/93 [==============================] - 0s 602us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01083: loss did not improve from 1.67686\n",
      "Epoch 1084/2000\n",
      "93/93 [==============================] - 0s 613us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01084: loss did not improve from 1.67686\n",
      "Epoch 1085/2000\n",
      "93/93 [==============================] - 0s 578us/step - loss: 1.6777 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01085: loss did not improve from 1.67686\n",
      "Epoch 1086/2000\n",
      "93/93 [==============================] - 0s 593us/step - loss: 1.6777 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01086: loss did not improve from 1.67686\n",
      "Epoch 1087/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01087: loss did not improve from 1.67686\n",
      "Epoch 1088/2000\n",
      "93/93 [==============================] - 0s 578us/step - loss: 1.6777 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01088: loss did not improve from 1.67686\n",
      "Epoch 1089/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01089: loss did not improve from 1.67686\n",
      "Epoch 1090/2000\n",
      "93/93 [==============================] - 0s 604us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01090: loss did not improve from 1.67686\n",
      "Epoch 1091/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01091: loss did not improve from 1.67686\n",
      "Epoch 1092/2000\n",
      "93/93 [==============================] - 0s 664us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01092: loss did not improve from 1.67686\n",
      "Epoch 1093/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01093: loss did not improve from 1.67686\n",
      "Epoch 1094/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01094: loss did not improve from 1.67686\n",
      "Epoch 1095/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6776 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01095: loss did not improve from 1.67686\n",
      "Epoch 1096/2000\n",
      "93/93 [==============================] - 0s 697us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01096: loss did not improve from 1.67686\n",
      "Epoch 1097/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6787 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01097: loss did not improve from 1.67686\n",
      "Epoch 1098/2000\n",
      "93/93 [==============================] - 0s 623us/step - loss: 1.6777 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01098: loss did not improve from 1.67686\n",
      "Epoch 1099/2000\n",
      "93/93 [==============================] - 0s 664us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01099: loss did not improve from 1.67686\n",
      "Epoch 1100/2000\n",
      "93/93 [==============================] - 0s 697us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01100: loss did not improve from 1.67686\n",
      "Epoch 1101/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01101: loss did not improve from 1.67686\n",
      "Epoch 1102/2000\n",
      "93/93 [==============================] - 0s 688us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01102: loss did not improve from 1.67686\n",
      "Epoch 1103/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6776 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01103: loss did not improve from 1.67686\n",
      "Epoch 1104/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01104: loss did not improve from 1.67686\n",
      "Epoch 1105/2000\n",
      "93/93 [==============================] - 0s 697us/step - loss: 1.6777 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01105: loss did not improve from 1.67686\n",
      "Epoch 1106/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01106: loss did not improve from 1.67686\n",
      "Epoch 1107/2000\n",
      "93/93 [==============================] - 0s 599us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01107: loss did not improve from 1.67686\n",
      "Epoch 1108/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6777 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01108: loss did not improve from 1.67686\n",
      "Epoch 1109/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6777 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01109: loss did not improve from 1.67686\n",
      "Epoch 1110/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6776 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01110: loss did not improve from 1.67686\n",
      "Epoch 1111/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6777 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01111: loss did not improve from 1.67686\n",
      "Epoch 1112/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01112: loss did not improve from 1.67686\n",
      "Epoch 1113/2000\n",
      "93/93 [==============================] - 0s 599us/step - loss: 1.6777 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01113: loss did not improve from 1.67686\n",
      "Epoch 1114/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01114: loss did not improve from 1.67686\n",
      "Epoch 1115/2000\n",
      "93/93 [==============================] - 0s 602us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01115: loss did not improve from 1.67686\n",
      "Epoch 1116/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6777 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01116: loss did not improve from 1.67686\n",
      "Epoch 1117/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01117: loss did not improve from 1.67686\n",
      "Epoch 1118/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6776 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01118: loss did not improve from 1.67686\n",
      "Epoch 1119/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01119: loss did not improve from 1.67686\n",
      "Epoch 1120/2000\n",
      "93/93 [==============================] - 0s 674us/step - loss: 1.6776 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01120: loss did not improve from 1.67686\n",
      "Epoch 1121/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01121: loss did not improve from 1.67686\n",
      "Epoch 1122/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01122: loss did not improve from 1.67686\n",
      "Epoch 1123/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01123: loss did not improve from 1.67686\n",
      "Epoch 1124/2000\n",
      "93/93 [==============================] - 0s 697us/step - loss: 1.6778 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01124: loss did not improve from 1.67686\n",
      "Epoch 1125/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01125: loss did not improve from 1.67686\n",
      "Epoch 1126/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6780 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01126: loss did not improve from 1.67686\n",
      "Epoch 1127/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01127: loss did not improve from 1.67686\n",
      "Epoch 1128/2000\n",
      "93/93 [==============================] - 0s 634us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01128: loss did not improve from 1.67686\n",
      "Epoch 1129/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 0s 600us/step - loss: 1.6776 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01129: loss did not improve from 1.67686\n",
      "Epoch 1130/2000\n",
      "93/93 [==============================] - 0s 656us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01130: loss did not improve from 1.67686\n",
      "Epoch 1131/2000\n",
      "93/93 [==============================] - 0s 599us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01131: loss did not improve from 1.67686\n",
      "Epoch 1132/2000\n",
      "93/93 [==============================] - 0s 653us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01132: loss did not improve from 1.67686\n",
      "Epoch 1133/2000\n",
      "93/93 [==============================] - 0s 594us/step - loss: 1.6776 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01133: loss did not improve from 1.67686\n",
      "Epoch 1134/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01134: loss did not improve from 1.67686\n",
      "Epoch 1135/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6776 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01135: loss did not improve from 1.67686\n",
      "Epoch 1136/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01136: loss did not improve from 1.67686\n",
      "Epoch 1137/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6775 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01137: loss did not improve from 1.67686\n",
      "Epoch 1138/2000\n",
      "93/93 [==============================] - 0s 613us/step - loss: 1.6776 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01138: loss did not improve from 1.67686\n",
      "Epoch 1139/2000\n",
      "93/93 [==============================] - 0s 600us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01139: loss did not improve from 1.67686\n",
      "Epoch 1140/2000\n",
      "93/93 [==============================] - 0s 608us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01140: loss did not improve from 1.67686\n",
      "Epoch 1141/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6813 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01141: loss did not improve from 1.67686\n",
      "Epoch 1142/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6816 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01142: loss did not improve from 1.67686\n",
      "Epoch 1143/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6779 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01143: loss did not improve from 1.67686\n",
      "Epoch 1144/2000\n",
      "93/93 [==============================] - 0s 587us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01144: loss did not improve from 1.67686\n",
      "Epoch 1145/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6778 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01145: loss did not improve from 1.67686\n",
      "Epoch 1146/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6779 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01146: loss did not improve from 1.67686\n",
      "Epoch 1147/2000\n",
      "93/93 [==============================] - 0s 645us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01147: loss did not improve from 1.67686\n",
      "Epoch 1148/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6777 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01148: loss did not improve from 1.67686\n",
      "Epoch 1149/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01149: loss did not improve from 1.67686\n",
      "Epoch 1150/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01150: loss did not improve from 1.67686\n",
      "Epoch 1151/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01151: loss did not improve from 1.67686\n",
      "Epoch 1152/2000\n",
      "93/93 [==============================] - 0s 607us/step - loss: 1.6778 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01152: loss did not improve from 1.67686\n",
      "Epoch 1153/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01153: loss did not improve from 1.67686\n",
      "Epoch 1154/2000\n",
      "93/93 [==============================] - 0s 683us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01154: loss did not improve from 1.67686\n",
      "Epoch 1155/2000\n",
      "93/93 [==============================] - 0s 717us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01155: loss did not improve from 1.67686\n",
      "Epoch 1156/2000\n",
      "93/93 [==============================] - 0s 602us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01156: loss did not improve from 1.67686\n",
      "Epoch 1157/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6777 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01157: loss did not improve from 1.67686\n",
      "Epoch 1158/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01158: loss did not improve from 1.67686\n",
      "Epoch 1159/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01159: loss did not improve from 1.67686\n",
      "Epoch 1160/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6819 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01160: loss did not improve from 1.67686\n",
      "Epoch 1161/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6826 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01161: loss did not improve from 1.67686\n",
      "Epoch 1162/2000\n",
      "93/93 [==============================] - 0s 664us/step - loss: 1.6775 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01162: loss did not improve from 1.67686\n",
      "Epoch 1163/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01163: loss did not improve from 1.67686\n",
      "Epoch 1164/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01164: loss did not improve from 1.67686\n",
      "Epoch 1165/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01165: loss did not improve from 1.67686\n",
      "Epoch 1166/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01166: loss did not improve from 1.67686\n",
      "Epoch 1167/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6777 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01167: loss did not improve from 1.67686\n",
      "Epoch 1168/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6776 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01168: loss did not improve from 1.67686\n",
      "Epoch 1169/2000\n",
      "93/93 [==============================] - 0s 678us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01169: loss did not improve from 1.67686\n",
      "Epoch 1170/2000\n",
      "93/93 [==============================] - 0s 612us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01170: loss did not improve from 1.67686\n",
      "Epoch 1171/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01171: loss did not improve from 1.67686\n",
      "Epoch 1172/2000\n",
      "93/93 [==============================] - 0s 610us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01172: loss did not improve from 1.67686\n",
      "Epoch 1173/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01173: loss did not improve from 1.67686\n",
      "Epoch 1174/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01174: loss did not improve from 1.67686\n",
      "Epoch 1175/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01175: loss did not improve from 1.67686\n",
      "Epoch 1176/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01176: loss did not improve from 1.67686\n",
      "Epoch 1177/2000\n",
      "93/93 [==============================] - 0s 603us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01177: loss did not improve from 1.67686\n",
      "Epoch 1178/2000\n",
      "93/93 [==============================] - 0s 600us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01178: loss did not improve from 1.67686\n",
      "Epoch 1179/2000\n",
      "93/93 [==============================] - 0s 610us/step - loss: 1.6803 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01179: loss did not improve from 1.67686\n",
      "Epoch 1180/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.7000 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01180: loss did not improve from 1.67686\n",
      "Epoch 1181/2000\n",
      "93/93 [==============================] - 0s 638us/step - loss: 1.6791 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01181: loss did not improve from 1.67686\n",
      "Epoch 1182/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6870 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01182: loss did not improve from 1.67686\n",
      "Epoch 1183/2000\n",
      "93/93 [==============================] - 0s 588us/step - loss: 1.6783 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01183: loss did not improve from 1.67686\n",
      "Epoch 1184/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6780 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01184: loss did not improve from 1.67686\n",
      "Epoch 1185/2000\n",
      "93/93 [==============================] - 0s 626us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01185: loss did not improve from 1.67686\n",
      "Epoch 1186/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6779 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01186: loss did not improve from 1.67686\n",
      "Epoch 1187/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6784 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01187: loss did not improve from 1.67686\n",
      "Epoch 1188/2000\n",
      "93/93 [==============================] - 0s 699us/step - loss: 1.6781 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01188: loss did not improve from 1.67686\n",
      "Epoch 1189/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6778 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01189: loss did not improve from 1.67686\n",
      "Epoch 1190/2000\n",
      "93/93 [==============================] - 0s 588us/step - loss: 1.6776 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01190: loss did not improve from 1.67686\n",
      "Epoch 1191/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01191: loss did not improve from 1.67686\n",
      "Epoch 1192/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01192: loss did not improve from 1.67686\n",
      "Epoch 1193/2000\n",
      "93/93 [==============================] - 0s 585us/step - loss: 1.6780 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01193: loss did not improve from 1.67686\n",
      "Epoch 1194/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6783 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01194: loss did not improve from 1.67686\n",
      "Epoch 1195/2000\n",
      "93/93 [==============================] - 0s 586us/step - loss: 1.6778 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01195: loss did not improve from 1.67686\n",
      "Epoch 1196/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6777 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01196: loss did not improve from 1.67686\n",
      "Epoch 1197/2000\n",
      "93/93 [==============================] - 0s 602us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01197: loss did not improve from 1.67686\n",
      "Epoch 1198/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01198: loss did not improve from 1.67686\n",
      "Epoch 1199/2000\n",
      "93/93 [==============================] - 0s 602us/step - loss: 1.6785 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01199: loss did not improve from 1.67686\n",
      "Epoch 1200/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6780 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01200: loss did not improve from 1.67686\n",
      "Epoch 1201/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6777 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01201: loss did not improve from 1.67686\n",
      "Epoch 1202/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6777 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01202: loss did not improve from 1.67686\n",
      "Epoch 1203/2000\n",
      "93/93 [==============================] - 0s 719us/step - loss: 1.6777 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01203: loss did not improve from 1.67686\n",
      "Epoch 1204/2000\n",
      "93/93 [==============================] - 0s 761us/step - loss: 1.6777 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01204: loss did not improve from 1.67686\n",
      "Epoch 1205/2000\n",
      "93/93 [==============================] - 0s 761us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01205: loss did not improve from 1.67686\n",
      "Epoch 1206/2000\n",
      "93/93 [==============================] - 0s 707us/step - loss: 1.6778 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01206: loss did not improve from 1.67686\n",
      "Epoch 1207/2000\n",
      "93/93 [==============================] - 0s 771us/step - loss: 1.6776 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01207: loss did not improve from 1.67686\n",
      "Epoch 1208/2000\n",
      "93/93 [==============================] - 0s 719us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01208: loss did not improve from 1.67686\n",
      "Epoch 1209/2000\n",
      "93/93 [==============================] - 0s 740us/step - loss: 1.6776 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01209: loss did not improve from 1.67686\n",
      "Epoch 1210/2000\n",
      "93/93 [==============================] - 0s 772us/step - loss: 1.6773 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01210: loss did not improve from 1.67686\n",
      "Epoch 1211/2000\n",
      "93/93 [==============================] - 0s 761us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01211: loss did not improve from 1.67686\n",
      "Epoch 1212/2000\n",
      "93/93 [==============================] - 0s 729us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01212: loss did not improve from 1.67686\n",
      "Epoch 1213/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01213: loss did not improve from 1.67686\n",
      "Epoch 1214/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01214: loss did not improve from 1.67686\n",
      "Epoch 1215/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01215: loss did not improve from 1.67686\n",
      "Epoch 1216/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6777 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01216: loss did not improve from 1.67686\n",
      "Epoch 1217/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6777 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01217: loss did not improve from 1.67686\n",
      "Epoch 1218/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6775 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01218: loss did not improve from 1.67686\n",
      "Epoch 1219/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01219: loss did not improve from 1.67686\n",
      "Epoch 1220/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6780 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01220: loss did not improve from 1.67686\n",
      "Epoch 1221/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01221: loss did not improve from 1.67686\n",
      "Epoch 1222/2000\n",
      "93/93 [==============================] - 0s 598us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01222: loss did not improve from 1.67686\n",
      "Epoch 1223/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6776 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01223: loss did not improve from 1.67686\n",
      "Epoch 1224/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01224: loss did not improve from 1.67686\n",
      "Epoch 1225/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01225: loss did not improve from 1.67686\n",
      "Epoch 1226/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6776 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01226: loss did not improve from 1.67686\n",
      "Epoch 1227/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01227: loss did not improve from 1.67686\n",
      "Epoch 1228/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6777 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01228: loss did not improve from 1.67686\n",
      "Epoch 1229/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6776 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01229: loss did not improve from 1.67686\n",
      "Epoch 1230/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01230: loss did not improve from 1.67686\n",
      "Epoch 1231/2000\n",
      "93/93 [==============================] - 0s 602us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01231: loss did not improve from 1.67686\n",
      "Epoch 1232/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6777 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01232: loss did not improve from 1.67686\n",
      "Epoch 1233/2000\n",
      "93/93 [==============================] - 0s 597us/step - loss: 1.6777 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01233: loss did not improve from 1.67686\n",
      "Epoch 1234/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6775 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01234: loss did not improve from 1.67686\n",
      "Epoch 1235/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 0s 601us/step - loss: 1.6777 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01235: loss did not improve from 1.67686\n",
      "Epoch 1236/2000\n",
      "93/93 [==============================] - 0s 630us/step - loss: 1.6779 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01236: loss did not improve from 1.67686\n",
      "Epoch 1237/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01237: loss did not improve from 1.67686\n",
      "Epoch 1238/2000\n",
      "93/93 [==============================] - 0s 612us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01238: loss did not improve from 1.67686\n",
      "Epoch 1239/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6778 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01239: loss did not improve from 1.67686\n",
      "Epoch 1240/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01240: loss did not improve from 1.67686\n",
      "Epoch 1241/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01241: loss did not improve from 1.67686\n",
      "Epoch 1242/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01242: loss did not improve from 1.67686\n",
      "Epoch 1243/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01243: loss did not improve from 1.67686\n",
      "Epoch 1244/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01244: loss did not improve from 1.67686\n",
      "Epoch 1245/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01245: loss did not improve from 1.67686\n",
      "Epoch 1246/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01246: loss did not improve from 1.67686\n",
      "Epoch 1247/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01247: loss did not improve from 1.67686\n",
      "Epoch 1248/2000\n",
      "93/93 [==============================] - 0s 585us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01248: loss did not improve from 1.67686\n",
      "Epoch 1249/2000\n",
      "93/93 [==============================] - 0s 579us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01249: loss did not improve from 1.67686\n",
      "Epoch 1250/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6777 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01250: loss did not improve from 1.67686\n",
      "Epoch 1251/2000\n",
      "93/93 [==============================] - 0s 593us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01251: loss did not improve from 1.67686\n",
      "Epoch 1252/2000\n",
      "93/93 [==============================] - 0s 614us/step - loss: 1.6776 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01252: loss did not improve from 1.67686\n",
      "Epoch 1253/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6776 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01253: loss did not improve from 1.67686\n",
      "Epoch 1254/2000\n",
      "93/93 [==============================] - 0s 603us/step - loss: 1.6776 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01254: loss did not improve from 1.67686\n",
      "Epoch 1255/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01255: loss did not improve from 1.67686\n",
      "Epoch 1256/2000\n",
      "93/93 [==============================] - 0s 578us/step - loss: 1.6776 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01256: loss did not improve from 1.67686\n",
      "Epoch 1257/2000\n",
      "93/93 [==============================] - 0s 603us/step - loss: 1.6776 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01257: loss did not improve from 1.67686\n",
      "Epoch 1258/2000\n",
      "93/93 [==============================] - 0s 610us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01258: loss did not improve from 1.67686\n",
      "Epoch 1259/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01259: loss did not improve from 1.67686\n",
      "Epoch 1260/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01260: loss did not improve from 1.67686\n",
      "Epoch 1261/2000\n",
      "93/93 [==============================] - 0s 581us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01261: loss did not improve from 1.67686\n",
      "Epoch 1262/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01262: loss did not improve from 1.67686\n",
      "Epoch 1263/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6776 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01263: loss did not improve from 1.67686\n",
      "Epoch 1264/2000\n",
      "93/93 [==============================] - 0s 583us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01264: loss did not improve from 1.67686\n",
      "Epoch 1265/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6776 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01265: loss did not improve from 1.67686\n",
      "Epoch 1266/2000\n",
      "93/93 [==============================] - 0s 578us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01266: loss did not improve from 1.67686\n",
      "Epoch 1267/2000\n",
      "93/93 [==============================] - 0s 624us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01267: loss did not improve from 1.67686\n",
      "Epoch 1268/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6776 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01268: loss did not improve from 1.67686\n",
      "Epoch 1269/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6776 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01269: loss did not improve from 1.67686\n",
      "Epoch 1270/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01270: loss did not improve from 1.67686\n",
      "Epoch 1271/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01271: loss did not improve from 1.67686\n",
      "Epoch 1272/2000\n",
      "93/93 [==============================] - 0s 593us/step - loss: 1.6776 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01272: loss did not improve from 1.67686\n",
      "Epoch 1273/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01273: loss did not improve from 1.67686\n",
      "Epoch 1274/2000\n",
      "93/93 [==============================] - 0s 578us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01274: loss did not improve from 1.67686\n",
      "Epoch 1275/2000\n",
      "93/93 [==============================] - 0s 588us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01275: loss did not improve from 1.67686\n",
      "Epoch 1276/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01276: loss did not improve from 1.67686\n",
      "Epoch 1277/2000\n",
      "93/93 [==============================] - 0s 595us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01277: loss did not improve from 1.67686\n",
      "Epoch 1278/2000\n",
      "93/93 [==============================] - 0s 624us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01278: loss did not improve from 1.67686\n",
      "Epoch 1279/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6776 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01279: loss did not improve from 1.67686\n",
      "Epoch 1280/2000\n",
      "93/93 [==============================] - 0s 602us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01280: loss did not improve from 1.67686\n",
      "Epoch 1281/2000\n",
      "93/93 [==============================] - 0s 578us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01281: loss did not improve from 1.67686\n",
      "Epoch 1282/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01282: loss did not improve from 1.67686\n",
      "Epoch 1283/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6842 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01283: loss did not improve from 1.67686\n",
      "Epoch 1284/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6782 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01284: loss did not improve from 1.67686\n",
      "Epoch 1285/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6777 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01285: loss did not improve from 1.67686\n",
      "Epoch 1286/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01286: loss did not improve from 1.67686\n",
      "Epoch 1287/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6773 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01287: loss did not improve from 1.67686\n",
      "Epoch 1288/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6777 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01288: loss did not improve from 1.67686\n",
      "Epoch 1289/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6792 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01289: loss did not improve from 1.67686\n",
      "Epoch 1290/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01290: loss did not improve from 1.67686\n",
      "Epoch 1291/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01291: loss did not improve from 1.67686\n",
      "Epoch 1292/2000\n",
      "93/93 [==============================] - 0s 581us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01292: loss did not improve from 1.67686\n",
      "Epoch 1293/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01293: loss did not improve from 1.67686\n",
      "Epoch 1294/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6776 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01294: loss did not improve from 1.67686\n",
      "Epoch 1295/2000\n",
      "93/93 [==============================] - 0s 587us/step - loss: 1.6776 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01295: loss did not improve from 1.67686\n",
      "Epoch 1296/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6775 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01296: loss did not improve from 1.67686\n",
      "Epoch 1297/2000\n",
      "93/93 [==============================] - 0s 607us/step - loss: 1.6813 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01297: loss did not improve from 1.67686\n",
      "Epoch 1298/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6780 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01298: loss did not improve from 1.67686\n",
      "Epoch 1299/2000\n",
      "93/93 [==============================] - 0s 602us/step - loss: 1.6780 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01299: loss did not improve from 1.67686\n",
      "Epoch 1300/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6756 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01300: loss improved from 1.67686 to 1.67561, saving model to ./model/1000001000100010002.hdf5\n",
      "Epoch 1301/2000\n",
      "93/93 [==============================] - 0s 626us/step - loss: 1.6948 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01301: loss did not improve from 1.67561\n",
      "Epoch 1302/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6776 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01302: loss did not improve from 1.67561\n",
      "Epoch 1303/2000\n",
      "93/93 [==============================] - 0s 632us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01303: loss did not improve from 1.67561\n",
      "Epoch 1304/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01304: loss did not improve from 1.67561\n",
      "Epoch 1305/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01305: loss did not improve from 1.67561\n",
      "Epoch 1306/2000\n",
      "93/93 [==============================] - 0s 659us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01306: loss did not improve from 1.67561\n",
      "Epoch 1307/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6777 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01307: loss did not improve from 1.67561\n",
      "Epoch 1308/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01308: loss did not improve from 1.67561\n",
      "Epoch 1309/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6778 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01309: loss did not improve from 1.67561\n",
      "Epoch 1310/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01310: loss did not improve from 1.67561\n",
      "Epoch 1311/2000\n",
      "93/93 [==============================] - 0s 651us/step - loss: 1.6778 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01311: loss did not improve from 1.67561\n",
      "Epoch 1312/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6777 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01312: loss did not improve from 1.67561\n",
      "Epoch 1313/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6782 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01313: loss did not improve from 1.67561\n",
      "Epoch 1314/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6797 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01314: loss did not improve from 1.67561\n",
      "Epoch 1315/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6784 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01315: loss did not improve from 1.67561\n",
      "Epoch 1316/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6781 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01316: loss did not improve from 1.67561\n",
      "Epoch 1317/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6781 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01317: loss did not improve from 1.67561\n",
      "Epoch 1318/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6780 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01318: loss did not improve from 1.67561\n",
      "Epoch 1319/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01319: loss did not improve from 1.67561\n",
      "Epoch 1320/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01320: loss did not improve from 1.67561\n",
      "Epoch 1321/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6776 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01321: loss did not improve from 1.67561\n",
      "Epoch 1322/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01322: loss did not improve from 1.67561\n",
      "Epoch 1323/2000\n",
      "93/93 [==============================] - 0s 697us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01323: loss did not improve from 1.67561\n",
      "Epoch 1324/2000\n",
      "93/93 [==============================] - 0s 719us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01324: loss did not improve from 1.67561\n",
      "Epoch 1325/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01325: loss did not improve from 1.67561\n",
      "Epoch 1326/2000\n",
      "93/93 [==============================] - 0s 640us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01326: loss did not improve from 1.67561\n",
      "Epoch 1327/2000\n",
      "93/93 [==============================] - 0s 644us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01327: loss did not improve from 1.67561\n",
      "Epoch 1328/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01328: loss did not improve from 1.67561\n",
      "Epoch 1329/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01329: loss did not improve from 1.67561\n",
      "Epoch 1330/2000\n",
      "93/93 [==============================] - 0s 655us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01330: loss did not improve from 1.67561\n",
      "Epoch 1331/2000\n",
      "93/93 [==============================] - 0s 632us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01331: loss did not improve from 1.67561\n",
      "Epoch 1332/2000\n",
      "93/93 [==============================] - 0s 644us/step - loss: 1.6776 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01332: loss did not improve from 1.67561\n",
      "Epoch 1333/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01333: loss did not improve from 1.67561\n",
      "Epoch 1334/2000\n",
      "93/93 [==============================] - 0s 695us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01334: loss did not improve from 1.67561\n",
      "Epoch 1335/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01335: loss did not improve from 1.67561\n",
      "Epoch 1336/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01336: loss did not improve from 1.67561\n",
      "Epoch 1337/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6959 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01337: loss did not improve from 1.67561\n",
      "Epoch 1338/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6850 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01338: loss did not improve from 1.67561\n",
      "Epoch 1339/2000\n",
      "93/93 [==============================] - 0s 641us/step - loss: 1.6784 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01339: loss did not improve from 1.67561\n",
      "Epoch 1340/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6779 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01340: loss did not improve from 1.67561\n",
      "Epoch 1341/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 0s 655us/step - loss: 1.6779 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01341: loss did not improve from 1.67561\n",
      "Epoch 1342/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6780 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01342: loss did not improve from 1.67561\n",
      "Epoch 1343/2000\n",
      "93/93 [==============================] - 0s 719us/step - loss: 1.6779 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01343: loss did not improve from 1.67561\n",
      "Epoch 1344/2000\n",
      "93/93 [==============================] - 0s 719us/step - loss: 1.6786 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01344: loss did not improve from 1.67561\n",
      "Epoch 1345/2000\n",
      "93/93 [==============================] - 0s 642us/step - loss: 1.6780 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01345: loss did not improve from 1.67561\n",
      "Epoch 1346/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6778 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01346: loss did not improve from 1.67561\n",
      "Epoch 1347/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01347: loss did not improve from 1.67561\n",
      "Epoch 1348/2000\n",
      "93/93 [==============================] - 0s 729us/step - loss: 1.6776 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01348: loss did not improve from 1.67561\n",
      "Epoch 1349/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6777 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01349: loss did not improve from 1.67561\n",
      "Epoch 1350/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01350: loss did not improve from 1.67561\n",
      "Epoch 1351/2000\n",
      "93/93 [==============================] - 0s 638us/step - loss: 1.6777 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01351: loss did not improve from 1.67561\n",
      "Epoch 1352/2000\n",
      "93/93 [==============================] - 0s 670us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01352: loss did not improve from 1.67561\n",
      "Epoch 1353/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6778 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01353: loss did not improve from 1.67561\n",
      "Epoch 1354/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6776 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01354: loss did not improve from 1.67561\n",
      "Epoch 1355/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01355: loss did not improve from 1.67561\n",
      "Epoch 1356/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6776 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01356: loss did not improve from 1.67561\n",
      "Epoch 1357/2000\n",
      "93/93 [==============================] - 0s 697us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01357: loss did not improve from 1.67561\n",
      "Epoch 1358/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01358: loss did not improve from 1.67561\n",
      "Epoch 1359/2000\n",
      "93/93 [==============================] - 0s 662us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01359: loss did not improve from 1.67561\n",
      "Epoch 1360/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01360: loss did not improve from 1.67561\n",
      "Epoch 1361/2000\n",
      "93/93 [==============================] - 0s 644us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01361: loss did not improve from 1.67561\n",
      "Epoch 1362/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01362: loss did not improve from 1.67561\n",
      "Epoch 1363/2000\n",
      "93/93 [==============================] - 0s 632us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01363: loss did not improve from 1.67561\n",
      "Epoch 1364/2000\n",
      "93/93 [==============================] - 0s 645us/step - loss: 1.6776 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01364: loss did not improve from 1.67561\n",
      "Epoch 1365/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01365: loss did not improve from 1.67561\n",
      "Epoch 1366/2000\n",
      "93/93 [==============================] - 0s 718us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01366: loss did not improve from 1.67561\n",
      "Epoch 1367/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6776 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01367: loss did not improve from 1.67561\n",
      "Epoch 1368/2000\n",
      "93/93 [==============================] - 0s 648us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01368: loss did not improve from 1.67561\n",
      "Epoch 1369/2000\n",
      "93/93 [==============================] - 0s 669us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01369: loss did not improve from 1.67561\n",
      "Epoch 1370/2000\n",
      "93/93 [==============================] - 0s 697us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01370: loss did not improve from 1.67561\n",
      "Epoch 1371/2000\n",
      "93/93 [==============================] - 0s 663us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01371: loss did not improve from 1.67561\n",
      "Epoch 1372/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6776 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01372: loss did not improve from 1.67561\n",
      "Epoch 1373/2000\n",
      "93/93 [==============================] - 0s 675us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01373: loss did not improve from 1.67561\n",
      "Epoch 1374/2000\n",
      "93/93 [==============================] - 0s 640us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01374: loss did not improve from 1.67561\n",
      "Epoch 1375/2000\n",
      "93/93 [==============================] - 0s 635us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01375: loss did not improve from 1.67561\n",
      "Epoch 1376/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01376: loss did not improve from 1.67561\n",
      "Epoch 1377/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01377: loss did not improve from 1.67561\n",
      "Epoch 1378/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6776 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01378: loss did not improve from 1.67561\n",
      "Epoch 1379/2000\n",
      "93/93 [==============================] - 0s 697us/step - loss: 1.6776 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01379: loss did not improve from 1.67561\n",
      "Epoch 1380/2000\n",
      "93/93 [==============================] - 0s 761us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01380: loss did not improve from 1.67561\n",
      "Epoch 1381/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01381: loss did not improve from 1.67561\n",
      "Epoch 1382/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6776 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01382: loss did not improve from 1.67561\n",
      "Epoch 1383/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01383: loss did not improve from 1.67561\n",
      "Epoch 1384/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01384: loss did not improve from 1.67561\n",
      "Epoch 1385/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6773 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01385: loss did not improve from 1.67561\n",
      "Epoch 1386/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01386: loss did not improve from 1.67561\n",
      "Epoch 1387/2000\n",
      "93/93 [==============================] - 0s 613us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01387: loss did not improve from 1.67561\n",
      "Epoch 1388/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01388: loss did not improve from 1.67561\n",
      "Epoch 1389/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6776 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01389: loss did not improve from 1.67561\n",
      "Epoch 1390/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6776 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01390: loss did not improve from 1.67561\n",
      "Epoch 1391/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6778 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01391: loss did not improve from 1.67561\n",
      "Epoch 1392/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6779 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01392: loss did not improve from 1.67561\n",
      "Epoch 1393/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01393: loss did not improve from 1.67561\n",
      "Epoch 1394/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01394: loss did not improve from 1.67561\n",
      "Epoch 1395/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01395: loss did not improve from 1.67561\n",
      "Epoch 1396/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01396: loss did not improve from 1.67561\n",
      "Epoch 1397/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01397: loss did not improve from 1.67561\n",
      "Epoch 1398/2000\n",
      "93/93 [==============================] - 0s 669us/step - loss: 1.6776 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01398: loss did not improve from 1.67561\n",
      "Epoch 1399/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01399: loss did not improve from 1.67561\n",
      "Epoch 1400/2000\n",
      "93/93 [==============================] - 0s 624us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01400: loss did not improve from 1.67561\n",
      "Epoch 1401/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6776 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01401: loss did not improve from 1.67561\n",
      "Epoch 1402/2000\n",
      "93/93 [==============================] - 0s 588us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01402: loss did not improve from 1.67561\n",
      "Epoch 1403/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01403: loss did not improve from 1.67561\n",
      "Epoch 1404/2000\n",
      "93/93 [==============================] - 0s 598us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01404: loss did not improve from 1.67561\n",
      "Epoch 1405/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01405: loss did not improve from 1.67561\n",
      "Epoch 1406/2000\n",
      "93/93 [==============================] - 0s 605us/step - loss: 1.6776 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01406: loss did not improve from 1.67561\n",
      "Epoch 1407/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01407: loss did not improve from 1.67561\n",
      "Epoch 1408/2000\n",
      "93/93 [==============================] - 0s 634us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01408: loss did not improve from 1.67561\n",
      "Epoch 1409/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01409: loss did not improve from 1.67561\n",
      "Epoch 1410/2000\n",
      "93/93 [==============================] - 0s 593us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01410: loss did not improve from 1.67561\n",
      "Epoch 1411/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01411: loss did not improve from 1.67561\n",
      "Epoch 1412/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01412: loss did not improve from 1.67561\n",
      "Epoch 1413/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01413: loss did not improve from 1.67561\n",
      "Epoch 1414/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01414: loss did not improve from 1.67561\n",
      "Epoch 1415/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01415: loss did not improve from 1.67561\n",
      "Epoch 1416/2000\n",
      "93/93 [==============================] - 0s 602us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01416: loss did not improve from 1.67561\n",
      "Epoch 1417/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01417: loss did not improve from 1.67561\n",
      "Epoch 1418/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01418: loss did not improve from 1.67561\n",
      "Epoch 1419/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01419: loss did not improve from 1.67561\n",
      "Epoch 1420/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01420: loss did not improve from 1.67561\n",
      "Epoch 1421/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01421: loss did not improve from 1.67561\n",
      "Epoch 1422/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6776 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01422: loss did not improve from 1.67561\n",
      "Epoch 1423/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6851 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01423: loss did not improve from 1.67561\n",
      "Epoch 1424/2000\n",
      "93/93 [==============================] - 0s 624us/step - loss: 3.3448 - accuracy: 0.4409\n",
      "\n",
      "Epoch 01424: loss did not improve from 1.67561\n",
      "Epoch 1425/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 2.6948 - accuracy: 0.4301\n",
      "\n",
      "Epoch 01425: loss did not improve from 1.67561\n",
      "Epoch 1426/2000\n",
      "93/93 [==============================] - 0s 597us/step - loss: 1.6901 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01426: loss did not improve from 1.67561\n",
      "Epoch 1427/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6777 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01427: loss did not improve from 1.67561\n",
      "Epoch 1428/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6773 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01428: loss did not improve from 1.67561\n",
      "Epoch 1429/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6776 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01429: loss did not improve from 1.67561\n",
      "Epoch 1430/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01430: loss did not improve from 1.67561\n",
      "Epoch 1431/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01431: loss did not improve from 1.67561\n",
      "Epoch 1432/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01432: loss did not improve from 1.67561\n",
      "Epoch 1433/2000\n",
      "93/93 [==============================] - 0s 586us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01433: loss did not improve from 1.67561\n",
      "Epoch 1434/2000\n",
      "93/93 [==============================] - 0s 594us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01434: loss did not improve from 1.67561\n",
      "Epoch 1435/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01435: loss did not improve from 1.67561\n",
      "Epoch 1436/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01436: loss did not improve from 1.67561\n",
      "Epoch 1437/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01437: loss did not improve from 1.67561\n",
      "Epoch 1438/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01438: loss did not improve from 1.67561\n",
      "Epoch 1439/2000\n",
      "93/93 [==============================] - 0s 587us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01439: loss did not improve from 1.67561\n",
      "Epoch 1440/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01440: loss did not improve from 1.67561\n",
      "Epoch 1441/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6774 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01441: loss did not improve from 1.67561\n",
      "Epoch 1442/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01442: loss did not improve from 1.67561\n",
      "Epoch 1443/2000\n",
      "93/93 [==============================] - 0s 586us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01443: loss did not improve from 1.67561\n",
      "Epoch 1444/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01444: loss did not improve from 1.67561\n",
      "Epoch 1445/2000\n",
      "93/93 [==============================] - 0s 623us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01445: loss did not improve from 1.67561\n",
      "Epoch 1446/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01446: loss did not improve from 1.67561\n",
      "Epoch 1447/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 0s 580us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01447: loss did not improve from 1.67561\n",
      "Epoch 1448/2000\n",
      "93/93 [==============================] - 0s 603us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01448: loss did not improve from 1.67561\n",
      "Epoch 1449/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01449: loss did not improve from 1.67561\n",
      "Epoch 1450/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01450: loss did not improve from 1.67561\n",
      "Epoch 1451/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01451: loss did not improve from 1.67561\n",
      "Epoch 1452/2000\n",
      "93/93 [==============================] - 0s 599us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01452: loss did not improve from 1.67561\n",
      "Epoch 1453/2000\n",
      "93/93 [==============================] - 0s 646us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01453: loss did not improve from 1.67561\n",
      "Epoch 1454/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01454: loss did not improve from 1.67561\n",
      "Epoch 1455/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01455: loss did not improve from 1.67561\n",
      "Epoch 1456/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01456: loss did not improve from 1.67561\n",
      "Epoch 1457/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01457: loss did not improve from 1.67561\n",
      "Epoch 1458/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01458: loss did not improve from 1.67561\n",
      "Epoch 1459/2000\n",
      "93/93 [==============================] - 0s 579us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01459: loss did not improve from 1.67561\n",
      "Epoch 1460/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01460: loss did not improve from 1.67561\n",
      "Epoch 1461/2000\n",
      "93/93 [==============================] - 0s 582us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01461: loss did not improve from 1.67561\n",
      "Epoch 1462/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01462: loss did not improve from 1.67561\n",
      "Epoch 1463/2000\n",
      "93/93 [==============================] - 0s 579us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01463: loss did not improve from 1.67561\n",
      "Epoch 1464/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01464: loss did not improve from 1.67561\n",
      "Epoch 1465/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01465: loss did not improve from 1.67561\n",
      "Epoch 1466/2000\n",
      "93/93 [==============================] - 0s 583us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01466: loss did not improve from 1.67561\n",
      "Epoch 1467/2000\n",
      "93/93 [==============================] - 0s 613us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01467: loss did not improve from 1.67561\n",
      "Epoch 1468/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01468: loss did not improve from 1.67561\n",
      "Epoch 1469/2000\n",
      "93/93 [==============================] - 0s 594us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01469: loss did not improve from 1.67561\n",
      "Epoch 1470/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01470: loss did not improve from 1.67561\n",
      "Epoch 1471/2000\n",
      "93/93 [==============================] - 0s 720us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01471: loss did not improve from 1.67561\n",
      "Epoch 1472/2000\n",
      "93/93 [==============================] - 0s 761us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01472: loss did not improve from 1.67561\n",
      "Epoch 1473/2000\n",
      "93/93 [==============================] - 0s 736us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01473: loss did not improve from 1.67561\n",
      "Epoch 1474/2000\n",
      "93/93 [==============================] - 0s 739us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01474: loss did not improve from 1.67561\n",
      "Epoch 1475/2000\n",
      "93/93 [==============================] - 0s 750us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01475: loss did not improve from 1.67561\n",
      "Epoch 1476/2000\n",
      "93/93 [==============================] - 0s 718us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01476: loss did not improve from 1.67561\n",
      "Epoch 1477/2000\n",
      "93/93 [==============================] - 0s 724us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01477: loss did not improve from 1.67561\n",
      "Epoch 1478/2000\n",
      "93/93 [==============================] - 0s 729us/step - loss: 1.6775 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01478: loss did not improve from 1.67561\n",
      "Epoch 1479/2000\n",
      "93/93 [==============================] - 0s 751us/step - loss: 1.6772 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01479: loss did not improve from 1.67561\n",
      "Epoch 1480/2000\n",
      "93/93 [==============================] - 0s 708us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01480: loss did not improve from 1.67561\n",
      "Epoch 1481/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01481: loss did not improve from 1.67561\n",
      "Epoch 1482/2000\n",
      "93/93 [==============================] - 0s 609us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01482: loss did not improve from 1.67561\n",
      "Epoch 1483/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01483: loss did not improve from 1.67561\n",
      "Epoch 1484/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01484: loss did not improve from 1.67561\n",
      "Epoch 1485/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01485: loss did not improve from 1.67561\n",
      "Epoch 1486/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6782 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01486: loss did not improve from 1.67561\n",
      "Epoch 1487/2000\n",
      "93/93 [==============================] - 0s 603us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01487: loss did not improve from 1.67561\n",
      "Epoch 1488/2000\n",
      "93/93 [==============================] - 0s 600us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01488: loss did not improve from 1.67561\n",
      "Epoch 1489/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01489: loss did not improve from 1.67561\n",
      "Epoch 1490/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01490: loss did not improve from 1.67561\n",
      "Epoch 1491/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01491: loss did not improve from 1.67561\n",
      "Epoch 1492/2000\n",
      "93/93 [==============================] - 0s 613us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01492: loss did not improve from 1.67561\n",
      "Epoch 1493/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01493: loss did not improve from 1.67561\n",
      "Epoch 1494/2000\n",
      "93/93 [==============================] - 0s 603us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01494: loss did not improve from 1.67561\n",
      "Epoch 1495/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01495: loss did not improve from 1.67561\n",
      "Epoch 1496/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01496: loss did not improve from 1.67561\n",
      "Epoch 1497/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01497: loss did not improve from 1.67561\n",
      "Epoch 1498/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01498: loss did not improve from 1.67561\n",
      "Epoch 1499/2000\n",
      "93/93 [==============================] - 0s 772us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01499: loss did not improve from 1.67561\n",
      "Epoch 1500/2000\n",
      "93/93 [==============================] - 0s 761us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01500: loss did not improve from 1.67561\n",
      "Epoch 1501/2000\n",
      "93/93 [==============================] - 0s 751us/step - loss: 1.6773 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01501: loss did not improve from 1.67561\n",
      "Epoch 1502/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01502: loss did not improve from 1.67561\n",
      "Epoch 1503/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01503: loss did not improve from 1.67561\n",
      "Epoch 1504/2000\n",
      "93/93 [==============================] - 0s 598us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01504: loss did not improve from 1.67561\n",
      "Epoch 1505/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6773 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01505: loss did not improve from 1.67561\n",
      "Epoch 1506/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01506: loss did not improve from 1.67561\n",
      "Epoch 1507/2000\n",
      "93/93 [==============================] - 0s 603us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01507: loss did not improve from 1.67561\n",
      "Epoch 1508/2000\n",
      "93/93 [==============================] - 0s 600us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01508: loss did not improve from 1.67561\n",
      "Epoch 1509/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01509: loss did not improve from 1.67561\n",
      "Epoch 1510/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6873 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01510: loss did not improve from 1.67561\n",
      "Epoch 1511/2000\n",
      "93/93 [==============================] - 0s 604us/step - loss: 1.6802 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01511: loss did not improve from 1.67561\n",
      "Epoch 1512/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6783 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01512: loss did not improve from 1.67561\n",
      "Epoch 1513/2000\n",
      "93/93 [==============================] - 0s 585us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01513: loss did not improve from 1.67561\n",
      "Epoch 1514/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01514: loss did not improve from 1.67561\n",
      "Epoch 1515/2000\n",
      "93/93 [==============================] - 0s 586us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01515: loss did not improve from 1.67561\n",
      "Epoch 1516/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01516: loss did not improve from 1.67561\n",
      "Epoch 1517/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01517: loss did not improve from 1.67561\n",
      "Epoch 1518/2000\n",
      "93/93 [==============================] - 0s 582us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01518: loss did not improve from 1.67561\n",
      "Epoch 1519/2000\n",
      "93/93 [==============================] - 0s 613us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01519: loss did not improve from 1.67561\n",
      "Epoch 1520/2000\n",
      "93/93 [==============================] - 0s 578us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01520: loss did not improve from 1.67561\n",
      "Epoch 1521/2000\n",
      "93/93 [==============================] - 0s 581us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01521: loss did not improve from 1.67561\n",
      "Epoch 1522/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01522: loss did not improve from 1.67561\n",
      "Epoch 1523/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6774 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01523: loss did not improve from 1.67561\n",
      "Epoch 1524/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01524: loss did not improve from 1.67561\n",
      "Epoch 1525/2000\n",
      "93/93 [==============================] - 0s 579us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01525: loss did not improve from 1.67561\n",
      "Epoch 1526/2000\n",
      "93/93 [==============================] - 0s 595us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01526: loss did not improve from 1.67561\n",
      "Epoch 1527/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01527: loss did not improve from 1.67561\n",
      "Epoch 1528/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01528: loss did not improve from 1.67561\n",
      "Epoch 1529/2000\n",
      "93/93 [==============================] - 0s 603us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01529: loss did not improve from 1.67561\n",
      "Epoch 1530/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01530: loss did not improve from 1.67561\n",
      "Epoch 1531/2000\n",
      "93/93 [==============================] - 0s 581us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01531: loss did not improve from 1.67561\n",
      "Epoch 1532/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01532: loss did not improve from 1.67561\n",
      "Epoch 1533/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01533: loss did not improve from 1.67561\n",
      "Epoch 1534/2000\n",
      "93/93 [==============================] - 0s 587us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01534: loss did not improve from 1.67561\n",
      "Epoch 1535/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01535: loss did not improve from 1.67561\n",
      "Epoch 1536/2000\n",
      "93/93 [==============================] - 0s 585us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01536: loss did not improve from 1.67561\n",
      "Epoch 1537/2000\n",
      "93/93 [==============================] - 0s 603us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01537: loss did not improve from 1.67561\n",
      "Epoch 1538/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01538: loss did not improve from 1.67561\n",
      "Epoch 1539/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01539: loss did not improve from 1.67561\n",
      "Epoch 1540/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01540: loss did not improve from 1.67561\n",
      "Epoch 1541/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6773 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01541: loss did not improve from 1.67561\n",
      "Epoch 1542/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6776 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01542: loss did not improve from 1.67561\n",
      "Epoch 1543/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01543: loss did not improve from 1.67561\n",
      "Epoch 1544/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01544: loss did not improve from 1.67561\n",
      "Epoch 1545/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01545: loss did not improve from 1.67561\n",
      "Epoch 1546/2000\n",
      "93/93 [==============================] - 0s 582us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01546: loss did not improve from 1.67561\n",
      "Epoch 1547/2000\n",
      "93/93 [==============================] - 0s 613us/step - loss: 1.6773 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01547: loss did not improve from 1.67561\n",
      "Epoch 1548/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01548: loss did not improve from 1.67561\n",
      "Epoch 1549/2000\n",
      "93/93 [==============================] - 0s 581us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01549: loss did not improve from 1.67561\n",
      "Epoch 1550/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01550: loss did not improve from 1.67561\n",
      "Epoch 1551/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01551: loss did not improve from 1.67561\n",
      "Epoch 1552/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01552: loss did not improve from 1.67561\n",
      "Epoch 1553/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01553: loss did not improve from 1.67561\n",
      "Epoch 1554/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01554: loss did not improve from 1.67561\n",
      "Epoch 1555/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01555: loss did not improve from 1.67561\n",
      "Epoch 1556/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01556: loss did not improve from 1.67561\n",
      "Epoch 1557/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01557: loss did not improve from 1.67561\n",
      "Epoch 1558/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01558: loss did not improve from 1.67561\n",
      "Epoch 1559/2000\n",
      "93/93 [==============================] - 0s 579us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01559: loss did not improve from 1.67561\n",
      "Epoch 1560/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01560: loss did not improve from 1.67561\n",
      "Epoch 1561/2000\n",
      "93/93 [==============================] - 0s 597us/step - loss: 1.6776 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01561: loss did not improve from 1.67561\n",
      "Epoch 1562/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01562: loss did not improve from 1.67561\n",
      "Epoch 1563/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01563: loss did not improve from 1.67561\n",
      "Epoch 1564/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01564: loss did not improve from 1.67561\n",
      "Epoch 1565/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01565: loss did not improve from 1.67561\n",
      "Epoch 1566/2000\n",
      "93/93 [==============================] - 0s 594us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01566: loss did not improve from 1.67561\n",
      "Epoch 1567/2000\n",
      "93/93 [==============================] - 0s 613us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01567: loss did not improve from 1.67561\n",
      "Epoch 1568/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01568: loss did not improve from 1.67561\n",
      "Epoch 1569/2000\n",
      "93/93 [==============================] - 0s 579us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01569: loss did not improve from 1.67561\n",
      "Epoch 1570/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6775 - accuracy: 0.4946\n",
      "\n",
      "Epoch 01570: loss did not improve from 1.67561\n",
      "Epoch 1571/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01571: loss did not improve from 1.67561\n",
      "Epoch 1572/2000\n",
      "93/93 [==============================] - 0s 606us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01572: loss did not improve from 1.67561\n",
      "Epoch 1573/2000\n",
      "93/93 [==============================] - 0s 578us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01573: loss did not improve from 1.67561\n",
      "Epoch 1574/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01574: loss did not improve from 1.67561\n",
      "Epoch 1575/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01575: loss did not improve from 1.67561\n",
      "Epoch 1576/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01576: loss did not improve from 1.67561\n",
      "Epoch 1577/2000\n",
      "93/93 [==============================] - 0s 613us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01577: loss did not improve from 1.67561\n",
      "Epoch 1578/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01578: loss did not improve from 1.67561\n",
      "Epoch 1579/2000\n",
      "93/93 [==============================] - 0s 581us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01579: loss did not improve from 1.67561\n",
      "Epoch 1580/2000\n",
      "93/93 [==============================] - 0s 613us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01580: loss did not improve from 1.67561\n",
      "Epoch 1581/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01581: loss did not improve from 1.67561\n",
      "Epoch 1582/2000\n",
      "93/93 [==============================] - 0s 612us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01582: loss did not improve from 1.67561\n",
      "Epoch 1583/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01583: loss did not improve from 1.67561\n",
      "Epoch 1584/2000\n",
      "93/93 [==============================] - 0s 581us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01584: loss did not improve from 1.67561\n",
      "Epoch 1585/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01585: loss did not improve from 1.67561\n",
      "Epoch 1586/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01586: loss did not improve from 1.67561\n",
      "Epoch 1587/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01587: loss did not improve from 1.67561\n",
      "Epoch 1588/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6774 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01588: loss did not improve from 1.67561\n",
      "Epoch 1589/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01589: loss did not improve from 1.67561\n",
      "Epoch 1590/2000\n",
      "93/93 [==============================] - 0s 613us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01590: loss did not improve from 1.67561\n",
      "Epoch 1591/2000\n",
      "93/93 [==============================] - 0s 577us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01591: loss did not improve from 1.67561\n",
      "Epoch 1592/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01592: loss did not improve from 1.67561\n",
      "Epoch 1593/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01593: loss did not improve from 1.67561\n",
      "Epoch 1594/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01594: loss did not improve from 1.67561\n",
      "Epoch 1595/2000\n",
      "93/93 [==============================] - 0s 614us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01595: loss did not improve from 1.67561\n",
      "Epoch 1596/2000\n",
      "93/93 [==============================] - 0s 610us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01596: loss did not improve from 1.67561\n",
      "Epoch 1597/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01597: loss did not improve from 1.67561\n",
      "Epoch 1598/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01598: loss did not improve from 1.67561\n",
      "Epoch 1599/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01599: loss did not improve from 1.67561\n",
      "Epoch 1600/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6811 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01600: loss did not improve from 1.67561\n",
      "Epoch 1601/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6865 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01601: loss did not improve from 1.67561\n",
      "Epoch 1602/2000\n",
      "93/93 [==============================] - 0s 740us/step - loss: 1.6785 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01602: loss did not improve from 1.67561\n",
      "Epoch 1603/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 2.3200 - accuracy: 0.4409\n",
      "\n",
      "Epoch 01603: loss did not improve from 1.67561\n",
      "Epoch 1604/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.8279 - accuracy: 0.4301\n",
      "\n",
      "Epoch 01604: loss did not improve from 1.67561\n",
      "Epoch 1605/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6783 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01605: loss did not improve from 1.67561\n",
      "Epoch 1606/2000\n",
      "93/93 [==============================] - 0s 610us/step - loss: 1.6779 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01606: loss did not improve from 1.67561\n",
      "Epoch 1607/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6779 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01607: loss did not improve from 1.67561\n",
      "Epoch 1608/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01608: loss did not improve from 1.67561\n",
      "Epoch 1609/2000\n",
      "93/93 [==============================] - 0s 613us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01609: loss did not improve from 1.67561\n",
      "Epoch 1610/2000\n",
      "93/93 [==============================] - 0s 578us/step - loss: 1.6778 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01610: loss did not improve from 1.67561\n",
      "Epoch 1611/2000\n",
      "93/93 [==============================] - 0s 581us/step - loss: 1.6779 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01611: loss did not improve from 1.67561\n",
      "Epoch 1612/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6776 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01612: loss did not improve from 1.67561\n",
      "Epoch 1613/2000\n",
      "93/93 [==============================] - 0s 578us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01613: loss did not improve from 1.67561\n",
      "Epoch 1614/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01614: loss did not improve from 1.67561\n",
      "Epoch 1615/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01615: loss did not improve from 1.67561\n",
      "Epoch 1616/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6776 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01616: loss did not improve from 1.67561\n",
      "Epoch 1617/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01617: loss did not improve from 1.67561\n",
      "Epoch 1618/2000\n",
      "93/93 [==============================] - 0s 604us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01618: loss did not improve from 1.67561\n",
      "Epoch 1619/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01619: loss did not improve from 1.67561\n",
      "Epoch 1620/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01620: loss did not improve from 1.67561\n",
      "Epoch 1621/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01621: loss did not improve from 1.67561\n",
      "Epoch 1622/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01622: loss did not improve from 1.67561\n",
      "Epoch 1623/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01623: loss did not improve from 1.67561\n",
      "Epoch 1624/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01624: loss did not improve from 1.67561\n",
      "Epoch 1625/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01625: loss did not improve from 1.67561\n",
      "Epoch 1626/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01626: loss did not improve from 1.67561\n",
      "Epoch 1627/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6773 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01627: loss did not improve from 1.67561\n",
      "Epoch 1628/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01628: loss did not improve from 1.67561\n",
      "Epoch 1629/2000\n",
      "93/93 [==============================] - 0s 593us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01629: loss did not improve from 1.67561\n",
      "Epoch 1630/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01630: loss did not improve from 1.67561\n",
      "Epoch 1631/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01631: loss did not improve from 1.67561\n",
      "Epoch 1632/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01632: loss did not improve from 1.67561\n",
      "Epoch 1633/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01633: loss did not improve from 1.67561\n",
      "Epoch 1634/2000\n",
      "93/93 [==============================] - 0s 655us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01634: loss did not improve from 1.67561\n",
      "Epoch 1635/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01635: loss did not improve from 1.67561\n",
      "Epoch 1636/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6773 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01636: loss did not improve from 1.67561\n",
      "Epoch 1637/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6773 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01637: loss did not improve from 1.67561\n",
      "Epoch 1638/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01638: loss did not improve from 1.67561\n",
      "Epoch 1639/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01639: loss did not improve from 1.67561\n",
      "Epoch 1640/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01640: loss did not improve from 1.67561\n",
      "Epoch 1641/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01641: loss did not improve from 1.67561\n",
      "Epoch 1642/2000\n",
      "93/93 [==============================] - 0s 593us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01642: loss did not improve from 1.67561\n",
      "Epoch 1643/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01643: loss did not improve from 1.67561\n",
      "Epoch 1644/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01644: loss did not improve from 1.67561\n",
      "Epoch 1645/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01645: loss did not improve from 1.67561\n",
      "Epoch 1646/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01646: loss did not improve from 1.67561\n",
      "Epoch 1647/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01647: loss did not improve from 1.67561\n",
      "Epoch 1648/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01648: loss did not improve from 1.67561\n",
      "Epoch 1649/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6772 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01649: loss did not improve from 1.67561\n",
      "Epoch 1650/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01650: loss did not improve from 1.67561\n",
      "Epoch 1651/2000\n",
      "93/93 [==============================] - 0s 761us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01651: loss did not improve from 1.67561\n",
      "Epoch 1652/2000\n",
      "93/93 [==============================] - 0s 761us/step - loss: 1.6773 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01652: loss did not improve from 1.67561\n",
      "Epoch 1653/2000\n",
      "93/93 [==============================] - 0s 751us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01653: loss did not improve from 1.67561\n",
      "Epoch 1654/2000\n",
      "93/93 [==============================] - 0s 642us/step - loss: 1.6773 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01654: loss did not improve from 1.67561\n",
      "Epoch 1655/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01655: loss did not improve from 1.67561\n",
      "Epoch 1656/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01656: loss did not improve from 1.67561\n",
      "Epoch 1657/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6774 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01657: loss did not improve from 1.67561\n",
      "Epoch 1658/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6781 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01658: loss did not improve from 1.67561\n",
      "Epoch 1659/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 0s 633us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01659: loss did not improve from 1.67561\n",
      "Epoch 1660/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01660: loss did not improve from 1.67561\n",
      "Epoch 1661/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01661: loss did not improve from 1.67561\n",
      "Epoch 1662/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01662: loss did not improve from 1.67561\n",
      "Epoch 1663/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01663: loss did not improve from 1.67561\n",
      "Epoch 1664/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01664: loss did not improve from 1.67561\n",
      "Epoch 1665/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01665: loss did not improve from 1.67561\n",
      "Epoch 1666/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01666: loss did not improve from 1.67561\n",
      "Epoch 1667/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01667: loss did not improve from 1.67561\n",
      "Epoch 1668/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01668: loss did not improve from 1.67561\n",
      "Epoch 1669/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01669: loss did not improve from 1.67561\n",
      "Epoch 1670/2000\n",
      "93/93 [==============================] - 0s 729us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01670: loss did not improve from 1.67561\n",
      "Epoch 1671/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01671: loss did not improve from 1.67561\n",
      "Epoch 1672/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01672: loss did not improve from 1.67561\n",
      "Epoch 1673/2000\n",
      "93/93 [==============================] - 0s 605us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01673: loss did not improve from 1.67561\n",
      "Epoch 1674/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6773 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01674: loss did not improve from 1.67561\n",
      "Epoch 1675/2000\n",
      "93/93 [==============================] - 0s 612us/step - loss: 1.6774 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01675: loss did not improve from 1.67561\n",
      "Epoch 1676/2000\n",
      "93/93 [==============================] - 0s 689us/step - loss: 1.6773 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01676: loss did not improve from 1.67561\n",
      "Epoch 1677/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01677: loss did not improve from 1.67561\n",
      "Epoch 1678/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01678: loss did not improve from 1.67561\n",
      "Epoch 1679/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01679: loss did not improve from 1.67561\n",
      "Epoch 1680/2000\n",
      "93/93 [==============================] - 0s 579us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01680: loss did not improve from 1.67561\n",
      "Epoch 1681/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01681: loss did not improve from 1.67561\n",
      "Epoch 1682/2000\n",
      "93/93 [==============================] - 0s 579us/step - loss: 1.6772 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01682: loss did not improve from 1.67561\n",
      "Epoch 1683/2000\n",
      "93/93 [==============================] - 0s 614us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01683: loss did not improve from 1.67561\n",
      "Epoch 1684/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01684: loss did not improve from 1.67561\n",
      "Epoch 1685/2000\n",
      "93/93 [==============================] - 0s 586us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01685: loss did not improve from 1.67561\n",
      "Epoch 1686/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6794 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01686: loss did not improve from 1.67561\n",
      "Epoch 1687/2000\n",
      "93/93 [==============================] - 0s 586us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01687: loss did not improve from 1.67561\n",
      "Epoch 1688/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01688: loss did not improve from 1.67561\n",
      "Epoch 1689/2000\n",
      "93/93 [==============================] - 0s 602us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01689: loss did not improve from 1.67561\n",
      "Epoch 1690/2000\n",
      "93/93 [==============================] - 0s 594us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01690: loss did not improve from 1.67561\n",
      "Epoch 1691/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01691: loss did not improve from 1.67561\n",
      "Epoch 1692/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01692: loss did not improve from 1.67561\n",
      "Epoch 1693/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01693: loss did not improve from 1.67561\n",
      "Epoch 1694/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01694: loss did not improve from 1.67561\n",
      "Epoch 1695/2000\n",
      "93/93 [==============================] - 0s 579us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01695: loss did not improve from 1.67561\n",
      "Epoch 1696/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01696: loss did not improve from 1.67561\n",
      "Epoch 1697/2000\n",
      "93/93 [==============================] - 0s 596us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01697: loss did not improve from 1.67561\n",
      "Epoch 1698/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01698: loss did not improve from 1.67561\n",
      "Epoch 1699/2000\n",
      "93/93 [==============================] - 0s 588us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01699: loss did not improve from 1.67561\n",
      "Epoch 1700/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01700: loss did not improve from 1.67561\n",
      "Epoch 1701/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6773 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01701: loss did not improve from 1.67561\n",
      "Epoch 1702/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01702: loss did not improve from 1.67561\n",
      "Epoch 1703/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01703: loss did not improve from 1.67561\n",
      "Epoch 1704/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6778 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01704: loss did not improve from 1.67561\n",
      "Epoch 1705/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01705: loss did not improve from 1.67561\n",
      "Epoch 1706/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6775 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01706: loss did not improve from 1.67561\n",
      "Epoch 1707/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01707: loss did not improve from 1.67561\n",
      "Epoch 1708/2000\n",
      "93/93 [==============================] - 0s 603us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01708: loss did not improve from 1.67561\n",
      "Epoch 1709/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01709: loss did not improve from 1.67561\n",
      "Epoch 1710/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01710: loss did not improve from 1.67561\n",
      "Epoch 1711/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01711: loss did not improve from 1.67561\n",
      "Epoch 1712/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01712: loss did not improve from 1.67561\n",
      "Epoch 1713/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01713: loss did not improve from 1.67561\n",
      "Epoch 1714/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01714: loss did not improve from 1.67561\n",
      "Epoch 1715/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01715: loss did not improve from 1.67561\n",
      "Epoch 1716/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01716: loss did not improve from 1.67561\n",
      "Epoch 1717/2000\n",
      "93/93 [==============================] - 0s 598us/step - loss: 1.6773 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01717: loss did not improve from 1.67561\n",
      "Epoch 1718/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01718: loss did not improve from 1.67561\n",
      "Epoch 1719/2000\n",
      "93/93 [==============================] - 0s 576us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01719: loss did not improve from 1.67561\n",
      "Epoch 1720/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01720: loss did not improve from 1.67561\n",
      "Epoch 1721/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01721: loss did not improve from 1.67561\n",
      "Epoch 1722/2000\n",
      "93/93 [==============================] - 0s 604us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01722: loss did not improve from 1.67561\n",
      "Epoch 1723/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01723: loss did not improve from 1.67561\n",
      "Epoch 1724/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01724: loss did not improve from 1.67561\n",
      "Epoch 1725/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01725: loss did not improve from 1.67561\n",
      "Epoch 1726/2000\n",
      "93/93 [==============================] - 0s 579us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01726: loss did not improve from 1.67561\n",
      "Epoch 1727/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01727: loss did not improve from 1.67561\n",
      "Epoch 1728/2000\n",
      "93/93 [==============================] - 0s 582us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01728: loss did not improve from 1.67561\n",
      "Epoch 1729/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01729: loss did not improve from 1.67561\n",
      "Epoch 1730/2000\n",
      "93/93 [==============================] - 0s 603us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01730: loss did not improve from 1.67561\n",
      "Epoch 1731/2000\n",
      "93/93 [==============================] - 0s 578us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01731: loss did not improve from 1.67561\n",
      "Epoch 1732/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6773 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01732: loss did not improve from 1.67561\n",
      "Epoch 1733/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01733: loss did not improve from 1.67561\n",
      "Epoch 1734/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01734: loss did not improve from 1.67561\n",
      "Epoch 1735/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01735: loss did not improve from 1.67561\n",
      "Epoch 1736/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6779 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01736: loss did not improve from 1.67561\n",
      "Epoch 1737/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01737: loss did not improve from 1.67561\n",
      "Epoch 1738/2000\n",
      "93/93 [==============================] - 0s 604us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01738: loss did not improve from 1.67561\n",
      "Epoch 1739/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01739: loss did not improve from 1.67561\n",
      "Epoch 1740/2000\n",
      "93/93 [==============================] - 0s 600us/step - loss: 1.6797 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01740: loss did not improve from 1.67561\n",
      "Epoch 1741/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6780 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01741: loss did not improve from 1.67561\n",
      "Epoch 1742/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01742: loss did not improve from 1.67561\n",
      "Epoch 1743/2000\n",
      "93/93 [==============================] - 0s 619us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01743: loss did not improve from 1.67561\n",
      "Epoch 1744/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01744: loss did not improve from 1.67561\n",
      "Epoch 1745/2000\n",
      "93/93 [==============================] - 0s 789us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01745: loss did not improve from 1.67561\n",
      "Epoch 1746/2000\n",
      "93/93 [==============================] - 0s 760us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01746: loss did not improve from 1.67561\n",
      "Epoch 1747/2000\n",
      "93/93 [==============================] - 0s 730us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01747: loss did not improve from 1.67561\n",
      "Epoch 1748/2000\n",
      "93/93 [==============================] - 0s 799us/step - loss: 1.6776 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01748: loss did not improve from 1.67561\n",
      "Epoch 1749/2000\n",
      "93/93 [==============================] - 0s 750us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01749: loss did not improve from 1.67561\n",
      "Epoch 1750/2000\n",
      "93/93 [==============================] - 0s 783us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01750: loss did not improve from 1.67561\n",
      "Epoch 1751/2000\n",
      "93/93 [==============================] - 0s 767us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01751: loss did not improve from 1.67561\n",
      "Epoch 1752/2000\n",
      "93/93 [==============================] - 0s 772us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01752: loss did not improve from 1.67561\n",
      "Epoch 1753/2000\n",
      "93/93 [==============================] - 0s 836us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01753: loss did not improve from 1.67561\n",
      "Epoch 1754/2000\n",
      "93/93 [==============================] - 0s 686us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01754: loss did not improve from 1.67561\n",
      "Epoch 1755/2000\n",
      "93/93 [==============================] - 0s 594us/step - loss: 1.6772 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01755: loss did not improve from 1.67561\n",
      "Epoch 1756/2000\n",
      "93/93 [==============================] - 0s 716us/step - loss: 1.6771 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01756: loss did not improve from 1.67561\n",
      "Epoch 1757/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01757: loss did not improve from 1.67561\n",
      "Epoch 1758/2000\n",
      "93/93 [==============================] - 0s 656us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01758: loss did not improve from 1.67561\n",
      "Epoch 1759/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01759: loss did not improve from 1.67561\n",
      "Epoch 1760/2000\n",
      "93/93 [==============================] - 0s 606us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01760: loss did not improve from 1.67561\n",
      "Epoch 1761/2000\n",
      "93/93 [==============================] - 0s 616us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01761: loss did not improve from 1.67561\n",
      "Epoch 1762/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6773 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01762: loss did not improve from 1.67561\n",
      "Epoch 1763/2000\n",
      "93/93 [==============================] - 0s 610us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01763: loss did not improve from 1.67561\n",
      "Epoch 1764/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01764: loss did not improve from 1.67561\n",
      "Epoch 1765/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 0s 664us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01765: loss did not improve from 1.67561\n",
      "Epoch 1766/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01766: loss did not improve from 1.67561\n",
      "Epoch 1767/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6773 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01767: loss did not improve from 1.67561\n",
      "Epoch 1768/2000\n",
      "93/93 [==============================] - 0s 664us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01768: loss did not improve from 1.67561\n",
      "Epoch 1769/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6772 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01769: loss did not improve from 1.67561\n",
      "Epoch 1770/2000\n",
      "93/93 [==============================] - 0s 697us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01770: loss did not improve from 1.67561\n",
      "Epoch 1771/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01771: loss did not improve from 1.67561\n",
      "Epoch 1772/2000\n",
      "93/93 [==============================] - 0s 602us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01772: loss did not improve from 1.67561\n",
      "Epoch 1773/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01773: loss did not improve from 1.67561\n",
      "Epoch 1774/2000\n",
      "93/93 [==============================] - 0s 583us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01774: loss did not improve from 1.67561\n",
      "Epoch 1775/2000\n",
      "93/93 [==============================] - 0s 608us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01775: loss did not improve from 1.67561\n",
      "Epoch 1776/2000\n",
      "93/93 [==============================] - 0s 578us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01776: loss did not improve from 1.67561\n",
      "Epoch 1777/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01777: loss did not improve from 1.67561\n",
      "Epoch 1778/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01778: loss did not improve from 1.67561\n",
      "Epoch 1779/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6772 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01779: loss did not improve from 1.67561\n",
      "Epoch 1780/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01780: loss did not improve from 1.67561\n",
      "Epoch 1781/2000\n",
      "93/93 [==============================] - 0s 613us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01781: loss did not improve from 1.67561\n",
      "Epoch 1782/2000\n",
      "93/93 [==============================] - 0s 613us/step - loss: 1.6773 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01782: loss did not improve from 1.67561\n",
      "Epoch 1783/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6772 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01783: loss did not improve from 1.67561\n",
      "Epoch 1784/2000\n",
      "93/93 [==============================] - 0s 607us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01784: loss did not improve from 1.67561\n",
      "Epoch 1785/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01785: loss did not improve from 1.67561\n",
      "Epoch 1786/2000\n",
      "93/93 [==============================] - 0s 586us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01786: loss did not improve from 1.67561\n",
      "Epoch 1787/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01787: loss did not improve from 1.67561\n",
      "Epoch 1788/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01788: loss did not improve from 1.67561\n",
      "Epoch 1789/2000\n",
      "93/93 [==============================] - 0s 610us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01789: loss did not improve from 1.67561\n",
      "Epoch 1790/2000\n",
      "93/93 [==============================] - 0s 624us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01790: loss did not improve from 1.67561\n",
      "Epoch 1791/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01791: loss did not improve from 1.67561\n",
      "Epoch 1792/2000\n",
      "93/93 [==============================] - 0s 634us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01792: loss did not improve from 1.67561\n",
      "Epoch 1793/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6773 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01793: loss did not improve from 1.67561\n",
      "Epoch 1794/2000\n",
      "93/93 [==============================] - 0s 613us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01794: loss did not improve from 1.67561\n",
      "Epoch 1795/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01795: loss did not improve from 1.67561\n",
      "Epoch 1796/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01796: loss did not improve from 1.67561\n",
      "Epoch 1797/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01797: loss did not improve from 1.67561\n",
      "Epoch 1798/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01798: loss did not improve from 1.67561\n",
      "Epoch 1799/2000\n",
      "93/93 [==============================] - 0s 655us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01799: loss did not improve from 1.67561\n",
      "Epoch 1800/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01800: loss did not improve from 1.67561\n",
      "Epoch 1801/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01801: loss did not improve from 1.67561\n",
      "Epoch 1802/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6774 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01802: loss did not improve from 1.67561\n",
      "Epoch 1803/2000\n",
      "93/93 [==============================] - 0s 654us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01803: loss did not improve from 1.67561\n",
      "Epoch 1804/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01804: loss did not improve from 1.67561\n",
      "Epoch 1805/2000\n",
      "93/93 [==============================] - 0s 631us/step - loss: 1.6771 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01805: loss did not improve from 1.67561\n",
      "Epoch 1806/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6783 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01806: loss did not improve from 1.67561\n",
      "Epoch 1807/2000\n",
      "93/93 [==============================] - 0s 636us/step - loss: 1.6777 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01807: loss did not improve from 1.67561\n",
      "Epoch 1808/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01808: loss did not improve from 1.67561\n",
      "Epoch 1809/2000\n",
      "93/93 [==============================] - 0s 666us/step - loss: 1.6772 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01809: loss did not improve from 1.67561\n",
      "Epoch 1810/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01810: loss did not improve from 1.67561\n",
      "Epoch 1811/2000\n",
      "93/93 [==============================] - 0s 697us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01811: loss did not improve from 1.67561\n",
      "Epoch 1812/2000\n",
      "93/93 [==============================] - 0s 665us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01812: loss did not improve from 1.67561\n",
      "Epoch 1813/2000\n",
      "93/93 [==============================] - 0s 631us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01813: loss did not improve from 1.67561\n",
      "Epoch 1814/2000\n",
      "93/93 [==============================] - 0s 645us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01814: loss did not improve from 1.67561\n",
      "Epoch 1815/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01815: loss did not improve from 1.67561\n",
      "Epoch 1816/2000\n",
      "93/93 [==============================] - 0s 675us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01816: loss did not improve from 1.67561\n",
      "Epoch 1817/2000\n",
      "93/93 [==============================] - 0s 623us/step - loss: 1.6781 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01817: loss did not improve from 1.67561\n",
      "Epoch 1818/2000\n",
      "93/93 [==============================] - 0s 643us/step - loss: 1.6779 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01818: loss did not improve from 1.67561\n",
      "Epoch 1819/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6778 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01819: loss did not improve from 1.67561\n",
      "Epoch 1820/2000\n",
      "93/93 [==============================] - 0s 588us/step - loss: 1.6775 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01820: loss did not improve from 1.67561\n",
      "Epoch 1821/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01821: loss did not improve from 1.67561\n",
      "Epoch 1822/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01822: loss did not improve from 1.67561\n",
      "Epoch 1823/2000\n",
      "93/93 [==============================] - 0s 584us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01823: loss did not improve from 1.67561\n",
      "Epoch 1824/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01824: loss did not improve from 1.67561\n",
      "Epoch 1825/2000\n",
      "93/93 [==============================] - 0s 602us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01825: loss did not improve from 1.67561\n",
      "Epoch 1826/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01826: loss did not improve from 1.67561\n",
      "Epoch 1827/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01827: loss did not improve from 1.67561\n",
      "Epoch 1828/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6773 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01828: loss did not improve from 1.67561\n",
      "Epoch 1829/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6772 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01829: loss did not improve from 1.67561\n",
      "Epoch 1830/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01830: loss did not improve from 1.67561\n",
      "Epoch 1831/2000\n",
      "93/93 [==============================] - 0s 584us/step - loss: 1.6772 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01831: loss did not improve from 1.67561\n",
      "Epoch 1832/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01832: loss did not improve from 1.67561\n",
      "Epoch 1833/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01833: loss did not improve from 1.67561\n",
      "Epoch 1834/2000\n",
      "93/93 [==============================] - 0s 624us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01834: loss did not improve from 1.67561\n",
      "Epoch 1835/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01835: loss did not improve from 1.67561\n",
      "Epoch 1836/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01836: loss did not improve from 1.67561\n",
      "Epoch 1837/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01837: loss did not improve from 1.67561\n",
      "Epoch 1838/2000\n",
      "93/93 [==============================] - 0s 581us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01838: loss did not improve from 1.67561\n",
      "Epoch 1839/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6773 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01839: loss did not improve from 1.67561\n",
      "Epoch 1840/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01840: loss did not improve from 1.67561\n",
      "Epoch 1841/2000\n",
      "93/93 [==============================] - 0s 603us/step - loss: 1.6773 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01841: loss did not improve from 1.67561\n",
      "Epoch 1842/2000\n",
      "93/93 [==============================] - 0s 578us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01842: loss did not improve from 1.67561\n",
      "Epoch 1843/2000\n",
      "93/93 [==============================] - 0s 581us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01843: loss did not improve from 1.67561\n",
      "Epoch 1844/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01844: loss did not improve from 1.67561\n",
      "Epoch 1845/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01845: loss did not improve from 1.67561\n",
      "Epoch 1846/2000\n",
      "93/93 [==============================] - 0s 613us/step - loss: 1.6772 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01846: loss did not improve from 1.67561\n",
      "Epoch 1847/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6771 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01847: loss did not improve from 1.67561\n",
      "Epoch 1848/2000\n",
      "93/93 [==============================] - 0s 581us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01848: loss did not improve from 1.67561\n",
      "Epoch 1849/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6772 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01849: loss did not improve from 1.67561\n",
      "Epoch 1850/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6772 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01850: loss did not improve from 1.67561\n",
      "Epoch 1851/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6773 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01851: loss did not improve from 1.67561\n",
      "Epoch 1852/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6773 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01852: loss did not improve from 1.67561\n",
      "Epoch 1853/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6772 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01853: loss did not improve from 1.67561\n",
      "Epoch 1854/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6772 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01854: loss did not improve from 1.67561\n",
      "Epoch 1855/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01855: loss did not improve from 1.67561\n",
      "Epoch 1856/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6780 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01856: loss did not improve from 1.67561\n",
      "Epoch 1857/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01857: loss did not improve from 1.67561\n",
      "Epoch 1858/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01858: loss did not improve from 1.67561\n",
      "Epoch 1859/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6772 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01859: loss did not improve from 1.67561\n",
      "Epoch 1860/2000\n",
      "93/93 [==============================] - 0s 585us/step - loss: 1.6772 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01860: loss did not improve from 1.67561\n",
      "Epoch 1861/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6772 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01861: loss did not improve from 1.67561\n",
      "Epoch 1862/2000\n",
      "93/93 [==============================] - 0s 602us/step - loss: 1.6772 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01862: loss did not improve from 1.67561\n",
      "Epoch 1863/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6772 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01863: loss did not improve from 1.67561\n",
      "Epoch 1864/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01864: loss did not improve from 1.67561\n",
      "Epoch 1865/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6772 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01865: loss did not improve from 1.67561\n",
      "Epoch 1866/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01866: loss did not improve from 1.67561\n",
      "Epoch 1867/2000\n",
      "93/93 [==============================] - 0s 583us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01867: loss did not improve from 1.67561\n",
      "Epoch 1868/2000\n",
      "93/93 [==============================] - 0s 613us/step - loss: 1.6772 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01868: loss did not improve from 1.67561\n",
      "Epoch 1869/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6772 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01869: loss did not improve from 1.67561\n",
      "Epoch 1870/2000\n",
      "93/93 [==============================] - 0s 594us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01870: loss did not improve from 1.67561\n",
      "Epoch 1871/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 0s 622us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01871: loss did not improve from 1.67561\n",
      "Epoch 1872/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01872: loss did not improve from 1.67561\n",
      "Epoch 1873/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01873: loss did not improve from 1.67561\n",
      "Epoch 1874/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01874: loss did not improve from 1.67561\n",
      "Epoch 1875/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01875: loss did not improve from 1.67561\n",
      "Epoch 1876/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01876: loss did not improve from 1.67561\n",
      "Epoch 1877/2000\n",
      "93/93 [==============================] - 0s 586us/step - loss: 1.6773 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01877: loss did not improve from 1.67561\n",
      "Epoch 1878/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6772 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01878: loss did not improve from 1.67561\n",
      "Epoch 1879/2000\n",
      "93/93 [==============================] - 0s 586us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01879: loss did not improve from 1.67561\n",
      "Epoch 1880/2000\n",
      "93/93 [==============================] - 0s 603us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01880: loss did not improve from 1.67561\n",
      "Epoch 1881/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01881: loss did not improve from 1.67561\n",
      "Epoch 1882/2000\n",
      "93/93 [==============================] - 0s 581us/step - loss: 1.6772 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01882: loss did not improve from 1.67561\n",
      "Epoch 1883/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01883: loss did not improve from 1.67561\n",
      "Epoch 1884/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01884: loss did not improve from 1.67561\n",
      "Epoch 1885/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01885: loss did not improve from 1.67561\n",
      "Epoch 1886/2000\n",
      "93/93 [==============================] - 0s 679us/step - loss: 1.6771 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01886: loss did not improve from 1.67561\n",
      "Epoch 1887/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6807 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01887: loss did not improve from 1.67561\n",
      "Epoch 1888/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6776 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01888: loss did not improve from 1.67561\n",
      "Epoch 1889/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01889: loss did not improve from 1.67561\n",
      "Epoch 1890/2000\n",
      "93/93 [==============================] - 0s 600us/step - loss: 1.6773 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01890: loss did not improve from 1.67561\n",
      "Epoch 1891/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6772 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01891: loss did not improve from 1.67561\n",
      "Epoch 1892/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6772 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01892: loss did not improve from 1.67561\n",
      "Epoch 1893/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01893: loss did not improve from 1.67561\n",
      "Epoch 1894/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01894: loss did not improve from 1.67561\n",
      "Epoch 1895/2000\n",
      "93/93 [==============================] - 0s 582us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01895: loss did not improve from 1.67561\n",
      "Epoch 1896/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6772 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01896: loss did not improve from 1.67561\n",
      "Epoch 1897/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01897: loss did not improve from 1.67561\n",
      "Epoch 1898/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6772 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01898: loss did not improve from 1.67561\n",
      "Epoch 1899/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01899: loss did not improve from 1.67561\n",
      "Epoch 1900/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6772 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01900: loss did not improve from 1.67561\n",
      "Epoch 1901/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6771 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01901: loss did not improve from 1.67561\n",
      "Epoch 1902/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01902: loss did not improve from 1.67561\n",
      "Epoch 1903/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6773 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01903: loss did not improve from 1.67561\n",
      "Epoch 1904/2000\n",
      "93/93 [==============================] - 0s 586us/step - loss: 1.6772 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01904: loss did not improve from 1.67561\n",
      "Epoch 1905/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6772 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01905: loss did not improve from 1.67561\n",
      "Epoch 1906/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01906: loss did not improve from 1.67561\n",
      "Epoch 1907/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01907: loss did not improve from 1.67561\n",
      "Epoch 1908/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6772 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01908: loss did not improve from 1.67561\n",
      "Epoch 1909/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6771 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01909: loss did not improve from 1.67561\n",
      "Epoch 1910/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01910: loss did not improve from 1.67561\n",
      "Epoch 1911/2000\n",
      "93/93 [==============================] - 0s 579us/step - loss: 1.6772 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01911: loss did not improve from 1.67561\n",
      "Epoch 1912/2000\n",
      "93/93 [==============================] - 0s 581us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01912: loss did not improve from 1.67561\n",
      "Epoch 1913/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01913: loss did not improve from 1.67561\n",
      "Epoch 1914/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6771 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01914: loss did not improve from 1.67561\n",
      "Epoch 1915/2000\n",
      "93/93 [==============================] - 0s 596us/step - loss: 1.6771 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01915: loss did not improve from 1.67561\n",
      "Epoch 1916/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01916: loss did not improve from 1.67561\n",
      "Epoch 1917/2000\n",
      "93/93 [==============================] - 0s 576us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01917: loss did not improve from 1.67561\n",
      "Epoch 1918/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01918: loss did not improve from 1.67561\n",
      "Epoch 1919/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01919: loss did not improve from 1.67561\n",
      "Epoch 1920/2000\n",
      "93/93 [==============================] - 0s 583us/step - loss: 1.6772 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01920: loss did not improve from 1.67561\n",
      "Epoch 1921/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01921: loss did not improve from 1.67561\n",
      "Epoch 1922/2000\n",
      "93/93 [==============================] - 0s 581us/step - loss: 1.6772 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01922: loss did not improve from 1.67561\n",
      "Epoch 1923/2000\n",
      "93/93 [==============================] - 0s 583us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01923: loss did not improve from 1.67561\n",
      "Epoch 1924/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01924: loss did not improve from 1.67561\n",
      "Epoch 1925/2000\n",
      "93/93 [==============================] - 0s 578us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01925: loss did not improve from 1.67561\n",
      "Epoch 1926/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6772 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01926: loss did not improve from 1.67561\n",
      "Epoch 1927/2000\n",
      "93/93 [==============================] - 0s 602us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01927: loss did not improve from 1.67561\n",
      "Epoch 1928/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6771 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01928: loss did not improve from 1.67561\n",
      "Epoch 1929/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6772 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01929: loss did not improve from 1.67561\n",
      "Epoch 1930/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01930: loss did not improve from 1.67561\n",
      "Epoch 1931/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01931: loss did not improve from 1.67561\n",
      "Epoch 1932/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6772 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01932: loss did not improve from 1.67561\n",
      "Epoch 1933/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6770 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01933: loss did not improve from 1.67561\n",
      "Epoch 1934/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6772 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01934: loss did not improve from 1.67561\n",
      "Epoch 1935/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01935: loss did not improve from 1.67561\n",
      "Epoch 1936/2000\n",
      "93/93 [==============================] - 0s 613us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01936: loss did not improve from 1.67561\n",
      "Epoch 1937/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6772 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01937: loss did not improve from 1.67561\n",
      "Epoch 1938/2000\n",
      "93/93 [==============================] - 0s 581us/step - loss: 1.6773 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01938: loss did not improve from 1.67561\n",
      "Epoch 1939/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6772 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01939: loss did not improve from 1.67561\n",
      "Epoch 1940/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6772 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01940: loss did not improve from 1.67561\n",
      "Epoch 1941/2000\n",
      "93/93 [==============================] - 0s 575us/step - loss: 1.6772 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01941: loss did not improve from 1.67561\n",
      "Epoch 1942/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6772 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01942: loss did not improve from 1.67561\n",
      "Epoch 1943/2000\n",
      "93/93 [==============================] - 0s 586us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01943: loss did not improve from 1.67561\n",
      "Epoch 1944/2000\n",
      "93/93 [==============================] - 0s 603us/step - loss: 1.6772 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01944: loss did not improve from 1.67561\n",
      "Epoch 1945/2000\n",
      "93/93 [==============================] - 0s 588us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01945: loss did not improve from 1.67561\n",
      "Epoch 1946/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01946: loss did not improve from 1.67561\n",
      "Epoch 1947/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01947: loss did not improve from 1.67561\n",
      "Epoch 1948/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01948: loss did not improve from 1.67561\n",
      "Epoch 1949/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01949: loss did not improve from 1.67561\n",
      "Epoch 1950/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6773 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01950: loss did not improve from 1.67561\n",
      "Epoch 1951/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01951: loss did not improve from 1.67561\n",
      "Epoch 1952/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01952: loss did not improve from 1.67561\n",
      "Epoch 1953/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6772 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01953: loss did not improve from 1.67561\n",
      "Epoch 1954/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6772 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01954: loss did not improve from 1.67561\n",
      "Epoch 1955/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01955: loss did not improve from 1.67561\n",
      "Epoch 1956/2000\n",
      "93/93 [==============================] - 0s 615us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01956: loss did not improve from 1.67561\n",
      "Epoch 1957/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01957: loss did not improve from 1.67561\n",
      "Epoch 1958/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01958: loss did not improve from 1.67561\n",
      "Epoch 1959/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01959: loss did not improve from 1.67561\n",
      "Epoch 1960/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01960: loss did not improve from 1.67561\n",
      "Epoch 1961/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01961: loss did not improve from 1.67561\n",
      "Epoch 1962/2000\n",
      "93/93 [==============================] - 0s 580us/step - loss: 1.6772 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01962: loss did not improve from 1.67561\n",
      "Epoch 1963/2000\n",
      "93/93 [==============================] - 0s 590us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01963: loss did not improve from 1.67561\n",
      "Epoch 1964/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6772 - accuracy: 0.4839\n",
      "\n",
      "Epoch 01964: loss did not improve from 1.67561\n",
      "Epoch 1965/2000\n",
      "93/93 [==============================] - 0s 593us/step - loss: 1.6777 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01965: loss did not improve from 1.67561\n",
      "Epoch 1966/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6789 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01966: loss did not improve from 1.67561\n",
      "Epoch 1967/2000\n",
      "93/93 [==============================] - 0s 591us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01967: loss did not improve from 1.67561\n",
      "Epoch 1968/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01968: loss did not improve from 1.67561\n",
      "Epoch 1969/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01969: loss did not improve from 1.67561\n",
      "Epoch 1970/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01970: loss did not improve from 1.67561\n",
      "Epoch 1971/2000\n",
      "93/93 [==============================] - 0s 589us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01971: loss did not improve from 1.67561\n",
      "Epoch 1972/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6792 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01972: loss did not improve from 1.67561\n",
      "Epoch 1973/2000\n",
      "93/93 [==============================] - 0s 583us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01973: loss did not improve from 1.67561\n",
      "Epoch 1974/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01974: loss did not improve from 1.67561\n",
      "Epoch 1975/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6771 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01975: loss did not improve from 1.67561\n",
      "Epoch 1976/2000\n",
      "93/93 [==============================] - 0s 615us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01976: loss did not improve from 1.67561\n",
      "Epoch 1977/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 0s 611us/step - loss: 1.6772 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01977: loss did not improve from 1.67561\n",
      "Epoch 1978/2000\n",
      "93/93 [==============================] - 0s 602us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01978: loss did not improve from 1.67561\n",
      "Epoch 1979/2000\n",
      "93/93 [==============================] - 0s 578us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01979: loss did not improve from 1.67561\n",
      "Epoch 1980/2000\n",
      "93/93 [==============================] - 0s 579us/step - loss: 1.6772 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01980: loss did not improve from 1.67561\n",
      "Epoch 1981/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01981: loss did not improve from 1.67561\n",
      "Epoch 1982/2000\n",
      "93/93 [==============================] - 0s 582us/step - loss: 1.6770 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01982: loss did not improve from 1.67561\n",
      "Epoch 1983/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01983: loss did not improve from 1.67561\n",
      "Epoch 1984/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6779 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01984: loss did not improve from 1.67561\n",
      "Epoch 1985/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6774 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01985: loss did not improve from 1.67561\n",
      "Epoch 1986/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6772 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01986: loss did not improve from 1.67561\n",
      "Epoch 1987/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6774 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01987: loss did not improve from 1.67561\n",
      "Epoch 1988/2000\n",
      "93/93 [==============================] - 0s 583us/step - loss: 1.6772 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01988: loss did not improve from 1.67561\n",
      "Epoch 1989/2000\n",
      "93/93 [==============================] - 0s 581us/step - loss: 1.6771 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01989: loss did not improve from 1.67561\n",
      "Epoch 1990/2000\n",
      "93/93 [==============================] - 0s 603us/step - loss: 1.6772 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01990: loss did not improve from 1.67561\n",
      "Epoch 1991/2000\n",
      "93/93 [==============================] - 0s 578us/step - loss: 1.6772 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01991: loss did not improve from 1.67561\n",
      "Epoch 1992/2000\n",
      "93/93 [==============================] - 0s 594us/step - loss: 1.6773 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01992: loss did not improve from 1.67561\n",
      "Epoch 1993/2000\n",
      "93/93 [==============================] - 0s 676us/step - loss: 1.6771 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01993: loss did not improve from 1.67561\n",
      "Epoch 1994/2000\n",
      "93/93 [==============================] - 0s 633us/step - loss: 1.6788 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01994: loss did not improve from 1.67561\n",
      "Epoch 1995/2000\n",
      "93/93 [==============================] - 0s 588us/step - loss: 1.6791 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01995: loss did not improve from 1.67561\n",
      "Epoch 1996/2000\n",
      "93/93 [==============================] - 0s 611us/step - loss: 1.6775 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01996: loss did not improve from 1.67561\n",
      "Epoch 1997/2000\n",
      "93/93 [==============================] - 0s 602us/step - loss: 1.6779 - accuracy: 0.4624\n",
      "\n",
      "Epoch 01997: loss did not improve from 1.67561\n",
      "Epoch 1998/2000\n",
      "93/93 [==============================] - 0s 592us/step - loss: 1.6773 - accuracy: 0.4516\n",
      "\n",
      "Epoch 01998: loss did not improve from 1.67561\n",
      "Epoch 1999/2000\n",
      "93/93 [==============================] - 0s 601us/step - loss: 1.6772 - accuracy: 0.4731\n",
      "\n",
      "Epoch 01999: loss did not improve from 1.67561\n",
      "Epoch 2000/2000\n",
      "93/93 [==============================] - 0s 622us/step - loss: 1.6773 - accuracy: 0.4731\n",
      "\n",
      "Epoch 02000: loss did not improve from 1.67561\n"
     ]
    }
   ],
   "source": [
    "readdata_and_savemodel(\"1000001000100010002.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 5.5841 - accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00001: loss improved from inf to 5.58407, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 2/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 5.3354 - accuracy: 0.0167\n",
      "\n",
      "Epoch 00002: loss improved from 5.58407 to 5.33543, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 3/2000\n",
      "240/240 [==============================] - 0s 749us/step - loss: 4.5788 - accuracy: 0.0917\n",
      "\n",
      "Epoch 00003: loss improved from 5.33543 to 4.57881, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 4/2000\n",
      "240/240 [==============================] - 0s 881us/step - loss: 3.5781 - accuracy: 0.2333\n",
      "\n",
      "Epoch 00004: loss improved from 4.57881 to 3.57810, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 5/2000\n",
      "240/240 [==============================] - 0s 823us/step - loss: 2.9487 - accuracy: 0.3083\n",
      "\n",
      "Epoch 00005: loss improved from 3.57810 to 2.94869, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 6/2000\n",
      "240/240 [==============================] - 0s 816us/step - loss: 2.6299 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00006: loss improved from 2.94869 to 2.62989, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 7/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.5026 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00007: loss improved from 2.62989 to 2.50257, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 8/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 2.4174 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00008: loss improved from 2.50257 to 2.41743, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 9/2000\n",
      "240/240 [==============================] - 0s 718us/step - loss: 2.3813 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00009: loss improved from 2.41743 to 2.38133, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 10/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 2.3507 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00010: loss improved from 2.38133 to 2.35069, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 11/2000\n",
      "240/240 [==============================] - 0s 744us/step - loss: 2.3200 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00011: loss improved from 2.35069 to 2.31999, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 12/2000\n",
      "240/240 [==============================] - 0s 722us/step - loss: 2.2952 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00012: loss improved from 2.31999 to 2.29521, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 13/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 2.2805 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00013: loss improved from 2.29521 to 2.28053, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 14/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 2.2636 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00014: loss improved from 2.28053 to 2.26364, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 15/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 2.2573 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00015: loss improved from 2.26364 to 2.25732, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 16/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 2.2554 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00016: loss improved from 2.25732 to 2.25541, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 17/2000\n",
      "240/240 [==============================] - 0s 781us/step - loss: 2.2450 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00017: loss improved from 2.25541 to 2.24496, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 18/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 2.2317 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00018: loss improved from 2.24496 to 2.23166, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 19/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 2.2297 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00019: loss improved from 2.23166 to 2.22966, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 20/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 2.2161 - accuracy: 0.3125\n",
      "\n",
      "Epoch 00020: loss improved from 2.22966 to 2.21611, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 21/2000\n",
      "240/240 [==============================] - 0s 726us/step - loss: 2.2112 - accuracy: 0.3625\n",
      "\n",
      "Epoch 00021: loss improved from 2.21611 to 2.21117, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 22/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 2.2114 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00022: loss did not improve from 2.21117\n",
      "Epoch 23/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 2.2030 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00023: loss improved from 2.21117 to 2.20296, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 24/2000\n",
      "240/240 [==============================] - 0s 746us/step - loss: 2.2030 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00024: loss did not improve from 2.20296\n",
      "Epoch 25/2000\n",
      "240/240 [==============================] - 0s 802us/step - loss: 2.1919 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00025: loss improved from 2.20296 to 2.19187, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 26/2000\n",
      "240/240 [==============================] - 0s 758us/step - loss: 2.1889 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00026: loss improved from 2.19187 to 2.18888, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 27/2000\n",
      "240/240 [==============================] - 0s 732us/step - loss: 2.1900 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00027: loss did not improve from 2.18888\n",
      "Epoch 28/2000\n",
      "240/240 [==============================] - 0s 782us/step - loss: 2.1826 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00028: loss improved from 2.18888 to 2.18256, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 29/2000\n",
      "240/240 [==============================] - 0s 746us/step - loss: 2.1719 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00029: loss improved from 2.18256 to 2.17188, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 30/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 2.1775 - accuracy: 0.3125\n",
      "\n",
      "Epoch 00030: loss did not improve from 2.17188\n",
      "Epoch 31/2000\n",
      "240/240 [==============================] - 0s 743us/step - loss: 2.1724 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00031: loss did not improve from 2.17188\n",
      "Epoch 32/2000\n",
      "240/240 [==============================] - 0s 757us/step - loss: 2.1736 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00032: loss did not improve from 2.17188\n",
      "Epoch 33/2000\n",
      "240/240 [==============================] - 0s 761us/step - loss: 2.1632 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00033: loss improved from 2.17188 to 2.16325, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 34/2000\n",
      "240/240 [==============================] - 0s 756us/step - loss: 2.1571 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00034: loss improved from 2.16325 to 2.15708, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 35/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 2.1636 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00035: loss did not improve from 2.15708\n",
      "Epoch 36/2000\n",
      "240/240 [==============================] - 0s 722us/step - loss: 2.1519 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00036: loss improved from 2.15708 to 2.15191, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 37/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 2.1551 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00037: loss did not improve from 2.15191\n",
      "Epoch 38/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 2.1473 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00038: loss improved from 2.15191 to 2.14726, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 39/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 2.1494 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00039: loss did not improve from 2.14726\n",
      "Epoch 40/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 2.1447 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00040: loss improved from 2.14726 to 2.14472, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 41/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 2.1407 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00041: loss improved from 2.14472 to 2.14066, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 42/2000\n",
      "240/240 [==============================] - 0s 718us/step - loss: 2.1360 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00042: loss improved from 2.14066 to 2.13599, saving model to ./model/1000001000100010003.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 2.1329 - accuracy: 0.3083\n",
      "\n",
      "Epoch 00043: loss improved from 2.13599 to 2.13292, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 44/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 2.1394 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00044: loss did not improve from 2.13292\n",
      "Epoch 45/2000\n",
      "240/240 [==============================] - 0s 740us/step - loss: 2.1318 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00045: loss improved from 2.13292 to 2.13185, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 46/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 2.1277 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00046: loss improved from 2.13185 to 2.12773, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 47/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 2.1272 - accuracy: 0.3125\n",
      "\n",
      "Epoch 00047: loss improved from 2.12773 to 2.12722, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 48/2000\n",
      "240/240 [==============================] - 0s 774us/step - loss: 2.1215 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00048: loss improved from 2.12722 to 2.12155, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 49/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 2.1224 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00049: loss did not improve from 2.12155\n",
      "Epoch 50/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 2.1237 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00050: loss did not improve from 2.12155\n",
      "Epoch 51/2000\n",
      "240/240 [==============================] - 0s 753us/step - loss: 2.1153 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00051: loss improved from 2.12155 to 2.11530, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 52/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 2.1156 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00052: loss did not improve from 2.11530\n",
      "Epoch 53/2000\n",
      "240/240 [==============================] - 0s 721us/step - loss: 2.1117 - accuracy: 0.3167\n",
      "\n",
      "Epoch 00053: loss improved from 2.11530 to 2.11165, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 54/2000\n",
      "240/240 [==============================] - 0s 742us/step - loss: 2.1091 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00054: loss improved from 2.11165 to 2.10909, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 55/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 2.1076 - accuracy: 0.3583\n",
      "\n",
      "Epoch 00055: loss improved from 2.10909 to 2.10759, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 56/2000\n",
      "240/240 [==============================] - 0s 738us/step - loss: 2.1103 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00056: loss did not improve from 2.10759\n",
      "Epoch 57/2000\n",
      "240/240 [==============================] - 0s 750us/step - loss: 2.1074 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00057: loss improved from 2.10759 to 2.10742, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 58/2000\n",
      "240/240 [==============================] - 0s 726us/step - loss: 2.1032 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00058: loss improved from 2.10742 to 2.10315, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 59/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 2.1024 - accuracy: 0.3083\n",
      "\n",
      "Epoch 00059: loss improved from 2.10315 to 2.10242, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 60/2000\n",
      "240/240 [==============================] - 0s 765us/step - loss: 2.1019 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00060: loss improved from 2.10242 to 2.10192, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 61/2000\n",
      "240/240 [==============================] - 0s 734us/step - loss: 2.0981 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00061: loss improved from 2.10192 to 2.09812, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 62/2000\n",
      "240/240 [==============================] - 0s 757us/step - loss: 2.0976 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00062: loss improved from 2.09812 to 2.09756, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 63/2000\n",
      "240/240 [==============================] - 0s 745us/step - loss: 2.0957 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00063: loss improved from 2.09756 to 2.09573, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 64/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 2.0989 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00064: loss did not improve from 2.09573\n",
      "Epoch 65/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 2.0927 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00065: loss improved from 2.09573 to 2.09270, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 66/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 2.0905 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00066: loss improved from 2.09270 to 2.09051, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 67/2000\n",
      "240/240 [==============================] - 0s 745us/step - loss: 2.0895 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00067: loss improved from 2.09051 to 2.08952, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 68/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 2.0898 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00068: loss did not improve from 2.08952\n",
      "Epoch 69/2000\n",
      "240/240 [==============================] - 0s 752us/step - loss: 2.0877 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00069: loss improved from 2.08952 to 2.08771, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 70/2000\n",
      "240/240 [==============================] - 0s 753us/step - loss: 2.0880 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00070: loss did not improve from 2.08771\n",
      "Epoch 71/2000\n",
      "240/240 [==============================] - 0s 762us/step - loss: 2.0858 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00071: loss improved from 2.08771 to 2.08585, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 72/2000\n",
      "240/240 [==============================] - 0s 761us/step - loss: 2.0891 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00072: loss did not improve from 2.08585\n",
      "Epoch 73/2000\n",
      "240/240 [==============================] - 0s 745us/step - loss: 2.0867 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00073: loss did not improve from 2.08585\n",
      "Epoch 74/2000\n",
      "240/240 [==============================] - 0s 744us/step - loss: 2.0839 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00074: loss improved from 2.08585 to 2.08391, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 75/2000\n",
      "240/240 [==============================] - 0s 732us/step - loss: 2.0792 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00075: loss improved from 2.08391 to 2.07924, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 76/2000\n",
      "240/240 [==============================] - 0s 738us/step - loss: 2.0798 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00076: loss did not improve from 2.07924\n",
      "Epoch 77/2000\n",
      "240/240 [==============================] - 0s 754us/step - loss: 2.0798 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00077: loss did not improve from 2.07924\n",
      "Epoch 78/2000\n",
      "240/240 [==============================] - 0s 744us/step - loss: 2.0804 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00078: loss did not improve from 2.07924\n",
      "Epoch 79/2000\n",
      "240/240 [==============================] - 0s 759us/step - loss: 2.0780 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00079: loss improved from 2.07924 to 2.07796, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 80/2000\n",
      "240/240 [==============================] - 0s 789us/step - loss: 2.0784 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00080: loss did not improve from 2.07796\n",
      "Epoch 81/2000\n",
      "240/240 [==============================] - 0s 741us/step - loss: 2.0752 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00081: loss improved from 2.07796 to 2.07519, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 82/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 2.0808 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00082: loss did not improve from 2.07519\n",
      "Epoch 83/2000\n",
      "240/240 [==============================] - 0s 750us/step - loss: 2.0772 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00083: loss did not improve from 2.07519\n",
      "Epoch 84/2000\n",
      "240/240 [==============================] - 0s 759us/step - loss: 2.0757 - accuracy: 0.3167\n",
      "\n",
      "Epoch 00084: loss did not improve from 2.07519\n",
      "Epoch 85/2000\n",
      "240/240 [==============================] - 0s 767us/step - loss: 2.0769 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00085: loss did not improve from 2.07519\n",
      "Epoch 86/2000\n",
      "240/240 [==============================] - 0s 795us/step - loss: 2.0774 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00086: loss did not improve from 2.07519\n",
      "Epoch 87/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 2.0735 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00087: loss improved from 2.07519 to 2.07353, saving model to ./model/1000001000100010003.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/2000\n",
      "240/240 [==============================] - 0s 898us/step - loss: 2.0713 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00088: loss improved from 2.07353 to 2.07127, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 89/2000\n",
      "240/240 [==============================] - 0s 874us/step - loss: 2.0682 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00089: loss improved from 2.07127 to 2.06815, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 90/2000\n",
      "240/240 [==============================] - 0s 799us/step - loss: 2.0700 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00090: loss did not improve from 2.06815\n",
      "Epoch 91/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 2.0685 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00091: loss did not improve from 2.06815\n",
      "Epoch 92/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 2.0673 - accuracy: 0.3167\n",
      "\n",
      "Epoch 00092: loss improved from 2.06815 to 2.06728, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 93/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 2.0681 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00093: loss did not improve from 2.06728\n",
      "Epoch 94/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 2.0711 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00094: loss did not improve from 2.06728\n",
      "Epoch 95/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 2.0737 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00095: loss did not improve from 2.06728\n",
      "Epoch 96/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 2.0673 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00096: loss improved from 2.06728 to 2.06726, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 97/2000\n",
      "240/240 [==============================] - 0s 747us/step - loss: 2.0682 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00097: loss did not improve from 2.06726\n",
      "Epoch 98/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 2.0652 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00098: loss improved from 2.06726 to 2.06518, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 99/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 2.0633 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00099: loss improved from 2.06518 to 2.06333, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 100/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0635 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00100: loss did not improve from 2.06333\n",
      "Epoch 101/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 2.0627 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00101: loss improved from 2.06333 to 2.06265, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 102/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 2.0622 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00102: loss improved from 2.06265 to 2.06219, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 103/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0643 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00103: loss did not improve from 2.06219\n",
      "Epoch 104/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0633 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00104: loss did not improve from 2.06219\n",
      "Epoch 105/2000\n",
      "240/240 [==============================] - 0s 672us/step - loss: 2.0630 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00105: loss did not improve from 2.06219\n",
      "Epoch 106/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0612 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00106: loss improved from 2.06219 to 2.06116, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 107/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 2.0628 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00107: loss did not improve from 2.06116\n",
      "Epoch 108/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 2.0615 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00108: loss did not improve from 2.06116\n",
      "Epoch 109/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 2.0588 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00109: loss improved from 2.06116 to 2.05882, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 110/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 2.0594 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00110: loss did not improve from 2.05882\n",
      "Epoch 111/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 2.0590 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00111: loss did not improve from 2.05882\n",
      "Epoch 112/2000\n",
      "240/240 [==============================] - 0s 697us/step - loss: 2.0571 - accuracy: 0.3167\n",
      "\n",
      "Epoch 00112: loss improved from 2.05882 to 2.05711, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 113/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 2.0563 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00113: loss improved from 2.05711 to 2.05634, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 114/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 2.0569 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00114: loss did not improve from 2.05634\n",
      "Epoch 115/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 2.0591 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00115: loss did not improve from 2.05634\n",
      "Epoch 116/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 2.0590 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00116: loss did not improve from 2.05634\n",
      "Epoch 117/2000\n",
      "240/240 [==============================] - 0s 721us/step - loss: 2.0557 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00117: loss improved from 2.05634 to 2.05574, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 118/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 2.0563 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00118: loss did not improve from 2.05574\n",
      "Epoch 119/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 2.0545 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00119: loss improved from 2.05574 to 2.05445, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 120/2000\n",
      "240/240 [==============================] - 0s 722us/step - loss: 2.0563 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00120: loss did not improve from 2.05445\n",
      "Epoch 121/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 2.0545 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00121: loss did not improve from 2.05445\n",
      "Epoch 122/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 2.0539 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00122: loss improved from 2.05445 to 2.05386, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 123/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 2.0537 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00123: loss improved from 2.05386 to 2.05368, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 124/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 2.0524 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00124: loss improved from 2.05368 to 2.05243, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 125/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 2.0561 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00125: loss did not improve from 2.05243\n",
      "Epoch 126/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.0531 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00126: loss did not improve from 2.05243\n",
      "Epoch 127/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 2.0527 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00127: loss did not improve from 2.05243\n",
      "Epoch 128/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 2.0608 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00128: loss did not improve from 2.05243\n",
      "Epoch 129/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 2.0531 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00129: loss did not improve from 2.05243\n",
      "Epoch 130/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 2.0509 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00130: loss improved from 2.05243 to 2.05089, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 131/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 2.0516 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00131: loss did not improve from 2.05089\n",
      "Epoch 132/2000\n",
      "240/240 [==============================] - 0s 659us/step - loss: 2.0514 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00132: loss did not improve from 2.05089\n",
      "Epoch 133/2000\n",
      "240/240 [==============================] - 0s 752us/step - loss: 2.0498 - accuracy: 0.3125\n",
      "\n",
      "Epoch 00133: loss improved from 2.05089 to 2.04980, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 134/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 2.0492 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00134: loss improved from 2.04980 to 2.04918, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 135/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 2.0485 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00135: loss improved from 2.04918 to 2.04851, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 136/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 2.0483 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00136: loss improved from 2.04851 to 2.04832, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 137/2000\n",
      "240/240 [==============================] - 0s 697us/step - loss: 2.0482 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00137: loss improved from 2.04832 to 2.04822, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 138/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 2.0491 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00138: loss did not improve from 2.04822\n",
      "Epoch 139/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 2.0476 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00139: loss improved from 2.04822 to 2.04760, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 140/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 2.0478 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00140: loss did not improve from 2.04760\n",
      "Epoch 141/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 2.0481 - accuracy: 0.3125\n",
      "\n",
      "Epoch 00141: loss did not improve from 2.04760\n",
      "Epoch 142/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 2.0481 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00142: loss did not improve from 2.04760\n",
      "Epoch 143/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 2.0494 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00143: loss did not improve from 2.04760\n",
      "Epoch 144/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 2.0476 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00144: loss improved from 2.04760 to 2.04758, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 145/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 2.0464 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00145: loss improved from 2.04758 to 2.04643, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 146/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 2.0469 - accuracy: 0.3583\n",
      "\n",
      "Epoch 00146: loss did not improve from 2.04643\n",
      "Epoch 147/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 2.0470 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00147: loss did not improve from 2.04643\n",
      "Epoch 148/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 2.0447 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00148: loss improved from 2.04643 to 2.04468, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 149/2000\n",
      "240/240 [==============================] - 0s 726us/step - loss: 2.0463 - accuracy: 0.3125\n",
      "\n",
      "Epoch 00149: loss did not improve from 2.04468\n",
      "Epoch 150/2000\n",
      "240/240 [==============================] - 0s 722us/step - loss: 2.0476 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00150: loss did not improve from 2.04468\n",
      "Epoch 151/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 2.0562 - accuracy: 0.3125\n",
      "\n",
      "Epoch 00151: loss did not improve from 2.04468\n",
      "Epoch 152/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 2.0454 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00152: loss did not improve from 2.04468\n",
      "Epoch 153/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 2.0445 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00153: loss improved from 2.04468 to 2.04451, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 154/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 2.0447 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00154: loss did not improve from 2.04451\n",
      "Epoch 155/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 2.0505 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00155: loss did not improve from 2.04451\n",
      "Epoch 156/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 2.0447 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00156: loss did not improve from 2.04451\n",
      "Epoch 157/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 2.0436 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00157: loss improved from 2.04451 to 2.04365, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 158/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 2.0456 - accuracy: 0.3583\n",
      "\n",
      "Epoch 00158: loss did not improve from 2.04365\n",
      "Epoch 159/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 2.0427 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00159: loss improved from 2.04365 to 2.04271, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 160/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 2.0431 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00160: loss did not improve from 2.04271\n",
      "Epoch 161/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 2.0417 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00161: loss improved from 2.04271 to 2.04172, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 162/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 2.0421 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00162: loss did not improve from 2.04172\n",
      "Epoch 163/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 2.0400 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00163: loss improved from 2.04172 to 2.03999, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 164/2000\n",
      "240/240 [==============================] - 0s 718us/step - loss: 2.0409 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00164: loss did not improve from 2.03999\n",
      "Epoch 165/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 2.0407 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00165: loss did not improve from 2.03999\n",
      "Epoch 166/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 2.0404 - accuracy: 0.3167\n",
      "\n",
      "Epoch 00166: loss did not improve from 2.03999\n",
      "Epoch 167/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 2.0406 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00167: loss did not improve from 2.03999\n",
      "Epoch 168/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 2.0394 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00168: loss improved from 2.03999 to 2.03943, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 169/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 2.0404 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00169: loss did not improve from 2.03943\n",
      "Epoch 170/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 2.0417 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00170: loss did not improve from 2.03943\n",
      "Epoch 171/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 2.0551 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00171: loss did not improve from 2.03943\n",
      "Epoch 172/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 2.0452 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00172: loss did not improve from 2.03943\n",
      "Epoch 173/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.0418 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00173: loss did not improve from 2.03943\n",
      "Epoch 174/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 2.0387 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00174: loss improved from 2.03943 to 2.03869, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 175/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 2.0397 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00175: loss did not improve from 2.03869\n",
      "Epoch 176/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 2.0385 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00176: loss improved from 2.03869 to 2.03847, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 177/2000\n",
      "240/240 [==============================] - 0s 861us/step - loss: 2.0382 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00177: loss improved from 2.03847 to 2.03824, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 178/2000\n",
      "240/240 [==============================] - 0s 807us/step - loss: 2.0422 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00178: loss did not improve from 2.03824\n",
      "Epoch 179/2000\n",
      "240/240 [==============================] - 0s 805us/step - loss: 2.0392 - accuracy: 0.3083\n",
      "\n",
      "Epoch 00179: loss did not improve from 2.03824\n",
      "Epoch 180/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 2.0388 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00180: loss did not improve from 2.03824\n",
      "Epoch 181/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 631us/step - loss: 2.0387 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00181: loss did not improve from 2.03824\n",
      "Epoch 182/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 2.0376 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00182: loss improved from 2.03824 to 2.03756, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 183/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 2.0385 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00183: loss did not improve from 2.03756\n",
      "Epoch 184/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 2.0393 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00184: loss did not improve from 2.03756\n",
      "Epoch 185/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 2.0377 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00185: loss did not improve from 2.03756\n",
      "Epoch 186/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.0370 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00186: loss improved from 2.03756 to 2.03696, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 187/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 2.0362 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00187: loss improved from 2.03696 to 2.03620, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 188/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 2.0372 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00188: loss did not improve from 2.03620\n",
      "Epoch 189/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.0358 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00189: loss improved from 2.03620 to 2.03581, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 190/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 2.0363 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00190: loss did not improve from 2.03581\n",
      "Epoch 191/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 2.0464 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00191: loss did not improve from 2.03581\n",
      "Epoch 192/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 2.0433 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00192: loss did not improve from 2.03581\n",
      "Epoch 193/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 2.0381 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00193: loss did not improve from 2.03581\n",
      "Epoch 194/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 2.0371 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00194: loss did not improve from 2.03581\n",
      "Epoch 195/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 2.0373 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00195: loss did not improve from 2.03581\n",
      "Epoch 196/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 2.0358 - accuracy: 0.3125\n",
      "\n",
      "Epoch 00196: loss improved from 2.03581 to 2.03576, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 197/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 2.0352 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00197: loss improved from 2.03576 to 2.03516, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 198/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 2.0341 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00198: loss improved from 2.03516 to 2.03415, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 199/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 2.0344 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00199: loss did not improve from 2.03415\n",
      "Epoch 200/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 2.0439 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00200: loss did not improve from 2.03415\n",
      "Epoch 201/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 2.0347 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00201: loss did not improve from 2.03415\n",
      "Epoch 202/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 2.0343 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00202: loss did not improve from 2.03415\n",
      "Epoch 203/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 2.0347 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00203: loss did not improve from 2.03415\n",
      "Epoch 204/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 2.0344 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00204: loss did not improve from 2.03415\n",
      "Epoch 205/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 2.0333 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00205: loss improved from 2.03415 to 2.03326, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 206/2000\n",
      "240/240 [==============================] - 0s 743us/step - loss: 2.0344 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00206: loss did not improve from 2.03326\n",
      "Epoch 207/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 2.0341 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00207: loss did not improve from 2.03326\n",
      "Epoch 208/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 2.0360 - accuracy: 0.3167\n",
      "\n",
      "Epoch 00208: loss did not improve from 2.03326\n",
      "Epoch 209/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 2.0331 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00209: loss improved from 2.03326 to 2.03305, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 210/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 2.0342 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00210: loss did not improve from 2.03305\n",
      "Epoch 211/2000\n",
      "240/240 [==============================] - 0s 721us/step - loss: 2.0381 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00211: loss did not improve from 2.03305\n",
      "Epoch 212/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 2.0337 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00212: loss did not improve from 2.03305\n",
      "Epoch 213/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 2.0338 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00213: loss did not improve from 2.03305\n",
      "Epoch 214/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 2.0323 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00214: loss improved from 2.03305 to 2.03227, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 215/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 2.0324 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00215: loss did not improve from 2.03227\n",
      "Epoch 216/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 2.0324 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00216: loss did not improve from 2.03227\n",
      "Epoch 217/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 2.0328 - accuracy: 0.3000\n",
      "\n",
      "Epoch 00217: loss did not improve from 2.03227\n",
      "Epoch 218/2000\n",
      "240/240 [==============================] - 0s 734us/step - loss: 2.0324 - accuracy: 0.3167\n",
      "\n",
      "Epoch 00218: loss did not improve from 2.03227\n",
      "Epoch 219/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 2.0318 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00219: loss improved from 2.03227 to 2.03176, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 220/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 2.0357 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00220: loss did not improve from 2.03176\n",
      "Epoch 221/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 2.0366 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00221: loss did not improve from 2.03176\n",
      "Epoch 222/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 2.0331 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00222: loss did not improve from 2.03176\n",
      "Epoch 223/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 2.0350 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00223: loss did not improve from 2.03176\n",
      "Epoch 224/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 2.0359 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00224: loss did not improve from 2.03176\n",
      "Epoch 225/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 2.0348 - accuracy: 0.3667\n",
      "\n",
      "Epoch 00225: loss did not improve from 2.03176\n",
      "Epoch 226/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 2.0316 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00226: loss improved from 2.03176 to 2.03159, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 227/2000\n",
      "240/240 [==============================] - 0s 738us/step - loss: 2.0331 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00227: loss did not improve from 2.03159\n",
      "Epoch 228/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 2.0321 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00228: loss did not improve from 2.03159\n",
      "Epoch 229/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 2.0315 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00229: loss improved from 2.03159 to 2.03151, saving model to ./model/1000001000100010003.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 230/2000\n",
      "240/240 [==============================] - 0s 733us/step - loss: 2.0328 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00230: loss did not improve from 2.03151\n",
      "Epoch 231/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 2.0299 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00231: loss improved from 2.03151 to 2.02993, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 232/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 2.0311 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00232: loss did not improve from 2.02993\n",
      "Epoch 233/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 2.0317 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00233: loss did not improve from 2.02993\n",
      "Epoch 234/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 2.0313 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00234: loss did not improve from 2.02993\n",
      "Epoch 235/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 2.0323 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00235: loss did not improve from 2.02993\n",
      "Epoch 236/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 2.0311 - accuracy: 0.3167\n",
      "\n",
      "Epoch 00236: loss did not improve from 2.02993\n",
      "Epoch 237/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.0310 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00237: loss did not improve from 2.02993\n",
      "Epoch 238/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 2.0305 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00238: loss did not improve from 2.02993\n",
      "Epoch 239/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 2.0304 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00239: loss did not improve from 2.02993\n",
      "Epoch 240/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 2.0302 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00240: loss did not improve from 2.02993\n",
      "Epoch 241/2000\n",
      "240/240 [==============================] - 0s 726us/step - loss: 2.0306 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00241: loss did not improve from 2.02993\n",
      "Epoch 242/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 2.0297 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00242: loss improved from 2.02993 to 2.02973, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 243/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.0304 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00243: loss did not improve from 2.02973\n",
      "Epoch 244/2000\n",
      "240/240 [==============================] - 0s 774us/step - loss: 2.0302 - accuracy: 0.3583\n",
      "\n",
      "Epoch 00244: loss did not improve from 2.02973\n",
      "Epoch 245/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 2.0318 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00245: loss did not improve from 2.02973\n",
      "Epoch 246/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 2.0300 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00246: loss did not improve from 2.02973\n",
      "Epoch 247/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 2.0314 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00247: loss did not improve from 2.02973\n",
      "Epoch 248/2000\n",
      "240/240 [==============================] - 0s 726us/step - loss: 2.0305 - accuracy: 0.3125\n",
      "\n",
      "Epoch 00248: loss did not improve from 2.02973\n",
      "Epoch 249/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 2.0296 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00249: loss improved from 2.02973 to 2.02964, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 250/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 2.0323 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00250: loss did not improve from 2.02964\n",
      "Epoch 251/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 2.0333 - accuracy: 0.3125\n",
      "\n",
      "Epoch 00251: loss did not improve from 2.02964\n",
      "Epoch 252/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 2.0299 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00252: loss did not improve from 2.02964\n",
      "Epoch 253/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 2.0304 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00253: loss did not improve from 2.02964\n",
      "Epoch 254/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 2.0305 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00254: loss did not improve from 2.02964\n",
      "Epoch 255/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 2.0299 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00255: loss did not improve from 2.02964\n",
      "Epoch 256/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 2.0295 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00256: loss improved from 2.02964 to 2.02945, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 257/2000\n",
      "240/240 [==============================] - 0s 740us/step - loss: 2.0287 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00257: loss improved from 2.02945 to 2.02865, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 258/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 2.0278 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00258: loss improved from 2.02865 to 2.02776, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 259/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 2.0282 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00259: loss did not improve from 2.02776\n",
      "Epoch 260/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 2.0286 - accuracy: 0.3167\n",
      "\n",
      "Epoch 00260: loss did not improve from 2.02776\n",
      "Epoch 261/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 2.0289 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00261: loss did not improve from 2.02776\n",
      "Epoch 262/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 2.0297 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00262: loss did not improve from 2.02776\n",
      "Epoch 263/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 2.0300 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00263: loss did not improve from 2.02776\n",
      "Epoch 264/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 2.0303 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00264: loss did not improve from 2.02776\n",
      "Epoch 265/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 2.0285 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00265: loss did not improve from 2.02776\n",
      "Epoch 266/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 2.0279 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00266: loss did not improve from 2.02776\n",
      "Epoch 267/2000\n",
      "240/240 [==============================] - 0s 785us/step - loss: 2.0274 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00267: loss improved from 2.02776 to 2.02744, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 268/2000\n",
      "240/240 [==============================] - 0s 825us/step - loss: 2.0276 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00268: loss did not improve from 2.02744\n",
      "Epoch 269/2000\n",
      "240/240 [==============================] - 0s 809us/step - loss: 2.0276 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00269: loss did not improve from 2.02744\n",
      "Epoch 270/2000\n",
      "240/240 [==============================] - 0s 790us/step - loss: 2.0273 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00270: loss improved from 2.02744 to 2.02729, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 271/2000\n",
      "240/240 [==============================] - 0s 649us/step - loss: 2.0271 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00271: loss improved from 2.02729 to 2.02711, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 272/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 2.0278 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00272: loss did not improve from 2.02711\n",
      "Epoch 273/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 2.0275 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00273: loss did not improve from 2.02711\n",
      "Epoch 274/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0271 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00274: loss improved from 2.02711 to 2.02710, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 275/2000\n",
      "240/240 [==============================] - 0s 732us/step - loss: 2.0271 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00275: loss improved from 2.02710 to 2.02710, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 276/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 2.0277 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00276: loss did not improve from 2.02710\n",
      "Epoch 277/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 2.0269 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00277: loss improved from 2.02710 to 2.02692, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 278/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 2.0362 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00278: loss did not improve from 2.02692\n",
      "Epoch 279/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 699us/step - loss: 2.0323 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00279: loss did not improve from 2.02692\n",
      "Epoch 280/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 2.0302 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00280: loss did not improve from 2.02692\n",
      "Epoch 281/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 2.0277 - accuracy: 0.3167\n",
      "\n",
      "Epoch 00281: loss did not improve from 2.02692\n",
      "Epoch 282/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0267 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00282: loss improved from 2.02692 to 2.02669, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 283/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 2.0261 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00283: loss improved from 2.02669 to 2.02609, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 284/2000\n",
      "240/240 [==============================] - 0s 718us/step - loss: 2.0264 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00284: loss did not improve from 2.02609\n",
      "Epoch 285/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 2.0265 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00285: loss did not improve from 2.02609\n",
      "Epoch 286/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 2.0267 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00286: loss did not improve from 2.02609\n",
      "Epoch 287/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 2.0259 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00287: loss improved from 2.02609 to 2.02594, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 288/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 2.0257 - accuracy: 0.3167\n",
      "\n",
      "Epoch 00288: loss improved from 2.02594 to 2.02570, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 289/2000\n",
      "240/240 [==============================] - 0s 739us/step - loss: 2.0257 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00289: loss improved from 2.02570 to 2.02567, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 290/2000\n",
      "240/240 [==============================] - 0s 697us/step - loss: 2.0265 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00290: loss did not improve from 2.02567\n",
      "Epoch 291/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 2.0263 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00291: loss did not improve from 2.02567\n",
      "Epoch 292/2000\n",
      "240/240 [==============================] - 0s 742us/step - loss: 2.0262 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00292: loss did not improve from 2.02567\n",
      "Epoch 293/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 2.0253 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00293: loss improved from 2.02567 to 2.02527, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 294/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 2.0296 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00294: loss did not improve from 2.02527\n",
      "Epoch 295/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 2.0263 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00295: loss did not improve from 2.02527\n",
      "Epoch 296/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 2.0275 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00296: loss did not improve from 2.02527\n",
      "Epoch 297/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 2.0276 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00297: loss did not improve from 2.02527\n",
      "Epoch 298/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 2.0280 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00298: loss did not improve from 2.02527\n",
      "Epoch 299/2000\n",
      "240/240 [==============================] - 0s 750us/step - loss: 2.0276 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00299: loss did not improve from 2.02527\n",
      "Epoch 300/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 2.0264 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00300: loss did not improve from 2.02527\n",
      "Epoch 301/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 2.0269 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00301: loss did not improve from 2.02527\n",
      "Epoch 302/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 2.0272 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00302: loss did not improve from 2.02527\n",
      "Epoch 303/2000\n",
      "240/240 [==============================] - 0s 745us/step - loss: 2.0257 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00303: loss did not improve from 2.02527\n",
      "Epoch 304/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 2.0257 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00304: loss did not improve from 2.02527\n",
      "Epoch 305/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 2.0264 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00305: loss did not improve from 2.02527\n",
      "Epoch 306/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 2.0250 - accuracy: 0.3167\n",
      "\n",
      "Epoch 00306: loss improved from 2.02527 to 2.02504, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 307/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 2.0252 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00307: loss did not improve from 2.02504\n",
      "Epoch 308/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 2.0252 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00308: loss did not improve from 2.02504\n",
      "Epoch 309/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 2.0252 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00309: loss did not improve from 2.02504\n",
      "Epoch 310/2000\n",
      "240/240 [==============================] - 0s 732us/step - loss: 2.0255 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00310: loss did not improve from 2.02504\n",
      "Epoch 311/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 2.0270 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00311: loss did not improve from 2.02504\n",
      "Epoch 312/2000\n",
      "240/240 [==============================] - 0s 764us/step - loss: 2.0290 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00312: loss did not improve from 2.02504\n",
      "Epoch 313/2000\n",
      "240/240 [==============================] - 0s 766us/step - loss: 2.0255 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00313: loss did not improve from 2.02504\n",
      "Epoch 314/2000\n",
      "240/240 [==============================] - 0s 814us/step - loss: 2.0246 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00314: loss improved from 2.02504 to 2.02463, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 315/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 2.0246 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00315: loss did not improve from 2.02463\n",
      "Epoch 316/2000\n",
      "240/240 [==============================] - 0s 662us/step - loss: 2.0248 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00316: loss did not improve from 2.02463\n",
      "Epoch 317/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0243 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00317: loss improved from 2.02463 to 2.02433, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 318/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 2.0246 - accuracy: 0.3625\n",
      "\n",
      "Epoch 00318: loss did not improve from 2.02433\n",
      "Epoch 319/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0245 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00319: loss did not improve from 2.02433\n",
      "Epoch 320/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 2.0296 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00320: loss did not improve from 2.02433\n",
      "Epoch 321/2000\n",
      "240/240 [==============================] - 0s 737us/step - loss: 2.0282 - accuracy: 0.3125\n",
      "\n",
      "Epoch 00321: loss did not improve from 2.02433\n",
      "Epoch 322/2000\n",
      "240/240 [==============================] - 0s 743us/step - loss: 2.0279 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00322: loss did not improve from 2.02433\n",
      "Epoch 323/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 2.0260 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00323: loss did not improve from 2.02433\n",
      "Epoch 324/2000\n",
      "240/240 [==============================] - 0s 737us/step - loss: 2.0262 - accuracy: 0.3167\n",
      "\n",
      "Epoch 00324: loss did not improve from 2.02433\n",
      "Epoch 325/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 2.0247 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00325: loss did not improve from 2.02433\n",
      "Epoch 326/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 2.0245 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00326: loss did not improve from 2.02433\n",
      "Epoch 327/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 2.0244 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00327: loss did not improve from 2.02433\n",
      "Epoch 328/2000\n",
      "240/240 [==============================] - 0s 758us/step - loss: 2.0233 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00328: loss improved from 2.02433 to 2.02330, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 329/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 743us/step - loss: 2.0244 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00329: loss did not improve from 2.02330\n",
      "Epoch 330/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 2.0278 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00330: loss did not improve from 2.02330\n",
      "Epoch 331/2000\n",
      "240/240 [==============================] - 0s 726us/step - loss: 2.0264 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00331: loss did not improve from 2.02330\n",
      "Epoch 332/2000\n",
      "240/240 [==============================] - 0s 734us/step - loss: 2.0268 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00332: loss did not improve from 2.02330\n",
      "Epoch 333/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 2.0252 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00333: loss did not improve from 2.02330\n",
      "Epoch 334/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 2.0242 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00334: loss did not improve from 2.02330\n",
      "Epoch 335/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 2.0250 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00335: loss did not improve from 2.02330\n",
      "Epoch 336/2000\n",
      "240/240 [==============================] - 0s 726us/step - loss: 2.0267 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00336: loss did not improve from 2.02330\n",
      "Epoch 337/2000\n",
      "240/240 [==============================] - 0s 750us/step - loss: 2.0237 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00337: loss did not improve from 2.02330\n",
      "Epoch 338/2000\n",
      "240/240 [==============================] - 0s 752us/step - loss: 2.0233 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00338: loss did not improve from 2.02330\n",
      "Epoch 339/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 2.0240 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00339: loss did not improve from 2.02330\n",
      "Epoch 340/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 2.0238 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00340: loss did not improve from 2.02330\n",
      "Epoch 341/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 2.0236 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00341: loss did not improve from 2.02330\n",
      "Epoch 342/2000\n",
      "240/240 [==============================] - 0s 776us/step - loss: 2.0238 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00342: loss did not improve from 2.02330\n",
      "Epoch 343/2000\n",
      "240/240 [==============================] - 0s 802us/step - loss: 2.0234 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00343: loss did not improve from 2.02330\n",
      "Epoch 344/2000\n",
      "240/240 [==============================] - 0s 762us/step - loss: 2.0248 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00344: loss did not improve from 2.02330\n",
      "Epoch 345/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 2.0270 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00345: loss did not improve from 2.02330\n",
      "Epoch 346/2000\n",
      "240/240 [==============================] - 0s 740us/step - loss: 2.0263 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00346: loss did not improve from 2.02330\n",
      "Epoch 347/2000\n",
      "240/240 [==============================] - 0s 786us/step - loss: 2.0243 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00347: loss did not improve from 2.02330\n",
      "Epoch 348/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 2.0234 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00348: loss did not improve from 2.02330\n",
      "Epoch 349/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 2.0232 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00349: loss improved from 2.02330 to 2.02316, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 350/2000\n",
      "240/240 [==============================] - 0s 757us/step - loss: 2.0230 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00350: loss improved from 2.02316 to 2.02305, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 351/2000\n",
      "240/240 [==============================] - 0s 761us/step - loss: 2.0270 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00351: loss did not improve from 2.02305\n",
      "Epoch 352/2000\n",
      "240/240 [==============================] - 0s 778us/step - loss: 2.0236 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00352: loss did not improve from 2.02305\n",
      "Epoch 353/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 2.0232 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00353: loss did not improve from 2.02305\n",
      "Epoch 354/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 2.0235 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00354: loss did not improve from 2.02305\n",
      "Epoch 355/2000\n",
      "240/240 [==============================] - 0s 768us/step - loss: 2.0232 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00355: loss did not improve from 2.02305\n",
      "Epoch 356/2000\n",
      "240/240 [==============================] - 0s 757us/step - loss: 2.0234 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00356: loss did not improve from 2.02305\n",
      "Epoch 357/2000\n",
      "240/240 [==============================] - 0s 898us/step - loss: 2.0238 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00357: loss did not improve from 2.02305\n",
      "Epoch 358/2000\n",
      "240/240 [==============================] - 0s 822us/step - loss: 2.0281 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00358: loss did not improve from 2.02305\n",
      "Epoch 359/2000\n",
      "240/240 [==============================] - 0s 918us/step - loss: 2.0246 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00359: loss did not improve from 2.02305\n",
      "Epoch 360/2000\n",
      "240/240 [==============================] - 0s 781us/step - loss: 2.0240 - accuracy: 0.3625\n",
      "\n",
      "Epoch 00360: loss did not improve from 2.02305\n",
      "Epoch 361/2000\n",
      "240/240 [==============================] - 0s 660us/step - loss: 2.0243 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00361: loss did not improve from 2.02305\n",
      "Epoch 362/2000\n",
      "240/240 [==============================] - 0s 654us/step - loss: 2.0239 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00362: loss did not improve from 2.02305\n",
      "Epoch 363/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0242 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00363: loss did not improve from 2.02305\n",
      "Epoch 364/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 2.0239 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00364: loss did not improve from 2.02305\n",
      "Epoch 365/2000\n",
      "240/240 [==============================] - 0s 639us/step - loss: 2.0231 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00365: loss did not improve from 2.02305\n",
      "Epoch 366/2000\n",
      "240/240 [==============================] - 0s 629us/step - loss: 2.0252 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00366: loss did not improve from 2.02305\n",
      "Epoch 367/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 2.0241 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00367: loss did not improve from 2.02305\n",
      "Epoch 368/2000\n",
      "240/240 [==============================] - 0s 650us/step - loss: 2.0250 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00368: loss did not improve from 2.02305\n",
      "Epoch 369/2000\n",
      "240/240 [==============================] - 0s 651us/step - loss: 2.0240 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00369: loss did not improve from 2.02305\n",
      "Epoch 370/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 2.0237 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00370: loss did not improve from 2.02305\n",
      "Epoch 371/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 2.0244 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00371: loss did not improve from 2.02305\n",
      "Epoch 372/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 2.0247 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00372: loss did not improve from 2.02305\n",
      "Epoch 373/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 2.0227 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00373: loss improved from 2.02305 to 2.02268, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 374/2000\n",
      "240/240 [==============================] - 0s 766us/step - loss: 2.0226 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00374: loss improved from 2.02268 to 2.02261, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 375/2000\n",
      "240/240 [==============================] - 0s 732us/step - loss: 2.0226 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00375: loss improved from 2.02261 to 2.02256, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 376/2000\n",
      "240/240 [==============================] - 0s 765us/step - loss: 2.0230 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00376: loss did not improve from 2.02256\n",
      "Epoch 377/2000\n",
      "240/240 [==============================] - 0s 781us/step - loss: 2.0227 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00377: loss did not improve from 2.02256\n",
      "Epoch 378/2000\n",
      "240/240 [==============================] - 0s 763us/step - loss: 2.0234 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00378: loss did not improve from 2.02256\n",
      "Epoch 379/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 2.0236 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00379: loss did not improve from 2.02256\n",
      "Epoch 380/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0232 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00380: loss did not improve from 2.02256\n",
      "Epoch 381/2000\n",
      "240/240 [==============================] - 0s 664us/step - loss: 2.0240 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00381: loss did not improve from 2.02256\n",
      "Epoch 382/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 2.0231 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00382: loss did not improve from 2.02256\n",
      "Epoch 383/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0228 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00383: loss did not improve from 2.02256\n",
      "Epoch 384/2000\n",
      "240/240 [==============================] - 0s 737us/step - loss: 2.0259 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00384: loss did not improve from 2.02256\n",
      "Epoch 385/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 2.0247 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00385: loss did not improve from 2.02256\n",
      "Epoch 386/2000\n",
      "240/240 [==============================] - 0s 745us/step - loss: 2.0225 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00386: loss improved from 2.02256 to 2.02251, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 387/2000\n",
      "240/240 [==============================] - 0s 760us/step - loss: 2.0222 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00387: loss improved from 2.02251 to 2.02222, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 388/2000\n",
      "240/240 [==============================] - 0s 742us/step - loss: 2.0222 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00388: loss did not improve from 2.02222\n",
      "Epoch 389/2000\n",
      "240/240 [==============================] - 0s 762us/step - loss: 2.0229 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00389: loss did not improve from 2.02222\n",
      "Epoch 390/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 2.0223 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00390: loss did not improve from 2.02222\n",
      "Epoch 391/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 2.0231 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00391: loss did not improve from 2.02222\n",
      "Epoch 392/2000\n",
      "240/240 [==============================] - 0s 733us/step - loss: 2.0274 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00392: loss did not improve from 2.02222\n",
      "Epoch 393/2000\n",
      "240/240 [==============================] - 0s 765us/step - loss: 2.0242 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00393: loss did not improve from 2.02222\n",
      "Epoch 394/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 2.0256 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00394: loss did not improve from 2.02222\n",
      "Epoch 395/2000\n",
      "240/240 [==============================] - 0s 718us/step - loss: 2.0229 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00395: loss did not improve from 2.02222\n",
      "Epoch 396/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 2.0234 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00396: loss did not improve from 2.02222\n",
      "Epoch 397/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 2.0229 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00397: loss did not improve from 2.02222\n",
      "Epoch 398/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.0224 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00398: loss did not improve from 2.02222\n",
      "Epoch 399/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 2.0217 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00399: loss improved from 2.02222 to 2.02170, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 400/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 2.0217 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00400: loss improved from 2.02170 to 2.02168, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 401/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 2.0222 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00401: loss did not improve from 2.02168\n",
      "Epoch 402/2000\n",
      "240/240 [==============================] - 0s 734us/step - loss: 2.0235 - accuracy: 0.3583\n",
      "\n",
      "Epoch 00402: loss did not improve from 2.02168\n",
      "Epoch 403/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 2.0244 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00403: loss did not improve from 2.02168\n",
      "Epoch 404/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 2.0237 - accuracy: 0.3667\n",
      "\n",
      "Epoch 00404: loss did not improve from 2.02168\n",
      "Epoch 405/2000\n",
      "240/240 [==============================] - 0s 737us/step - loss: 2.0221 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00405: loss did not improve from 2.02168\n",
      "Epoch 406/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 2.0230 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00406: loss did not improve from 2.02168\n",
      "Epoch 407/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 2.0228 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00407: loss did not improve from 2.02168\n",
      "Epoch 408/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 2.0236 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00408: loss did not improve from 2.02168\n",
      "Epoch 409/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 2.0226 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00409: loss did not improve from 2.02168\n",
      "Epoch 410/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 2.0223 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00410: loss did not improve from 2.02168\n",
      "Epoch 411/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 2.0236 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00411: loss did not improve from 2.02168\n",
      "Epoch 412/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 2.0254 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00412: loss did not improve from 2.02168\n",
      "Epoch 413/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 2.0226 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00413: loss did not improve from 2.02168\n",
      "Epoch 414/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 2.0237 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00414: loss did not improve from 2.02168\n",
      "Epoch 415/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 2.0231 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00415: loss did not improve from 2.02168\n",
      "Epoch 416/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 2.0238 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00416: loss did not improve from 2.02168\n",
      "Epoch 417/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0223 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00417: loss did not improve from 2.02168\n",
      "Epoch 418/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 2.0222 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00418: loss did not improve from 2.02168\n",
      "Epoch 419/2000\n",
      "240/240 [==============================] - 0s 718us/step - loss: 2.0227 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00419: loss did not improve from 2.02168\n",
      "Epoch 420/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 2.0375 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00420: loss did not improve from 2.02168\n",
      "Epoch 421/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 2.0238 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00421: loss did not improve from 2.02168\n",
      "Epoch 422/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 2.0230 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00422: loss did not improve from 2.02168\n",
      "Epoch 423/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 2.0220 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00423: loss did not improve from 2.02168\n",
      "Epoch 424/2000\n",
      "240/240 [==============================] - 0s 726us/step - loss: 2.0221 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00424: loss did not improve from 2.02168\n",
      "Epoch 425/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 2.0218 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00425: loss did not improve from 2.02168\n",
      "Epoch 426/2000\n",
      "240/240 [==============================] - 0s 916us/step - loss: 2.0214 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00426: loss improved from 2.02168 to 2.02139, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 427/2000\n",
      "240/240 [==============================] - 0s 718us/step - loss: 2.0219 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00427: loss did not improve from 2.02139\n",
      "Epoch 428/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 2.0221 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00428: loss did not improve from 2.02139\n",
      "Epoch 429/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 2.0215 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00429: loss did not improve from 2.02139\n",
      "Epoch 430/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 2.0216 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00430: loss did not improve from 2.02139\n",
      "Epoch 431/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 722us/step - loss: 2.0216 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00431: loss did not improve from 2.02139\n",
      "Epoch 432/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 2.0224 - accuracy: 0.3625\n",
      "\n",
      "Epoch 00432: loss did not improve from 2.02139\n",
      "Epoch 433/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 2.0222 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00433: loss did not improve from 2.02139\n",
      "Epoch 434/2000\n",
      "240/240 [==============================] - 0s 721us/step - loss: 2.0235 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00434: loss did not improve from 2.02139\n",
      "Epoch 435/2000\n",
      "240/240 [==============================] - 0s 739us/step - loss: 2.0223 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00435: loss did not improve from 2.02139\n",
      "Epoch 436/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 2.0247 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00436: loss did not improve from 2.02139\n",
      "Epoch 437/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 2.0234 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00437: loss did not improve from 2.02139\n",
      "Epoch 438/2000\n",
      "240/240 [==============================] - 0s 722us/step - loss: 2.0216 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00438: loss did not improve from 2.02139\n",
      "Epoch 439/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 2.0212 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00439: loss improved from 2.02139 to 2.02123, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 440/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 2.0214 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00440: loss did not improve from 2.02123\n",
      "Epoch 441/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 2.0211 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00441: loss improved from 2.02123 to 2.02110, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 442/2000\n",
      "240/240 [==============================] - 0s 752us/step - loss: 2.0238 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00442: loss did not improve from 2.02110\n",
      "Epoch 443/2000\n",
      "240/240 [==============================] - 0s 722us/step - loss: 2.0217 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00443: loss did not improve from 2.02110\n",
      "Epoch 444/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 2.0210 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00444: loss improved from 2.02110 to 2.02105, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 445/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 2.0212 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00445: loss did not improve from 2.02105\n",
      "Epoch 446/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 2.0219 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00446: loss did not improve from 2.02105\n",
      "Epoch 447/2000\n",
      "240/240 [==============================] - 0s 745us/step - loss: 2.0216 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00447: loss did not improve from 2.02105\n",
      "Epoch 448/2000\n",
      "240/240 [==============================] - 0s 822us/step - loss: 2.0208 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00448: loss improved from 2.02105 to 2.02077, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 449/2000\n",
      "240/240 [==============================] - 0s 823us/step - loss: 2.0215 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00449: loss did not improve from 2.02077\n",
      "Epoch 450/2000\n",
      "240/240 [==============================] - 0s 816us/step - loss: 2.0210 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00450: loss did not improve from 2.02077\n",
      "Epoch 451/2000\n",
      "240/240 [==============================] - 0s 662us/step - loss: 2.0210 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00451: loss did not improve from 2.02077\n",
      "Epoch 452/2000\n",
      "240/240 [==============================] - 0s 628us/step - loss: 2.0210 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00452: loss did not improve from 2.02077\n",
      "Epoch 453/2000\n",
      "240/240 [==============================] - 0s 639us/step - loss: 2.0216 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00453: loss did not improve from 2.02077\n",
      "Epoch 454/2000\n",
      "240/240 [==============================] - 0s 612us/step - loss: 2.0213 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00454: loss did not improve from 2.02077\n",
      "Epoch 455/2000\n",
      "240/240 [==============================] - 0s 667us/step - loss: 2.0208 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00455: loss did not improve from 2.02077\n",
      "Epoch 456/2000\n",
      "240/240 [==============================] - 0s 632us/step - loss: 2.0208 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00456: loss did not improve from 2.02077\n",
      "Epoch 457/2000\n",
      "240/240 [==============================] - 0s 618us/step - loss: 2.0214 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00457: loss did not improve from 2.02077\n",
      "Epoch 458/2000\n",
      "240/240 [==============================] - 0s 650us/step - loss: 2.0209 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00458: loss did not improve from 2.02077\n",
      "Epoch 459/2000\n",
      "240/240 [==============================] - 0s 635us/step - loss: 2.0212 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00459: loss did not improve from 2.02077\n",
      "Epoch 460/2000\n",
      "240/240 [==============================] - 0s 613us/step - loss: 2.0209 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00460: loss did not improve from 2.02077\n",
      "Epoch 461/2000\n",
      "240/240 [==============================] - 0s 652us/step - loss: 2.0223 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00461: loss did not improve from 2.02077\n",
      "Epoch 462/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0211 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00462: loss did not improve from 2.02077\n",
      "Epoch 463/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 2.0275 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00463: loss did not improve from 2.02077\n",
      "Epoch 464/2000\n",
      "240/240 [==============================] - 0s 815us/step - loss: 2.0258 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00464: loss did not improve from 2.02077\n",
      "Epoch 465/2000\n",
      "240/240 [==============================] - 0s 743us/step - loss: 2.1036 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00465: loss did not improve from 2.02077\n",
      "Epoch 466/2000\n",
      "240/240 [==============================] - 0s 778us/step - loss: 2.0250 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00466: loss did not improve from 2.02077\n",
      "Epoch 467/2000\n",
      "240/240 [==============================] - 0s 755us/step - loss: 2.0226 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00467: loss did not improve from 2.02077\n",
      "Epoch 468/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 2.0212 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00468: loss did not improve from 2.02077\n",
      "Epoch 469/2000\n",
      "240/240 [==============================] - 0s 757us/step - loss: 2.0208 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00469: loss improved from 2.02077 to 2.02076, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 470/2000\n",
      "240/240 [==============================] - 0s 770us/step - loss: 2.0212 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00470: loss did not improve from 2.02076\n",
      "Epoch 471/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 2.0244 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00471: loss did not improve from 2.02076\n",
      "Epoch 472/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 2.0213 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00472: loss did not improve from 2.02076\n",
      "Epoch 473/2000\n",
      "240/240 [==============================] - 0s 753us/step - loss: 2.0208 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00473: loss did not improve from 2.02076\n",
      "Epoch 474/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 2.0206 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00474: loss improved from 2.02076 to 2.02062, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 475/2000\n",
      "240/240 [==============================] - 0s 722us/step - loss: 2.0214 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00475: loss did not improve from 2.02062\n",
      "Epoch 476/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 2.0206 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00476: loss did not improve from 2.02062\n",
      "Epoch 477/2000\n",
      "240/240 [==============================] - 0s 740us/step - loss: 2.0210 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00477: loss did not improve from 2.02062\n",
      "Epoch 478/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 2.0206 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00478: loss did not improve from 2.02062\n",
      "Epoch 479/2000\n",
      "240/240 [==============================] - 0s 760us/step - loss: 2.0220 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00479: loss did not improve from 2.02062\n",
      "Epoch 480/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 2.0241 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00480: loss did not improve from 2.02062\n",
      "Epoch 481/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 2.0215 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00481: loss did not improve from 2.02062\n",
      "Epoch 482/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0210 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00482: loss did not improve from 2.02062\n",
      "Epoch 483/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 2.0202 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00483: loss improved from 2.02062 to 2.02023, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 484/2000\n",
      "240/240 [==============================] - 0s 914us/step - loss: 2.0206 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00484: loss did not improve from 2.02023\n",
      "Epoch 485/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 2.0201 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00485: loss improved from 2.02023 to 2.02007, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 486/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 2.0205 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00486: loss did not improve from 2.02007\n",
      "Epoch 487/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 2.0205 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00487: loss did not improve from 2.02007\n",
      "Epoch 488/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 2.0221 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00488: loss did not improve from 2.02007\n",
      "Epoch 489/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 2.0220 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00489: loss did not improve from 2.02007\n",
      "Epoch 490/2000\n",
      "240/240 [==============================] - 0s 810us/step - loss: 2.0204 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00490: loss did not improve from 2.02007\n",
      "Epoch 491/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 2.0207 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00491: loss did not improve from 2.02007\n",
      "Epoch 492/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 2.0202 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00492: loss did not improve from 2.02007\n",
      "Epoch 493/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 2.0203 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00493: loss did not improve from 2.02007\n",
      "Epoch 494/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.0200 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00494: loss improved from 2.02007 to 2.02004, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 495/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 2.0200 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00495: loss improved from 2.02004 to 2.01998, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 496/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 2.0204 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00496: loss did not improve from 2.01998\n",
      "Epoch 497/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 2.0205 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00497: loss did not improve from 2.01998\n",
      "Epoch 498/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 2.0200 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00498: loss improved from 2.01998 to 2.01996, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 499/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 2.0203 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00499: loss did not improve from 2.01996\n",
      "Epoch 500/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 2.0199 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00500: loss improved from 2.01996 to 2.01994, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 501/2000\n",
      "240/240 [==============================] - 0s 777us/step - loss: 2.0204 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00501: loss did not improve from 2.01994\n",
      "Epoch 502/2000\n",
      "240/240 [==============================] - 0s 697us/step - loss: 2.0205 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00502: loss did not improve from 2.01994\n",
      "Epoch 503/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 2.0256 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00503: loss did not improve from 2.01994\n",
      "Epoch 504/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 2.0231 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00504: loss did not improve from 2.01994\n",
      "Epoch 505/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 2.0208 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00505: loss did not improve from 2.01994\n",
      "Epoch 506/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0202 - accuracy: 0.3583\n",
      "\n",
      "Epoch 00506: loss did not improve from 2.01994\n",
      "Epoch 507/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 2.0205 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00507: loss did not improve from 2.01994\n",
      "Epoch 508/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0233 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00508: loss did not improve from 2.01994\n",
      "Epoch 509/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 2.0203 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00509: loss did not improve from 2.01994\n",
      "Epoch 510/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 2.0200 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00510: loss did not improve from 2.01994\n",
      "Epoch 511/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0197 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00511: loss improved from 2.01994 to 2.01970, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 512/2000\n",
      "240/240 [==============================] - 0s 664us/step - loss: 2.0213 - accuracy: 0.3167\n",
      "\n",
      "Epoch 00512: loss did not improve from 2.01970\n",
      "Epoch 513/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 2.0266 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00513: loss did not improve from 2.01970\n",
      "Epoch 514/2000\n",
      "240/240 [==============================] - 0s 666us/step - loss: 2.0231 - accuracy: 0.3167\n",
      "\n",
      "Epoch 00514: loss did not improve from 2.01970\n",
      "Epoch 515/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0203 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00515: loss did not improve from 2.01970\n",
      "Epoch 516/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 2.0197 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00516: loss did not improve from 2.01970\n",
      "Epoch 517/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 2.0201 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00517: loss did not improve from 2.01970\n",
      "Epoch 518/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 2.0201 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00518: loss did not improve from 2.01970\n",
      "Epoch 519/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 2.0204 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00519: loss did not improve from 2.01970\n",
      "Epoch 520/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 2.0200 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00520: loss did not improve from 2.01970\n",
      "Epoch 521/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 2.0197 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00521: loss improved from 2.01970 to 2.01967, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 522/2000\n",
      "240/240 [==============================] - 0s 757us/step - loss: 2.0199 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00522: loss did not improve from 2.01967\n",
      "Epoch 523/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 2.0200 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00523: loss did not improve from 2.01967\n",
      "Epoch 524/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 2.0204 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00524: loss did not improve from 2.01967\n",
      "Epoch 525/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 2.0201 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00525: loss did not improve from 2.01967\n",
      "Epoch 526/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 2.0210 - accuracy: 0.3625\n",
      "\n",
      "Epoch 00526: loss did not improve from 2.01967\n",
      "Epoch 527/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 2.0197 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00527: loss did not improve from 2.01967\n",
      "Epoch 528/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0208 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00528: loss did not improve from 2.01967\n",
      "Epoch 529/2000\n",
      "240/240 [==============================] - 0s 661us/step - loss: 2.0206 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00529: loss did not improve from 2.01967\n",
      "Epoch 530/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 2.0220 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00530: loss did not improve from 2.01967\n",
      "Epoch 531/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 2.0207 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00531: loss did not improve from 2.01967\n",
      "Epoch 532/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 691us/step - loss: 2.0207 - accuracy: 0.3083\n",
      "\n",
      "Epoch 00532: loss did not improve from 2.01967\n",
      "Epoch 533/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 2.0200 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00533: loss did not improve from 2.01967\n",
      "Epoch 534/2000\n",
      "240/240 [==============================] - 0s 667us/step - loss: 2.0205 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00534: loss did not improve from 2.01967\n",
      "Epoch 535/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 2.0208 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00535: loss did not improve from 2.01967\n",
      "Epoch 536/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0202 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00536: loss did not improve from 2.01967\n",
      "Epoch 537/2000\n",
      "240/240 [==============================] - 0s 666us/step - loss: 2.0195 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00537: loss improved from 2.01967 to 2.01954, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 538/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0198 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00538: loss did not improve from 2.01954\n",
      "Epoch 539/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 2.0293 - accuracy: 0.3125\n",
      "\n",
      "Epoch 00539: loss did not improve from 2.01954\n",
      "Epoch 540/2000\n",
      "240/240 [==============================] - 0s 751us/step - loss: 2.0205 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00540: loss did not improve from 2.01954\n",
      "Epoch 541/2000\n",
      "240/240 [==============================] - 0s 839us/step - loss: 2.0199 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00541: loss did not improve from 2.01954\n",
      "Epoch 542/2000\n",
      "240/240 [==============================] - 0s 818us/step - loss: 2.0193 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00542: loss improved from 2.01954 to 2.01934, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 543/2000\n",
      "240/240 [==============================] - 0s 827us/step - loss: 2.0193 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00543: loss improved from 2.01934 to 2.01932, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 544/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 2.0213 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00544: loss did not improve from 2.01932\n",
      "Epoch 545/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 2.0221 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00545: loss did not improve from 2.01932\n",
      "Epoch 546/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.0203 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00546: loss did not improve from 2.01932\n",
      "Epoch 547/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 2.0195 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00547: loss did not improve from 2.01932\n",
      "Epoch 548/2000\n",
      "240/240 [==============================] - 0s 648us/step - loss: 2.0191 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00548: loss improved from 2.01932 to 2.01912, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 549/2000\n",
      "240/240 [==============================] - 0s 648us/step - loss: 2.0196 - accuracy: 0.3125\n",
      "\n",
      "Epoch 00549: loss did not improve from 2.01912\n",
      "Epoch 550/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 2.0195 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00550: loss did not improve from 2.01912\n",
      "Epoch 551/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 2.0196 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00551: loss did not improve from 2.01912\n",
      "Epoch 552/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0194 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00552: loss did not improve from 2.01912\n",
      "Epoch 553/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.0209 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00553: loss did not improve from 2.01912\n",
      "Epoch 554/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0201 - accuracy: 0.3583\n",
      "\n",
      "Epoch 00554: loss did not improve from 2.01912\n",
      "Epoch 555/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0197 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00555: loss did not improve from 2.01912\n",
      "Epoch 556/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 2.0198 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00556: loss did not improve from 2.01912\n",
      "Epoch 557/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0202 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00557: loss did not improve from 2.01912\n",
      "Epoch 558/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 2.0196 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00558: loss did not improve from 2.01912\n",
      "Epoch 559/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 2.0195 - accuracy: 0.3167\n",
      "\n",
      "Epoch 00559: loss did not improve from 2.01912\n",
      "Epoch 560/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 2.0194 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00560: loss did not improve from 2.01912\n",
      "Epoch 561/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0196 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00561: loss did not improve from 2.01912\n",
      "Epoch 562/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 2.0193 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00562: loss did not improve from 2.01912\n",
      "Epoch 563/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 2.0214 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00563: loss did not improve from 2.01912\n",
      "Epoch 564/2000\n",
      "240/240 [==============================] - 0s 722us/step - loss: 2.0195 - accuracy: 0.3583\n",
      "\n",
      "Epoch 00564: loss did not improve from 2.01912\n",
      "Epoch 565/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 2.0194 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00565: loss did not improve from 2.01912\n",
      "Epoch 566/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0195 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00566: loss did not improve from 2.01912\n",
      "Epoch 567/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 2.0200 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00567: loss did not improve from 2.01912\n",
      "Epoch 568/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0217 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00568: loss did not improve from 2.01912\n",
      "Epoch 569/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0231 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00569: loss did not improve from 2.01912\n",
      "Epoch 570/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0214 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00570: loss did not improve from 2.01912\n",
      "Epoch 571/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 2.0196 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00571: loss did not improve from 2.01912\n",
      "Epoch 572/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 2.0194 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00572: loss did not improve from 2.01912\n",
      "Epoch 573/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 2.0193 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00573: loss did not improve from 2.01912\n",
      "Epoch 574/2000\n",
      "240/240 [==============================] - 0s 664us/step - loss: 2.0194 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00574: loss did not improve from 2.01912\n",
      "Epoch 575/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 2.0195 - accuracy: 0.3125\n",
      "\n",
      "Epoch 00575: loss did not improve from 2.01912\n",
      "Epoch 576/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 2.0195 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00576: loss did not improve from 2.01912\n",
      "Epoch 577/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0197 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00577: loss did not improve from 2.01912\n",
      "Epoch 578/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0212 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00578: loss did not improve from 2.01912\n",
      "Epoch 579/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0274 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00579: loss did not improve from 2.01912\n",
      "Epoch 580/2000\n",
      "240/240 [==============================] - 0s 662us/step - loss: 2.0206 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00580: loss did not improve from 2.01912\n",
      "Epoch 581/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 2.0213 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00581: loss did not improve from 2.01912\n",
      "Epoch 582/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0214 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00582: loss did not improve from 2.01912\n",
      "Epoch 583/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 2.0214 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00583: loss did not improve from 2.01912\n",
      "Epoch 584/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 728us/step - loss: 2.0193 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00584: loss did not improve from 2.01912\n",
      "Epoch 585/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 2.0193 - accuracy: 0.3583\n",
      "\n",
      "Epoch 00585: loss did not improve from 2.01912\n",
      "Epoch 586/2000\n",
      "240/240 [==============================] - 0s 737us/step - loss: 2.0193 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00586: loss did not improve from 2.01912\n",
      "Epoch 587/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 2.0192 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00587: loss did not improve from 2.01912\n",
      "Epoch 588/2000\n",
      "240/240 [==============================] - 0s 785us/step - loss: 2.0192 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00588: loss did not improve from 2.01912\n",
      "Epoch 589/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 2.0194 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00589: loss did not improve from 2.01912\n",
      "Epoch 590/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 2.0195 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00590: loss did not improve from 2.01912\n",
      "Epoch 591/2000\n",
      "240/240 [==============================] - 0s 789us/step - loss: 2.0237 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00591: loss did not improve from 2.01912\n",
      "Epoch 592/2000\n",
      "240/240 [==============================] - 0s 751us/step - loss: 2.0220 - accuracy: 0.3625\n",
      "\n",
      "Epoch 00592: loss did not improve from 2.01912\n",
      "Epoch 593/2000\n",
      "240/240 [==============================] - 0s 732us/step - loss: 2.0197 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00593: loss did not improve from 2.01912\n",
      "Epoch 594/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 2.0195 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00594: loss did not improve from 2.01912\n",
      "Epoch 595/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0200 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00595: loss did not improve from 2.01912\n",
      "Epoch 596/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0194 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00596: loss did not improve from 2.01912\n",
      "Epoch 597/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 2.0198 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00597: loss did not improve from 2.01912\n",
      "Epoch 598/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0189 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00598: loss improved from 2.01912 to 2.01889, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 599/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0193 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00599: loss did not improve from 2.01889\n",
      "Epoch 600/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 2.0192 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00600: loss did not improve from 2.01889\n",
      "Epoch 601/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 2.0198 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00601: loss did not improve from 2.01889\n",
      "Epoch 602/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 2.0203 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00602: loss did not improve from 2.01889\n",
      "Epoch 603/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 2.0262 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00603: loss did not improve from 2.01889\n",
      "Epoch 604/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 2.0271 - accuracy: 0.3125\n",
      "\n",
      "Epoch 00604: loss did not improve from 2.01889\n",
      "Epoch 605/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 2.0394 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00605: loss did not improve from 2.01889\n",
      "Epoch 606/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0290 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00606: loss did not improve from 2.01889\n",
      "Epoch 607/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 2.0203 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00607: loss did not improve from 2.01889\n",
      "Epoch 608/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0200 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00608: loss did not improve from 2.01889\n",
      "Epoch 609/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 2.0205 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00609: loss did not improve from 2.01889\n",
      "Epoch 610/2000\n",
      "240/240 [==============================] - 0s 914us/step - loss: 2.0193 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00610: loss did not improve from 2.01889\n",
      "Epoch 611/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 2.0193 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00611: loss did not improve from 2.01889\n",
      "Epoch 612/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 2.0191 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00612: loss did not improve from 2.01889\n",
      "Epoch 613/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 2.0193 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00613: loss did not improve from 2.01889\n",
      "Epoch 614/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 2.0191 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00614: loss did not improve from 2.01889\n",
      "Epoch 615/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 2.0192 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00615: loss did not improve from 2.01889\n",
      "Epoch 616/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0192 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00616: loss did not improve from 2.01889\n",
      "Epoch 617/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 2.0193 - accuracy: 0.3083\n",
      "\n",
      "Epoch 00617: loss did not improve from 2.01889\n",
      "Epoch 618/2000\n",
      "240/240 [==============================] - 0s 745us/step - loss: 2.0190 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00618: loss did not improve from 2.01889\n",
      "Epoch 619/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 2.0192 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00619: loss did not improve from 2.01889\n",
      "Epoch 620/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0198 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00620: loss did not improve from 2.01889\n",
      "Epoch 621/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 2.0203 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00621: loss did not improve from 2.01889\n",
      "Epoch 622/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0193 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00622: loss did not improve from 2.01889\n",
      "Epoch 623/2000\n",
      "240/240 [==============================] - 0s 746us/step - loss: 2.0214 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00623: loss did not improve from 2.01889\n",
      "Epoch 624/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 2.0196 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00624: loss did not improve from 2.01889\n",
      "Epoch 625/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 2.0198 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00625: loss did not improve from 2.01889\n",
      "Epoch 626/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0188 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00626: loss improved from 2.01889 to 2.01877, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 627/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 2.0192 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00627: loss did not improve from 2.01877\n",
      "Epoch 628/2000\n",
      "240/240 [==============================] - 0s 664us/step - loss: 2.0192 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00628: loss did not improve from 2.01877\n",
      "Epoch 629/2000\n",
      "240/240 [==============================] - 0s 663us/step - loss: 2.0200 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00629: loss did not improve from 2.01877\n",
      "Epoch 630/2000\n",
      "240/240 [==============================] - 0s 632us/step - loss: 2.0199 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00630: loss did not improve from 2.01877\n",
      "Epoch 631/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 2.0198 - accuracy: 0.3083\n",
      "\n",
      "Epoch 00631: loss did not improve from 2.01877\n",
      "Epoch 632/2000\n",
      "240/240 [==============================] - 0s 664us/step - loss: 2.0200 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00632: loss did not improve from 2.01877\n",
      "Epoch 633/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0200 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00633: loss did not improve from 2.01877\n",
      "Epoch 634/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 2.0196 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00634: loss did not improve from 2.01877\n",
      "Epoch 635/2000\n",
      "240/240 [==============================] - 0s 811us/step - loss: 2.0228 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00635: loss did not improve from 2.01877\n",
      "Epoch 636/2000\n",
      "240/240 [==============================] - 0s 810us/step - loss: 2.0194 - accuracy: 0.2958\n",
      "\n",
      "Epoch 00636: loss did not improve from 2.01877\n",
      "Epoch 637/2000\n",
      "240/240 [==============================] - 0s 806us/step - loss: 2.0191 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00637: loss did not improve from 2.01877\n",
      "Epoch 638/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 2.0192 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00638: loss did not improve from 2.01877\n",
      "Epoch 639/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 2.0193 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00639: loss did not improve from 2.01877\n",
      "Epoch 640/2000\n",
      "240/240 [==============================] - 0s 651us/step - loss: 2.0193 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00640: loss did not improve from 2.01877\n",
      "Epoch 641/2000\n",
      "240/240 [==============================] - 0s 640us/step - loss: 2.0190 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00641: loss did not improve from 2.01877\n",
      "Epoch 642/2000\n",
      "240/240 [==============================] - 0s 627us/step - loss: 2.0190 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00642: loss did not improve from 2.01877\n",
      "Epoch 643/2000\n",
      "240/240 [==============================] - 0s 652us/step - loss: 2.0195 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00643: loss did not improve from 2.01877\n",
      "Epoch 644/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 2.0186 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00644: loss improved from 2.01877 to 2.01863, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 645/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 2.0217 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00645: loss did not improve from 2.01863\n",
      "Epoch 646/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 2.0229 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00646: loss did not improve from 2.01863\n",
      "Epoch 647/2000\n",
      "240/240 [==============================] - 0s 609us/step - loss: 2.0251 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00647: loss did not improve from 2.01863\n",
      "Epoch 648/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 2.0215 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00648: loss did not improve from 2.01863\n",
      "Epoch 649/2000\n",
      "240/240 [==============================] - 0s 613us/step - loss: 2.0207 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00649: loss did not improve from 2.01863\n",
      "Epoch 650/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 2.0206 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00650: loss did not improve from 2.01863\n",
      "Epoch 651/2000\n",
      "240/240 [==============================] - 0s 629us/step - loss: 2.1232 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00651: loss did not improve from 2.01863\n",
      "Epoch 652/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 2.0221 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00652: loss did not improve from 2.01863\n",
      "Epoch 653/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 2.0195 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00653: loss did not improve from 2.01863\n",
      "Epoch 654/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 2.0191 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00654: loss did not improve from 2.01863\n",
      "Epoch 655/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 2.0197 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00655: loss did not improve from 2.01863\n",
      "Epoch 656/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 2.0186 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00656: loss improved from 2.01863 to 2.01861, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 657/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 2.0187 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00657: loss did not improve from 2.01861\n",
      "Epoch 658/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 2.0185 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00658: loss improved from 2.01861 to 2.01853, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 659/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 2.0185 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00659: loss improved from 2.01853 to 2.01848, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 660/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 2.0187 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00660: loss did not improve from 2.01848\n",
      "Epoch 661/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 2.0186 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00661: loss did not improve from 2.01848\n",
      "Epoch 662/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 2.0420 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00662: loss did not improve from 2.01848\n",
      "Epoch 663/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 2.0201 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00663: loss did not improve from 2.01848\n",
      "Epoch 664/2000\n",
      "240/240 [==============================] - 0s 733us/step - loss: 2.0190 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00664: loss did not improve from 2.01848\n",
      "Epoch 665/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 2.0187 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00665: loss did not improve from 2.01848\n",
      "Epoch 666/2000\n",
      "240/240 [==============================] - 0s 662us/step - loss: 2.0189 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00666: loss did not improve from 2.01848\n",
      "Epoch 667/2000\n",
      "240/240 [==============================] - 0s 766us/step - loss: 2.0190 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00667: loss did not improve from 2.01848\n",
      "Epoch 668/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 2.0189 - accuracy: 0.3583\n",
      "\n",
      "Epoch 00668: loss did not improve from 2.01848\n",
      "Epoch 669/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 2.0191 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00669: loss did not improve from 2.01848\n",
      "Epoch 670/2000\n",
      "240/240 [==============================] - 0s 718us/step - loss: 2.0189 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00670: loss did not improve from 2.01848\n",
      "Epoch 671/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 2.0187 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00671: loss did not improve from 2.01848\n",
      "Epoch 672/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 2.0186 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00672: loss did not improve from 2.01848\n",
      "Epoch 673/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 2.0187 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00673: loss did not improve from 2.01848\n",
      "Epoch 674/2000\n",
      "240/240 [==============================] - 0s 697us/step - loss: 2.0189 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00674: loss did not improve from 2.01848\n",
      "Epoch 675/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 2.0191 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00675: loss did not improve from 2.01848\n",
      "Epoch 676/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 2.0183 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00676: loss improved from 2.01848 to 2.01835, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 677/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 2.0188 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00677: loss did not improve from 2.01835\n",
      "Epoch 678/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 2.0188 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00678: loss did not improve from 2.01835\n",
      "Epoch 679/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 2.0185 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00679: loss did not improve from 2.01835\n",
      "Epoch 680/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.0483 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00680: loss did not improve from 2.01835\n",
      "Epoch 681/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 2.0257 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00681: loss did not improve from 2.01835\n",
      "Epoch 682/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 2.0205 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00682: loss did not improve from 2.01835\n",
      "Epoch 683/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 2.0199 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00683: loss did not improve from 2.01835\n",
      "Epoch 684/2000\n",
      "240/240 [==============================] - 0s 742us/step - loss: 2.0194 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00684: loss did not improve from 2.01835\n",
      "Epoch 685/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 2.0201 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00685: loss did not improve from 2.01835\n",
      "Epoch 686/2000\n",
      "240/240 [==============================] - 0s 747us/step - loss: 2.0194 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00686: loss did not improve from 2.01835\n",
      "Epoch 687/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 720us/step - loss: 2.0189 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00687: loss did not improve from 2.01835\n",
      "Epoch 688/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 2.0190 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00688: loss did not improve from 2.01835\n",
      "Epoch 689/2000\n",
      "240/240 [==============================] - 0s 744us/step - loss: 2.0192 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00689: loss did not improve from 2.01835\n",
      "Epoch 690/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 2.0186 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00690: loss did not improve from 2.01835\n",
      "Epoch 691/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 2.0188 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00691: loss did not improve from 2.01835\n",
      "Epoch 692/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 2.0190 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00692: loss did not improve from 2.01835\n",
      "Epoch 693/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0190 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00693: loss did not improve from 2.01835\n",
      "Epoch 694/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 2.0187 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00694: loss did not improve from 2.01835\n",
      "Epoch 695/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 2.0191 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00695: loss did not improve from 2.01835\n",
      "Epoch 696/2000\n",
      "240/240 [==============================] - 0s 753us/step - loss: 2.0219 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00696: loss did not improve from 2.01835\n",
      "Epoch 697/2000\n",
      "240/240 [==============================] - 0s 744us/step - loss: 2.0234 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00697: loss did not improve from 2.01835\n",
      "Epoch 698/2000\n",
      "240/240 [==============================] - 0s 800us/step - loss: 2.0213 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00698: loss did not improve from 2.01835\n",
      "Epoch 699/2000\n",
      "240/240 [==============================] - 0s 747us/step - loss: 2.0193 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00699: loss did not improve from 2.01835\n",
      "Epoch 700/2000\n",
      "240/240 [==============================] - 0s 769us/step - loss: 2.0193 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00700: loss did not improve from 2.01835\n",
      "Epoch 701/2000\n",
      "240/240 [==============================] - 0s 744us/step - loss: 2.0189 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00701: loss did not improve from 2.01835\n",
      "Epoch 702/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 2.0192 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00702: loss did not improve from 2.01835\n",
      "Epoch 703/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 2.0193 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00703: loss did not improve from 2.01835\n",
      "Epoch 704/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 2.0186 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00704: loss did not improve from 2.01835\n",
      "Epoch 705/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.0200 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00705: loss did not improve from 2.01835\n",
      "Epoch 706/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.0187 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00706: loss did not improve from 2.01835\n",
      "Epoch 707/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 2.0189 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00707: loss did not improve from 2.01835\n",
      "Epoch 708/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 2.0269 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00708: loss did not improve from 2.01835\n",
      "Epoch 709/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 2.0202 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00709: loss did not improve from 2.01835\n",
      "Epoch 710/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 2.0207 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00710: loss did not improve from 2.01835\n",
      "Epoch 711/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 2.0214 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00711: loss did not improve from 2.01835\n",
      "Epoch 712/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 2.0190 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00712: loss did not improve from 2.01835\n",
      "Epoch 713/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 2.0195 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00713: loss did not improve from 2.01835\n",
      "Epoch 714/2000\n",
      "240/240 [==============================] - 0s 756us/step - loss: 2.0187 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00714: loss did not improve from 2.01835\n",
      "Epoch 715/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 2.0185 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00715: loss did not improve from 2.01835\n",
      "Epoch 716/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 2.0199 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00716: loss did not improve from 2.01835\n",
      "Epoch 717/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 2.0196 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00717: loss did not improve from 2.01835\n",
      "Epoch 718/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 2.0186 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00718: loss did not improve from 2.01835\n",
      "Epoch 719/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 2.0185 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00719: loss did not improve from 2.01835\n",
      "Epoch 720/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 2.0185 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00720: loss did not improve from 2.01835\n",
      "Epoch 721/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 2.0182 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00721: loss improved from 2.01835 to 2.01822, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 722/2000\n",
      "240/240 [==============================] - 0s 751us/step - loss: 2.0183 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00722: loss did not improve from 2.01822\n",
      "Epoch 723/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 2.0197 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00723: loss did not improve from 2.01822\n",
      "Epoch 724/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 2.0184 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00724: loss did not improve from 2.01822\n",
      "Epoch 725/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 2.0220 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00725: loss did not improve from 2.01822\n",
      "Epoch 726/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 2.0200 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00726: loss did not improve from 2.01822\n",
      "Epoch 727/2000\n",
      "240/240 [==============================] - 0s 732us/step - loss: 2.0188 - accuracy: 0.3167\n",
      "\n",
      "Epoch 00727: loss did not improve from 2.01822\n",
      "Epoch 728/2000\n",
      "240/240 [==============================] - 0s 852us/step - loss: 2.0186 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00728: loss did not improve from 2.01822\n",
      "Epoch 729/2000\n",
      "240/240 [==============================] - 0s 805us/step - loss: 2.0187 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00729: loss did not improve from 2.01822\n",
      "Epoch 730/2000\n",
      "240/240 [==============================] - 0s 838us/step - loss: 2.0185 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00730: loss did not improve from 2.01822\n",
      "Epoch 731/2000\n",
      "240/240 [==============================] - 0s 804us/step - loss: 2.0183 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00731: loss did not improve from 2.01822\n",
      "Epoch 732/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 2.0180 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00732: loss improved from 2.01822 to 2.01803, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 733/2000\n",
      "240/240 [==============================] - 0s 639us/step - loss: 2.0188 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00733: loss did not improve from 2.01803\n",
      "Epoch 734/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 2.0191 - accuracy: 0.3583\n",
      "\n",
      "Epoch 00734: loss did not improve from 2.01803\n",
      "Epoch 735/2000\n",
      "240/240 [==============================] - 0s 647us/step - loss: 2.0350 - accuracy: 0.3167\n",
      "\n",
      "Epoch 00735: loss did not improve from 2.01803\n",
      "Epoch 736/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0204 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00736: loss did not improve from 2.01803\n",
      "Epoch 737/2000\n",
      "240/240 [==============================] - 0s 667us/step - loss: 2.0185 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00737: loss did not improve from 2.01803\n",
      "Epoch 738/2000\n",
      "240/240 [==============================] - 0s 658us/step - loss: 2.0179 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00738: loss improved from 2.01803 to 2.01785, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 739/2000\n",
      "240/240 [==============================] - 0s 747us/step - loss: 2.0186 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00739: loss did not improve from 2.01785\n",
      "Epoch 740/2000\n",
      "240/240 [==============================] - 0s 732us/step - loss: 2.0197 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00740: loss did not improve from 2.01785\n",
      "Epoch 741/2000\n",
      "240/240 [==============================] - 0s 745us/step - loss: 2.0188 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00741: loss did not improve from 2.01785\n",
      "Epoch 742/2000\n",
      "240/240 [==============================] - 0s 741us/step - loss: 2.0188 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00742: loss did not improve from 2.01785\n",
      "Epoch 743/2000\n",
      "240/240 [==============================] - 0s 743us/step - loss: 2.0188 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00743: loss did not improve from 2.01785\n",
      "Epoch 744/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 2.0186 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00744: loss did not improve from 2.01785\n",
      "Epoch 745/2000\n",
      "240/240 [==============================] - 0s 663us/step - loss: 2.0184 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00745: loss did not improve from 2.01785\n",
      "Epoch 746/2000\n",
      "240/240 [==============================] - 0s 623us/step - loss: 2.0180 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00746: loss did not improve from 2.01785\n",
      "Epoch 747/2000\n",
      "240/240 [==============================] - 0s 783us/step - loss: 2.0181 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00747: loss did not improve from 2.01785\n",
      "Epoch 748/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 2.0179 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00748: loss did not improve from 2.01785\n",
      "Epoch 749/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0188 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00749: loss did not improve from 2.01785\n",
      "Epoch 750/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0199 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00750: loss did not improve from 2.01785\n",
      "Epoch 751/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 2.0185 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00751: loss did not improve from 2.01785\n",
      "Epoch 752/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 2.0186 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00752: loss did not improve from 2.01785\n",
      "Epoch 753/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 2.0183 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00753: loss did not improve from 2.01785\n",
      "Epoch 754/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 2.0185 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00754: loss did not improve from 2.01785\n",
      "Epoch 755/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 2.0178 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00755: loss improved from 2.01785 to 2.01775, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 756/2000\n",
      "240/240 [==============================] - 0s 739us/step - loss: 2.0177 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00756: loss improved from 2.01775 to 2.01773, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 757/2000\n",
      "240/240 [==============================] - 0s 763us/step - loss: 2.0177 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00757: loss did not improve from 2.01773\n",
      "Epoch 758/2000\n",
      "240/240 [==============================] - 0s 757us/step - loss: 2.0178 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00758: loss did not improve from 2.01773\n",
      "Epoch 759/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 2.0177 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00759: loss improved from 2.01773 to 2.01771, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 760/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 2.0188 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00760: loss did not improve from 2.01771\n",
      "Epoch 761/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 2.0178 - accuracy: 0.3125\n",
      "\n",
      "Epoch 00761: loss did not improve from 2.01771\n",
      "Epoch 762/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 2.0203 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00762: loss did not improve from 2.01771\n",
      "Epoch 763/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.0208 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00763: loss did not improve from 2.01771\n",
      "Epoch 764/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 2.0191 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00764: loss did not improve from 2.01771\n",
      "Epoch 765/2000\n",
      "240/240 [==============================] - 0s 746us/step - loss: 2.0192 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00765: loss did not improve from 2.01771\n",
      "Epoch 766/2000\n",
      "240/240 [==============================] - 0s 790us/step - loss: 2.0187 - accuracy: 0.3125\n",
      "\n",
      "Epoch 00766: loss did not improve from 2.01771\n",
      "Epoch 767/2000\n",
      "240/240 [==============================] - 0s 796us/step - loss: 2.0179 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00767: loss did not improve from 2.01771\n",
      "Epoch 768/2000\n",
      "240/240 [==============================] - 0s 769us/step - loss: 2.0179 - accuracy: 0.3708\n",
      "\n",
      "Epoch 00768: loss did not improve from 2.01771\n",
      "Epoch 769/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 2.0179 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00769: loss did not improve from 2.01771\n",
      "Epoch 770/2000\n",
      "240/240 [==============================] - 0s 796us/step - loss: 2.0178 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00770: loss did not improve from 2.01771\n",
      "Epoch 771/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 2.0179 - accuracy: 0.3167\n",
      "\n",
      "Epoch 00771: loss did not improve from 2.01771\n",
      "Epoch 772/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 2.0180 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00772: loss did not improve from 2.01771\n",
      "Epoch 773/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 2.0180 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00773: loss did not improve from 2.01771\n",
      "Epoch 774/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 2.0178 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00774: loss did not improve from 2.01771\n",
      "Epoch 775/2000\n",
      "240/240 [==============================] - 0s 751us/step - loss: 2.0185 - accuracy: 0.3583\n",
      "\n",
      "Epoch 00775: loss did not improve from 2.01771\n",
      "Epoch 776/2000\n",
      "240/240 [==============================] - 0s 752us/step - loss: 2.0180 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00776: loss did not improve from 2.01771\n",
      "Epoch 777/2000\n",
      "240/240 [==============================] - 0s 741us/step - loss: 2.0195 - accuracy: 0.3667\n",
      "\n",
      "Epoch 00777: loss did not improve from 2.01771\n",
      "Epoch 778/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 2.0188 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00778: loss did not improve from 2.01771\n",
      "Epoch 779/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 2.0182 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00779: loss did not improve from 2.01771\n",
      "Epoch 780/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0198 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00780: loss did not improve from 2.01771\n",
      "Epoch 781/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 2.0189 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00781: loss did not improve from 2.01771\n",
      "Epoch 782/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 2.0184 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00782: loss did not improve from 2.01771\n",
      "Epoch 783/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 2.0192 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00783: loss did not improve from 2.01771\n",
      "Epoch 784/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 2.0183 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00784: loss did not improve from 2.01771\n",
      "Epoch 785/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 2.0181 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00785: loss did not improve from 2.01771\n",
      "Epoch 786/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 2.0178 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00786: loss did not improve from 2.01771\n",
      "Epoch 787/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 2.0176 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00787: loss improved from 2.01771 to 2.01756, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 788/2000\n",
      "240/240 [==============================] - 0s 718us/step - loss: 2.0178 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00788: loss did not improve from 2.01756\n",
      "Epoch 789/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 2.0181 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00789: loss did not improve from 2.01756\n",
      "Epoch 790/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 2.0179 - accuracy: 0.3583\n",
      "\n",
      "Epoch 00790: loss did not improve from 2.01756\n",
      "Epoch 791/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 717us/step - loss: 2.0578 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00791: loss did not improve from 2.01756\n",
      "Epoch 792/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 2.0278 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00792: loss did not improve from 2.01756\n",
      "Epoch 793/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 2.0193 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00793: loss did not improve from 2.01756\n",
      "Epoch 794/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 2.0192 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00794: loss did not improve from 2.01756\n",
      "Epoch 795/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 2.0189 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00795: loss did not improve from 2.01756\n",
      "Epoch 796/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 2.0180 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00796: loss did not improve from 2.01756\n",
      "Epoch 797/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 2.0179 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00797: loss did not improve from 2.01756\n",
      "Epoch 798/2000\n",
      "240/240 [==============================] - 0s 667us/step - loss: 2.0176 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00798: loss did not improve from 2.01756\n",
      "Epoch 799/2000\n",
      "240/240 [==============================] - 0s 842us/step - loss: 2.0179 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00799: loss did not improve from 2.01756\n",
      "Epoch 800/2000\n",
      "240/240 [==============================] - 0s 885us/step - loss: 2.0178 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00800: loss did not improve from 2.01756\n",
      "Epoch 801/2000\n",
      "240/240 [==============================] - 0s 910us/step - loss: 2.0186 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00801: loss did not improve from 2.01756\n",
      "Epoch 802/2000\n",
      "240/240 [==============================] - 0s 756us/step - loss: 2.0178 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00802: loss did not improve from 2.01756\n",
      "Epoch 803/2000\n",
      "240/240 [==============================] - 0s 737us/step - loss: 2.0179 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00803: loss did not improve from 2.01756\n",
      "Epoch 804/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 2.0176 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00804: loss did not improve from 2.01756\n",
      "Epoch 805/2000\n",
      "240/240 [==============================] - 0s 750us/step - loss: 2.0176 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00805: loss improved from 2.01756 to 2.01756, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 806/2000\n",
      "240/240 [==============================] - 0s 741us/step - loss: 2.0179 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00806: loss did not improve from 2.01756\n",
      "Epoch 807/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 2.0176 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00807: loss did not improve from 2.01756\n",
      "Epoch 808/2000\n",
      "240/240 [==============================] - 0s 734us/step - loss: 2.0176 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00808: loss did not improve from 2.01756\n",
      "Epoch 809/2000\n",
      "240/240 [==============================] - 0s 794us/step - loss: 2.0178 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00809: loss did not improve from 2.01756\n",
      "Epoch 810/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 2.0181 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00810: loss did not improve from 2.01756\n",
      "Epoch 811/2000\n",
      "240/240 [==============================] - 0s 732us/step - loss: 2.0180 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00811: loss did not improve from 2.01756\n",
      "Epoch 812/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 2.0191 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00812: loss did not improve from 2.01756\n",
      "Epoch 813/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0193 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00813: loss did not improve from 2.01756\n",
      "Epoch 814/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 2.0181 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00814: loss did not improve from 2.01756\n",
      "Epoch 815/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 2.0178 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00815: loss did not improve from 2.01756\n",
      "Epoch 816/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 2.0175 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00816: loss improved from 2.01756 to 2.01747, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 817/2000\n",
      "240/240 [==============================] - 0s 848us/step - loss: 2.0180 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00817: loss did not improve from 2.01747\n",
      "Epoch 818/2000\n",
      "240/240 [==============================] - 0s 718us/step - loss: 2.0176 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00818: loss did not improve from 2.01747\n",
      "Epoch 819/2000\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 2.0183 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00819: loss did not improve from 2.01747\n",
      "Epoch 820/2000\n",
      "240/240 [==============================] - 0s 839us/step - loss: 2.0176 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00820: loss did not improve from 2.01747\n",
      "Epoch 821/2000\n",
      "240/240 [==============================] - 0s 796us/step - loss: 2.0778 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00821: loss did not improve from 2.01747\n",
      "Epoch 822/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 2.0189 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00822: loss did not improve from 2.01747\n",
      "Epoch 823/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 2.0194 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00823: loss did not improve from 2.01747\n",
      "Epoch 824/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0601 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00824: loss did not improve from 2.01747\n",
      "Epoch 825/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 2.0184 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00825: loss did not improve from 2.01747\n",
      "Epoch 826/2000\n",
      "240/240 [==============================] - 0s 756us/step - loss: 2.0185 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00826: loss did not improve from 2.01747\n",
      "Epoch 827/2000\n",
      "240/240 [==============================] - 0s 768us/step - loss: 2.0178 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00827: loss did not improve from 2.01747\n",
      "Epoch 828/2000\n",
      "240/240 [==============================] - 0s 755us/step - loss: 2.0175 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00828: loss did not improve from 2.01747\n",
      "Epoch 829/2000\n",
      "240/240 [==============================] - 0s 902us/step - loss: 2.0174 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00829: loss improved from 2.01747 to 2.01738, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 830/2000\n",
      "240/240 [==============================] - 0s 906us/step - loss: 2.0175 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00830: loss did not improve from 2.01738\n",
      "Epoch 831/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 2.0172 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00831: loss improved from 2.01738 to 2.01724, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 832/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 2.0176 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00832: loss did not improve from 2.01724\n",
      "Epoch 833/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 2.0175 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00833: loss did not improve from 2.01724\n",
      "Epoch 834/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0192 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00834: loss did not improve from 2.01724\n",
      "Epoch 835/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0210 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00835: loss did not improve from 2.01724\n",
      "Epoch 836/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0178 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00836: loss did not improve from 2.01724\n",
      "Epoch 837/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 2.0175 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00837: loss did not improve from 2.01724\n",
      "Epoch 838/2000\n",
      "240/240 [==============================] - 0s 902us/step - loss: 2.0174 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00838: loss did not improve from 2.01724\n",
      "Epoch 839/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 2.0176 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00839: loss did not improve from 2.01724\n",
      "Epoch 840/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0175 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00840: loss did not improve from 2.01724\n",
      "Epoch 841/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 2.0175 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00841: loss did not improve from 2.01724\n",
      "Epoch 842/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 2.0172 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00842: loss improved from 2.01724 to 2.01720, saving model to ./model/1000001000100010003.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 843/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 2.0177 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00843: loss did not improve from 2.01720\n",
      "Epoch 844/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 2.0177 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00844: loss did not improve from 2.01720\n",
      "Epoch 845/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0175 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00845: loss did not improve from 2.01720\n",
      "Epoch 846/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 2.0176 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00846: loss did not improve from 2.01720\n",
      "Epoch 847/2000\n",
      "240/240 [==============================] - 0s 666us/step - loss: 2.0183 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00847: loss did not improve from 2.01720\n",
      "Epoch 848/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 2.0185 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00848: loss did not improve from 2.01720\n",
      "Epoch 849/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 2.0175 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00849: loss did not improve from 2.01720\n",
      "Epoch 850/2000\n",
      "240/240 [==============================] - 0s 819us/step - loss: 2.0175 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00850: loss did not improve from 2.01720\n",
      "Epoch 851/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 2.0171 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00851: loss improved from 2.01720 to 2.01706, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 852/2000\n",
      "240/240 [==============================] - 0s 910us/step - loss: 2.0172 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00852: loss did not improve from 2.01706\n",
      "Epoch 853/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 2.0175 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00853: loss did not improve from 2.01706\n",
      "Epoch 854/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 2.0175 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00854: loss did not improve from 2.01706\n",
      "Epoch 855/2000\n",
      "240/240 [==============================] - 0s 661us/step - loss: 2.0173 - accuracy: 0.3625\n",
      "\n",
      "Epoch 00855: loss did not improve from 2.01706\n",
      "Epoch 856/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 2.0170 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00856: loss improved from 2.01706 to 2.01695, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 857/2000\n",
      "240/240 [==============================] - 0s 833us/step - loss: 2.1672 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00857: loss did not improve from 2.01695\n",
      "Epoch 858/2000\n",
      "240/240 [==============================] - 0s 923us/step - loss: 2.0180 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00858: loss did not improve from 2.01695\n",
      "Epoch 859/2000\n",
      "240/240 [==============================] - 0s 738us/step - loss: 2.0176 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00859: loss did not improve from 2.01695\n",
      "Epoch 860/2000\n",
      "240/240 [==============================] - 0s 845us/step - loss: 2.0175 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00860: loss did not improve from 2.01695\n",
      "Epoch 861/2000\n",
      "240/240 [==============================] - 0s 827us/step - loss: 2.0176 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00861: loss did not improve from 2.01695\n",
      "Epoch 862/2000\n",
      "240/240 [==============================] - 0s 935us/step - loss: 2.0176 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00862: loss did not improve from 2.01695\n",
      "Epoch 863/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 2.0173 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00863: loss did not improve from 2.01695\n",
      "Epoch 864/2000\n",
      "240/240 [==============================] - 0s 910us/step - loss: 2.0172 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00864: loss did not improve from 2.01695\n",
      "Epoch 865/2000\n",
      "240/240 [==============================] - 0s 738us/step - loss: 2.0172 - accuracy: 0.3083\n",
      "\n",
      "Epoch 00865: loss did not improve from 2.01695\n",
      "Epoch 866/2000\n",
      "240/240 [==============================] - 0s 885us/step - loss: 2.0174 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00866: loss did not improve from 2.01695\n",
      "Epoch 867/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0173 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00867: loss did not improve from 2.01695\n",
      "Epoch 868/2000\n",
      "240/240 [==============================] - 0s 672us/step - loss: 2.0172 - accuracy: 0.3167\n",
      "\n",
      "Epoch 00868: loss did not improve from 2.01695\n",
      "Epoch 869/2000\n",
      "240/240 [==============================] - 0s 660us/step - loss: 2.0174 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00869: loss did not improve from 2.01695\n",
      "Epoch 870/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 2.0195 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00870: loss did not improve from 2.01695\n",
      "Epoch 871/2000\n",
      "240/240 [==============================] - 0s 663us/step - loss: 2.0181 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00871: loss did not improve from 2.01695\n",
      "Epoch 872/2000\n",
      "240/240 [==============================] - 0s 777us/step - loss: 2.0185 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00872: loss did not improve from 2.01695\n",
      "Epoch 873/2000\n",
      "240/240 [==============================] - 0s 814us/step - loss: 2.0177 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00873: loss did not improve from 2.01695\n",
      "Epoch 874/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 2.0176 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00874: loss did not improve from 2.01695\n",
      "Epoch 875/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 2.0174 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00875: loss did not improve from 2.01695\n",
      "Epoch 876/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.0173 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00876: loss did not improve from 2.01695\n",
      "Epoch 877/2000\n",
      "240/240 [==============================] - 0s 889us/step - loss: 2.0179 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00877: loss did not improve from 2.01695\n",
      "Epoch 878/2000\n",
      "240/240 [==============================] - 0s 805us/step - loss: 2.0175 - accuracy: 0.3583\n",
      "\n",
      "Epoch 00878: loss did not improve from 2.01695\n",
      "Epoch 879/2000\n",
      "240/240 [==============================] - 0s 906us/step - loss: 2.0174 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00879: loss did not improve from 2.01695\n",
      "Epoch 880/2000\n",
      "240/240 [==============================] - 0s 843us/step - loss: 2.0173 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00880: loss did not improve from 2.01695\n",
      "Epoch 881/2000\n",
      "240/240 [==============================] - 0s 791us/step - loss: 2.0172 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00881: loss did not improve from 2.01695\n",
      "Epoch 882/2000\n",
      "240/240 [==============================] - 0s 697us/step - loss: 2.0172 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00882: loss did not improve from 2.01695\n",
      "Epoch 883/2000\n",
      "240/240 [==============================] - 0s 893us/step - loss: 2.0183 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00883: loss did not improve from 2.01695\n",
      "Epoch 884/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 2.0176 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00884: loss did not improve from 2.01695\n",
      "Epoch 885/2000\n",
      "240/240 [==============================] - 0s 797us/step - loss: 2.0186 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00885: loss did not improve from 2.01695\n",
      "Epoch 886/2000\n",
      "240/240 [==============================] - 0s 885us/step - loss: 2.0174 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00886: loss did not improve from 2.01695\n",
      "Epoch 887/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 2.0178 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00887: loss did not improve from 2.01695\n",
      "Epoch 888/2000\n",
      "240/240 [==============================] - 0s 889us/step - loss: 2.0172 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00888: loss did not improve from 2.01695\n",
      "Epoch 889/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 2.0183 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00889: loss did not improve from 2.01695\n",
      "Epoch 890/2000\n",
      "240/240 [==============================] - 0s 873us/step - loss: 2.0288 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00890: loss did not improve from 2.01695\n",
      "Epoch 891/2000\n",
      "240/240 [==============================] - 0s 781us/step - loss: 2.0185 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00891: loss did not improve from 2.01695\n",
      "Epoch 892/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 2.0175 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00892: loss did not improve from 2.01695\n",
      "Epoch 893/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0172 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00893: loss did not improve from 2.01695\n",
      "Epoch 894/2000\n",
      "240/240 [==============================] - 0s 666us/step - loss: 2.0173 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00894: loss did not improve from 2.01695\n",
      "Epoch 895/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 2.0172 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00895: loss did not improve from 2.01695\n",
      "Epoch 896/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 2.0172 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00896: loss did not improve from 2.01695\n",
      "Epoch 897/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 2.0181 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00897: loss did not improve from 2.01695\n",
      "Epoch 898/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 2.0172 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00898: loss did not improve from 2.01695\n",
      "Epoch 899/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 2.0177 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00899: loss did not improve from 2.01695\n",
      "Epoch 900/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 2.0184 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00900: loss did not improve from 2.01695\n",
      "Epoch 901/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 2.0178 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00901: loss did not improve from 2.01695\n",
      "Epoch 902/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 2.0175 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00902: loss did not improve from 2.01695\n",
      "Epoch 903/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 2.0177 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00903: loss did not improve from 2.01695\n",
      "Epoch 904/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 2.0173 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00904: loss did not improve from 2.01695\n",
      "Epoch 905/2000\n",
      "240/240 [==============================] - 0s 877us/step - loss: 2.0173 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00905: loss did not improve from 2.01695\n",
      "Epoch 906/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 2.0195 - accuracy: 0.3167\n",
      "\n",
      "Epoch 00906: loss did not improve from 2.01695\n",
      "Epoch 907/2000\n",
      "240/240 [==============================] - 0s 839us/step - loss: 2.0185 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00907: loss did not improve from 2.01695\n",
      "Epoch 908/2000\n",
      "240/240 [==============================] - 0s 828us/step - loss: 2.0176 - accuracy: 0.3125\n",
      "\n",
      "Epoch 00908: loss did not improve from 2.01695\n",
      "Epoch 909/2000\n",
      "240/240 [==============================] - 0s 829us/step - loss: 2.0170 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00909: loss did not improve from 2.01695\n",
      "Epoch 910/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 2.0175 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00910: loss did not improve from 2.01695\n",
      "Epoch 911/2000\n",
      "240/240 [==============================] - 0s 652us/step - loss: 2.0171 - accuracy: 0.3125\n",
      "\n",
      "Epoch 00911: loss did not improve from 2.01695\n",
      "Epoch 912/2000\n",
      "240/240 [==============================] - 0s 625us/step - loss: 2.0173 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00912: loss did not improve from 2.01695\n",
      "Epoch 913/2000\n",
      "240/240 [==============================] - 0s 645us/step - loss: 2.0222 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00913: loss did not improve from 2.01695\n",
      "Epoch 914/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 2.0189 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00914: loss did not improve from 2.01695\n",
      "Epoch 915/2000\n",
      "240/240 [==============================] - 0s 655us/step - loss: 2.0174 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00915: loss did not improve from 2.01695\n",
      "Epoch 916/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 2.0171 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00916: loss did not improve from 2.01695\n",
      "Epoch 917/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 2.0176 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00917: loss did not improve from 2.01695\n",
      "Epoch 918/2000\n",
      "240/240 [==============================] - 0s 760us/step - loss: 2.0172 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00918: loss did not improve from 2.01695\n",
      "Epoch 919/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 2.0172 - accuracy: 0.3167\n",
      "\n",
      "Epoch 00919: loss did not improve from 2.01695\n",
      "Epoch 920/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 2.0173 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00920: loss did not improve from 2.01695\n",
      "Epoch 921/2000\n",
      "240/240 [==============================] - 0s 697us/step - loss: 2.0172 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00921: loss did not improve from 2.01695\n",
      "Epoch 922/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 2.0172 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00922: loss did not improve from 2.01695\n",
      "Epoch 923/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0171 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00923: loss did not improve from 2.01695\n",
      "Epoch 924/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0172 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00924: loss did not improve from 2.01695\n",
      "Epoch 925/2000\n",
      "240/240 [==============================] - 0s 697us/step - loss: 2.0170 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00925: loss did not improve from 2.01695\n",
      "Epoch 926/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 2.0169 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00926: loss improved from 2.01695 to 2.01687, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 927/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 2.0173 - accuracy: 0.3167\n",
      "\n",
      "Epoch 00927: loss did not improve from 2.01687\n",
      "Epoch 928/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 2.0175 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00928: loss did not improve from 2.01687\n",
      "Epoch 929/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.1188 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00929: loss did not improve from 2.01687\n",
      "Epoch 930/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0187 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00930: loss did not improve from 2.01687\n",
      "Epoch 931/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0196 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00931: loss did not improve from 2.01687\n",
      "Epoch 932/2000\n",
      "240/240 [==============================] - 0s 697us/step - loss: 2.0194 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00932: loss did not improve from 2.01687\n",
      "Epoch 933/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 2.0174 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00933: loss did not improve from 2.01687\n",
      "Epoch 934/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 2.0171 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00934: loss did not improve from 2.01687\n",
      "Epoch 935/2000\n",
      "240/240 [==============================] - 0s 601us/step - loss: 2.0171 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00935: loss did not improve from 2.01687\n",
      "Epoch 936/2000\n",
      "240/240 [==============================] - ETA: 0s - loss: 2.0085 - accuracy: 0.32 - 0s 694us/step - loss: 2.0168 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00936: loss improved from 2.01687 to 2.01684, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 937/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 2.0168 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00937: loss improved from 2.01684 to 2.01680, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 938/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 2.0168 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00938: loss did not improve from 2.01680\n",
      "Epoch 939/2000\n",
      "240/240 [==============================] - 0s 721us/step - loss: 2.0170 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00939: loss did not improve from 2.01680\n",
      "Epoch 940/2000\n",
      "240/240 [==============================] - 0s 745us/step - loss: 2.0172 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00940: loss did not improve from 2.01680\n",
      "Epoch 941/2000\n",
      "240/240 [==============================] - 0s 761us/step - loss: 2.0173 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00941: loss did not improve from 2.01680\n",
      "Epoch 942/2000\n",
      "240/240 [==============================] - 0s 752us/step - loss: 2.0173 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00942: loss did not improve from 2.01680\n",
      "Epoch 943/2000\n",
      "240/240 [==============================] - 0s 749us/step - loss: 2.0175 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00943: loss did not improve from 2.01680\n",
      "Epoch 944/2000\n",
      "240/240 [==============================] - 0s 744us/step - loss: 2.0172 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00944: loss did not improve from 2.01680\n",
      "Epoch 945/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 2.0170 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00945: loss did not improve from 2.01680\n",
      "Epoch 946/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 2.0171 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00946: loss did not improve from 2.01680\n",
      "Epoch 947/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 683us/step - loss: 2.0178 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00947: loss did not improve from 2.01680\n",
      "Epoch 948/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 2.0175 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00948: loss did not improve from 2.01680\n",
      "Epoch 949/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0169 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00949: loss did not improve from 2.01680\n",
      "Epoch 950/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 2.0167 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00950: loss improved from 2.01680 to 2.01673, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 951/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 2.0168 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00951: loss did not improve from 2.01673\n",
      "Epoch 952/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 2.0167 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00952: loss improved from 2.01673 to 2.01671, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 953/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 2.0166 - accuracy: 0.3167\n",
      "\n",
      "Epoch 00953: loss improved from 2.01671 to 2.01665, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 954/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 2.0168 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00954: loss did not improve from 2.01665\n",
      "Epoch 955/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 2.0170 - accuracy: 0.3625\n",
      "\n",
      "Epoch 00955: loss did not improve from 2.01665\n",
      "Epoch 956/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 2.0170 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00956: loss did not improve from 2.01665\n",
      "Epoch 957/2000\n",
      "240/240 [==============================] - 0s 762us/step - loss: 2.0174 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00957: loss did not improve from 2.01665\n",
      "Epoch 958/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 2.0545 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00958: loss did not improve from 2.01665\n",
      "Epoch 959/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 2.0218 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00959: loss did not improve from 2.01665\n",
      "Epoch 960/2000\n",
      "240/240 [==============================] - 0s 761us/step - loss: 2.0187 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00960: loss did not improve from 2.01665\n",
      "Epoch 961/2000\n",
      "240/240 [==============================] - 0s 752us/step - loss: 2.0179 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00961: loss did not improve from 2.01665\n",
      "Epoch 962/2000\n",
      "240/240 [==============================] - 0s 760us/step - loss: 2.0176 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00962: loss did not improve from 2.01665\n",
      "Epoch 963/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 2.0171 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00963: loss did not improve from 2.01665\n",
      "Epoch 964/2000\n",
      "240/240 [==============================] - 0s 756us/step - loss: 2.0190 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00964: loss did not improve from 2.01665\n",
      "Epoch 965/2000\n",
      "240/240 [==============================] - 0s 772us/step - loss: 2.0185 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00965: loss did not improve from 2.01665\n",
      "Epoch 966/2000\n",
      "240/240 [==============================] - 0s 750us/step - loss: 2.0175 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00966: loss did not improve from 2.01665\n",
      "Epoch 967/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 2.0174 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00967: loss did not improve from 2.01665\n",
      "Epoch 968/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0190 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00968: loss did not improve from 2.01665\n",
      "Epoch 969/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0172 - accuracy: 0.3167\n",
      "\n",
      "Epoch 00969: loss did not improve from 2.01665\n",
      "Epoch 970/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 2.0195 - accuracy: 0.3167\n",
      "\n",
      "Epoch 00970: loss did not improve from 2.01665\n",
      "Epoch 971/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 2.0178 - accuracy: 0.3167\n",
      "\n",
      "Epoch 00971: loss did not improve from 2.01665\n",
      "Epoch 972/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 2.0174 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00972: loss did not improve from 2.01665\n",
      "Epoch 973/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 2.0167 - accuracy: 0.3208\n",
      "\n",
      "Epoch 00973: loss did not improve from 2.01665\n",
      "Epoch 974/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0167 - accuracy: 0.3583\n",
      "\n",
      "Epoch 00974: loss did not improve from 2.01665\n",
      "Epoch 975/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 2.0280 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00975: loss did not improve from 2.01665\n",
      "Epoch 976/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 2.0216 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00976: loss did not improve from 2.01665\n",
      "Epoch 977/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 2.0242 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00977: loss did not improve from 2.01665\n",
      "Epoch 978/2000\n",
      "240/240 [==============================] - 0s 672us/step - loss: 2.0185 - accuracy: 0.3167\n",
      "\n",
      "Epoch 00978: loss did not improve from 2.01665\n",
      "Epoch 979/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 2.0177 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00979: loss did not improve from 2.01665\n",
      "Epoch 980/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 2.0172 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00980: loss did not improve from 2.01665\n",
      "Epoch 981/2000\n",
      "240/240 [==============================] - 0s 659us/step - loss: 2.0170 - accuracy: 0.3125\n",
      "\n",
      "Epoch 00981: loss did not improve from 2.01665\n",
      "Epoch 982/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 2.0169 - accuracy: 0.3083\n",
      "\n",
      "Epoch 00982: loss did not improve from 2.01665\n",
      "Epoch 983/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 2.0170 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00983: loss did not improve from 2.01665\n",
      "Epoch 984/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 2.0172 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00984: loss did not improve from 2.01665\n",
      "Epoch 985/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 2.0169 - accuracy: 0.3542\n",
      "\n",
      "Epoch 00985: loss did not improve from 2.01665\n",
      "Epoch 986/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 2.0170 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00986: loss did not improve from 2.01665\n",
      "Epoch 987/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 2.0215 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00987: loss did not improve from 2.01665\n",
      "Epoch 988/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 2.0181 - accuracy: 0.3333\n",
      "\n",
      "Epoch 00988: loss did not improve from 2.01665\n",
      "Epoch 989/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 2.0177 - accuracy: 0.3375\n",
      "\n",
      "Epoch 00989: loss did not improve from 2.01665\n",
      "Epoch 990/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 2.0182 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00990: loss did not improve from 2.01665\n",
      "Epoch 991/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.0174 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00991: loss did not improve from 2.01665\n",
      "Epoch 992/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 2.0182 - accuracy: 0.3500\n",
      "\n",
      "Epoch 00992: loss did not improve from 2.01665\n",
      "Epoch 993/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 2.0170 - accuracy: 0.3167\n",
      "\n",
      "Epoch 00993: loss did not improve from 2.01665\n",
      "Epoch 994/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 2.0169 - accuracy: 0.3292\n",
      "\n",
      "Epoch 00994: loss did not improve from 2.01665\n",
      "Epoch 995/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 2.0166 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00995: loss improved from 2.01665 to 2.01661, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 996/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 2.0166 - accuracy: 0.3250\n",
      "\n",
      "Epoch 00996: loss did not improve from 2.01661\n",
      "Epoch 997/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0167 - accuracy: 0.3417\n",
      "\n",
      "Epoch 00997: loss did not improve from 2.01661\n",
      "Epoch 998/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 2.0167 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00998: loss did not improve from 2.01661\n",
      "Epoch 999/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 674us/step - loss: 2.0196 - accuracy: 0.3458\n",
      "\n",
      "Epoch 00999: loss did not improve from 2.01661\n",
      "Epoch 1000/2000\n",
      "240/240 [==============================] - 0s 829us/step - loss: 2.0182 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01000: loss did not improve from 2.01661\n",
      "Epoch 1001/2000\n",
      "240/240 [==============================] - 0s 859us/step - loss: 2.0173 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01001: loss did not improve from 2.01661\n",
      "Epoch 1002/2000\n",
      "240/240 [==============================] - 0s 830us/step - loss: 2.0170 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01002: loss did not improve from 2.01661\n",
      "Epoch 1003/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 2.0166 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01003: loss did not improve from 2.01661\n",
      "Epoch 1004/2000\n",
      "240/240 [==============================] - 0s 639us/step - loss: 2.0166 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01004: loss did not improve from 2.01661\n",
      "Epoch 1005/2000\n",
      "240/240 [==============================] - 0s 655us/step - loss: 2.0166 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01005: loss improved from 2.01661 to 2.01661, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 1006/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 2.0166 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01006: loss improved from 2.01661 to 2.01656, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 1007/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 2.0166 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01007: loss did not improve from 2.01656\n",
      "Epoch 1008/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0168 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01008: loss did not improve from 2.01656\n",
      "Epoch 1009/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0170 - accuracy: 0.3583\n",
      "\n",
      "Epoch 01009: loss did not improve from 2.01656\n",
      "Epoch 1010/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 2.0167 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01010: loss did not improve from 2.01656\n",
      "Epoch 1011/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 2.0166 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01011: loss did not improve from 2.01656\n",
      "Epoch 1012/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 2.0164 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01012: loss improved from 2.01656 to 2.01640, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 1013/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0168 - accuracy: 0.3583\n",
      "\n",
      "Epoch 01013: loss did not improve from 2.01640\n",
      "Epoch 1014/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 2.0168 - accuracy: 0.3125\n",
      "\n",
      "Epoch 01014: loss did not improve from 2.01640\n",
      "Epoch 1015/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 2.0187 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01015: loss did not improve from 2.01640\n",
      "Epoch 1016/2000\n",
      "240/240 [==============================] - 0s 721us/step - loss: 2.0183 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01016: loss did not improve from 2.01640\n",
      "Epoch 1017/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 2.0172 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01017: loss did not improve from 2.01640\n",
      "Epoch 1018/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 2.0179 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01018: loss did not improve from 2.01640\n",
      "Epoch 1019/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 2.0173 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01019: loss did not improve from 2.01640\n",
      "Epoch 1020/2000\n",
      "240/240 [==============================] - 0s 739us/step - loss: 2.0170 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01020: loss did not improve from 2.01640\n",
      "Epoch 1021/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 2.0169 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01021: loss did not improve from 2.01640\n",
      "Epoch 1022/2000\n",
      "240/240 [==============================] - 0s 753us/step - loss: 2.0170 - accuracy: 0.3542\n",
      "\n",
      "Epoch 01022: loss did not improve from 2.01640\n",
      "Epoch 1023/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.0166 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01023: loss did not improve from 2.01640\n",
      "Epoch 1024/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 2.0165 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01024: loss did not improve from 2.01640\n",
      "Epoch 1025/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 2.0167 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01025: loss did not improve from 2.01640\n",
      "Epoch 1026/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0166 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01026: loss did not improve from 2.01640\n",
      "Epoch 1027/2000\n",
      "240/240 [==============================] - 0s 666us/step - loss: 2.0163 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01027: loss improved from 2.01640 to 2.01635, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 1028/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0164 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01028: loss did not improve from 2.01635\n",
      "Epoch 1029/2000\n",
      "240/240 [==============================] - 0s 667us/step - loss: 2.0167 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01029: loss did not improve from 2.01635\n",
      "Epoch 1030/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 2.0168 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01030: loss did not improve from 2.01635\n",
      "Epoch 1031/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0627 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01031: loss did not improve from 2.01635\n",
      "Epoch 1032/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0177 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01032: loss did not improve from 2.01635\n",
      "Epoch 1033/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0172 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01033: loss did not improve from 2.01635\n",
      "Epoch 1034/2000\n",
      "240/240 [==============================] - 0s 672us/step - loss: 2.0174 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01034: loss did not improve from 2.01635\n",
      "Epoch 1035/2000\n",
      "240/240 [==============================] - 0s 666us/step - loss: 2.0166 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01035: loss did not improve from 2.01635\n",
      "Epoch 1036/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0165 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01036: loss did not improve from 2.01635\n",
      "Epoch 1037/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0165 - accuracy: 0.3625\n",
      "\n",
      "Epoch 01037: loss did not improve from 2.01635\n",
      "Epoch 1038/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 2.0164 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01038: loss did not improve from 2.01635\n",
      "Epoch 1039/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0164 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01039: loss did not improve from 2.01635\n",
      "Epoch 1040/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 2.0166 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01040: loss did not improve from 2.01635\n",
      "Epoch 1041/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0169 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01041: loss did not improve from 2.01635\n",
      "Epoch 1042/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0194 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01042: loss did not improve from 2.01635\n",
      "Epoch 1043/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0172 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01043: loss did not improve from 2.01635\n",
      "Epoch 1044/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 2.0168 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01044: loss did not improve from 2.01635\n",
      "Epoch 1045/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 2.0196 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01045: loss did not improve from 2.01635\n",
      "Epoch 1046/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0164 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01046: loss did not improve from 2.01635\n",
      "Epoch 1047/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.1486 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01047: loss did not improve from 2.01635\n",
      "Epoch 1048/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0171 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01048: loss did not improve from 2.01635\n",
      "Epoch 1049/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0169 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01049: loss did not improve from 2.01635\n",
      "Epoch 1050/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 2.0163 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01050: loss improved from 2.01635 to 2.01632, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 1051/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 2.0184 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01051: loss did not improve from 2.01632\n",
      "Epoch 1052/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 2.0174 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01052: loss did not improve from 2.01632\n",
      "Epoch 1053/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 2.0173 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01053: loss did not improve from 2.01632\n",
      "Epoch 1054/2000\n",
      "240/240 [==============================] - 0s 661us/step - loss: 2.0169 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01054: loss did not improve from 2.01632\n",
      "Epoch 1055/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 2.0166 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01055: loss did not improve from 2.01632\n",
      "Epoch 1056/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0168 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01056: loss did not improve from 2.01632\n",
      "Epoch 1057/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0166 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01057: loss did not improve from 2.01632\n",
      "Epoch 1058/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 2.0165 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01058: loss did not improve from 2.01632\n",
      "Epoch 1059/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 2.0165 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01059: loss did not improve from 2.01632\n",
      "Epoch 1060/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 2.0169 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01060: loss did not improve from 2.01632\n",
      "Epoch 1061/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 2.0170 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01061: loss did not improve from 2.01632\n",
      "Epoch 1062/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0174 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01062: loss did not improve from 2.01632\n",
      "Epoch 1063/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 2.0165 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01063: loss did not improve from 2.01632\n",
      "Epoch 1064/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0167 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01064: loss did not improve from 2.01632\n",
      "Epoch 1065/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0167 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01065: loss did not improve from 2.01632\n",
      "Epoch 1066/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0166 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01066: loss did not improve from 2.01632\n",
      "Epoch 1067/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 2.0166 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01067: loss did not improve from 2.01632\n",
      "Epoch 1068/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0168 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01068: loss did not improve from 2.01632\n",
      "Epoch 1069/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0427 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01069: loss did not improve from 2.01632\n",
      "Epoch 1070/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 2.0216 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01070: loss did not improve from 2.01632\n",
      "Epoch 1071/2000\n",
      "240/240 [==============================] - 0s 823us/step - loss: 2.0260 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01071: loss did not improve from 2.01632\n",
      "Epoch 1072/2000\n",
      "240/240 [==============================] - 0s 769us/step - loss: 2.0186 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01072: loss did not improve from 2.01632\n",
      "Epoch 1073/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 2.0175 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01073: loss did not improve from 2.01632\n",
      "Epoch 1074/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 2.0172 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01074: loss did not improve from 2.01632\n",
      "Epoch 1075/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0168 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01075: loss did not improve from 2.01632\n",
      "Epoch 1076/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0168 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01076: loss did not improve from 2.01632\n",
      "Epoch 1077/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0168 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01077: loss did not improve from 2.01632\n",
      "Epoch 1078/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0168 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01078: loss did not improve from 2.01632\n",
      "Epoch 1079/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 2.0166 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01079: loss did not improve from 2.01632\n",
      "Epoch 1080/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0166 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01080: loss did not improve from 2.01632\n",
      "Epoch 1081/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 2.0167 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01081: loss did not improve from 2.01632\n",
      "Epoch 1082/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 2.0165 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01082: loss did not improve from 2.01632\n",
      "Epoch 1083/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 2.0169 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01083: loss did not improve from 2.01632\n",
      "Epoch 1084/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 2.0166 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01084: loss did not improve from 2.01632\n",
      "Epoch 1085/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 2.0170 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01085: loss did not improve from 2.01632\n",
      "Epoch 1086/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0165 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01086: loss did not improve from 2.01632\n",
      "Epoch 1087/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 2.0164 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01087: loss did not improve from 2.01632\n",
      "Epoch 1088/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 2.0161 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01088: loss improved from 2.01632 to 2.01615, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 1089/2000\n",
      "240/240 [==============================] - 0s 881us/step - loss: 2.0162 - accuracy: 0.3583\n",
      "\n",
      "Epoch 01089: loss did not improve from 2.01615\n",
      "Epoch 1090/2000\n",
      "240/240 [==============================] - 0s 744us/step - loss: 2.0166 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01090: loss did not improve from 2.01615\n",
      "Epoch 1091/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 2.0170 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01091: loss did not improve from 2.01615\n",
      "Epoch 1092/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 2.0174 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01092: loss did not improve from 2.01615\n",
      "Epoch 1093/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 2.0172 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01093: loss did not improve from 2.01615\n",
      "Epoch 1094/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 2.0167 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01094: loss did not improve from 2.01615\n",
      "Epoch 1095/2000\n",
      "240/240 [==============================] - 0s 836us/step - loss: 2.0165 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01095: loss did not improve from 2.01615\n",
      "Epoch 1096/2000\n",
      "240/240 [==============================] - 0s 936us/step - loss: 2.0164 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01096: loss did not improve from 2.01615\n",
      "Epoch 1097/2000\n",
      "240/240 [==============================] - 0s 805us/step - loss: 2.0164 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01097: loss did not improve from 2.01615\n",
      "Epoch 1098/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 2.0166 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01098: loss did not improve from 2.01615\n",
      "Epoch 1099/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 2.1155 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01099: loss did not improve from 2.01615\n",
      "Epoch 1100/2000\n",
      "240/240 [==============================] - 0s 658us/step - loss: 2.0181 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01100: loss did not improve from 2.01615\n",
      "Epoch 1101/2000\n",
      "240/240 [==============================] - 0s 666us/step - loss: 2.0166 - accuracy: 0.3542\n",
      "\n",
      "Epoch 01101: loss did not improve from 2.01615\n",
      "Epoch 1102/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 682us/step - loss: 2.0171 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01102: loss did not improve from 2.01615\n",
      "Epoch 1103/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.0169 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01103: loss did not improve from 2.01615\n",
      "Epoch 1104/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0164 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01104: loss did not improve from 2.01615\n",
      "Epoch 1105/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 2.0166 - accuracy: 0.3625\n",
      "\n",
      "Epoch 01105: loss did not improve from 2.01615\n",
      "Epoch 1106/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 2.0162 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01106: loss did not improve from 2.01615\n",
      "Epoch 1107/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 2.0166 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01107: loss did not improve from 2.01615\n",
      "Epoch 1108/2000\n",
      "240/240 [==============================] - 0s 756us/step - loss: 2.0167 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01108: loss did not improve from 2.01615\n",
      "Epoch 1109/2000\n",
      "240/240 [==============================] - 0s 864us/step - loss: 2.0359 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01109: loss did not improve from 2.01615\n",
      "Epoch 1110/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 2.0170 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01110: loss did not improve from 2.01615\n",
      "Epoch 1111/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 2.0166 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01111: loss did not improve from 2.01615\n",
      "Epoch 1112/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 2.0166 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01112: loss did not improve from 2.01615\n",
      "Epoch 1113/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0165 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01113: loss did not improve from 2.01615\n",
      "Epoch 1114/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 2.0165 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01114: loss did not improve from 2.01615\n",
      "Epoch 1115/2000\n",
      "240/240 [==============================] - 0s 742us/step - loss: 2.0166 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01115: loss did not improve from 2.01615\n",
      "Epoch 1116/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 2.0180 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01116: loss did not improve from 2.01615\n",
      "Epoch 1117/2000\n",
      "240/240 [==============================] - 0s 672us/step - loss: 2.0180 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01117: loss did not improve from 2.01615\n",
      "Epoch 1118/2000\n",
      "240/240 [==============================] - 0s 672us/step - loss: 2.0334 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01118: loss did not improve from 2.01615\n",
      "Epoch 1119/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0174 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01119: loss did not improve from 2.01615\n",
      "Epoch 1120/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 2.0170 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01120: loss did not improve from 2.01615\n",
      "Epoch 1121/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0165 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01121: loss did not improve from 2.01615\n",
      "Epoch 1122/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 2.0167 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01122: loss did not improve from 2.01615\n",
      "Epoch 1123/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0163 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01123: loss did not improve from 2.01615\n",
      "Epoch 1124/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 2.0164 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01124: loss did not improve from 2.01615\n",
      "Epoch 1125/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 2.0162 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01125: loss did not improve from 2.01615\n",
      "Epoch 1126/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 2.0162 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01126: loss did not improve from 2.01615\n",
      "Epoch 1127/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0166 - accuracy: 0.3542\n",
      "\n",
      "Epoch 01127: loss did not improve from 2.01615\n",
      "Epoch 1128/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0164 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01128: loss did not improve from 2.01615\n",
      "Epoch 1129/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0166 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01129: loss did not improve from 2.01615\n",
      "Epoch 1130/2000\n",
      "240/240 [==============================] - 0s 655us/step - loss: 2.0164 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01130: loss did not improve from 2.01615\n",
      "Epoch 1131/2000\n",
      "240/240 [==============================] - 0s 718us/step - loss: 2.0161 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01131: loss improved from 2.01615 to 2.01614, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 1132/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 2.0163 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01132: loss did not improve from 2.01614\n",
      "Epoch 1133/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 2.0169 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01133: loss did not improve from 2.01614\n",
      "Epoch 1134/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 2.0176 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01134: loss did not improve from 2.01614\n",
      "Epoch 1135/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 2.0179 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01135: loss did not improve from 2.01614\n",
      "Epoch 1136/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 2.0169 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01136: loss did not improve from 2.01614\n",
      "Epoch 1137/2000\n",
      "240/240 [==============================] - 0s 666us/step - loss: 2.0165 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01137: loss did not improve from 2.01614\n",
      "Epoch 1138/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 2.0204 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01138: loss did not improve from 2.01614\n",
      "Epoch 1139/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0277 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01139: loss did not improve from 2.01614\n",
      "Epoch 1140/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 2.0195 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01140: loss did not improve from 2.01614\n",
      "Epoch 1141/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 2.0179 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01141: loss did not improve from 2.01614\n",
      "Epoch 1142/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0171 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01142: loss did not improve from 2.01614\n",
      "Epoch 1143/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 2.0164 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01143: loss did not improve from 2.01614\n",
      "Epoch 1144/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0161 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01144: loss improved from 2.01614 to 2.01614, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 1145/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 2.0161 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01145: loss improved from 2.01614 to 2.01612, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 1146/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0161 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01146: loss did not improve from 2.01612\n",
      "Epoch 1147/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 2.0161 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01147: loss did not improve from 2.01612\n",
      "Epoch 1148/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 2.0161 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01148: loss improved from 2.01612 to 2.01611, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 1149/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 2.0909 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01149: loss did not improve from 2.01611\n",
      "Epoch 1150/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 2.0172 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01150: loss did not improve from 2.01611\n",
      "Epoch 1151/2000\n",
      "240/240 [==============================] - 0s 628us/step - loss: 2.0166 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01151: loss did not improve from 2.01611\n",
      "Epoch 1152/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 2.0163 - accuracy: 0.3583\n",
      "\n",
      "Epoch 01152: loss did not improve from 2.01611\n",
      "Epoch 1153/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 2.0164 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01153: loss did not improve from 2.01611\n",
      "Epoch 1154/2000\n",
      "240/240 [==============================] - 0s 662us/step - loss: 2.0164 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01154: loss did not improve from 2.01611\n",
      "Epoch 1155/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 2.0165 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01155: loss did not improve from 2.01611\n",
      "Epoch 1156/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 2.0172 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01156: loss did not improve from 2.01611\n",
      "Epoch 1157/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 2.0164 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01157: loss did not improve from 2.01611\n",
      "Epoch 1158/2000\n",
      "240/240 [==============================] - 0s 795us/step - loss: 2.0162 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01158: loss did not improve from 2.01611\n",
      "Epoch 1159/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 2.0162 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01159: loss did not improve from 2.01611\n",
      "Epoch 1160/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0162 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01160: loss did not improve from 2.01611\n",
      "Epoch 1161/2000\n",
      "240/240 [==============================] - 0s 663us/step - loss: 2.0161 - accuracy: 0.3583\n",
      "\n",
      "Epoch 01161: loss improved from 2.01611 to 2.01610, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 1162/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 2.0165 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01162: loss did not improve from 2.01610\n",
      "Epoch 1163/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 2.0163 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01163: loss did not improve from 2.01610\n",
      "Epoch 1164/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 2.0162 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01164: loss did not improve from 2.01610\n",
      "Epoch 1165/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0160 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01165: loss improved from 2.01610 to 2.01604, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 1166/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 2.0161 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01166: loss did not improve from 2.01604\n",
      "Epoch 1167/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0162 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01167: loss did not improve from 2.01604\n",
      "Epoch 1168/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 2.0169 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01168: loss did not improve from 2.01604\n",
      "Epoch 1169/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 2.0213 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01169: loss did not improve from 2.01604\n",
      "Epoch 1170/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 2.0200 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01170: loss did not improve from 2.01604\n",
      "Epoch 1171/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 2.0167 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01171: loss did not improve from 2.01604\n",
      "Epoch 1172/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0304 - accuracy: 0.3542\n",
      "\n",
      "Epoch 01172: loss did not improve from 2.01604\n",
      "Epoch 1173/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 2.0174 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01173: loss did not improve from 2.01604\n",
      "Epoch 1174/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0154 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01174: loss improved from 2.01604 to 2.01539, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 1175/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 2.0895 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01175: loss did not improve from 2.01539\n",
      "Epoch 1176/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0172 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01176: loss did not improve from 2.01539\n",
      "Epoch 1177/2000\n",
      "240/240 [==============================] - 0s 651us/step - loss: 2.0175 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01177: loss did not improve from 2.01539\n",
      "Epoch 1178/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0167 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01178: loss did not improve from 2.01539\n",
      "Epoch 1179/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0163 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01179: loss did not improve from 2.01539\n",
      "Epoch 1180/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0163 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01180: loss did not improve from 2.01539\n",
      "Epoch 1181/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 2.0161 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01181: loss did not improve from 2.01539\n",
      "Epoch 1182/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 2.0160 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01182: loss did not improve from 2.01539\n",
      "Epoch 1183/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0162 - accuracy: 0.3125\n",
      "\n",
      "Epoch 01183: loss did not improve from 2.01539\n",
      "Epoch 1184/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 2.0162 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01184: loss did not improve from 2.01539\n",
      "Epoch 1185/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0164 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01185: loss did not improve from 2.01539\n",
      "Epoch 1186/2000\n",
      "240/240 [==============================] - 0s 672us/step - loss: 2.0163 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01186: loss did not improve from 2.01539\n",
      "Epoch 1187/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 2.0162 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01187: loss did not improve from 2.01539\n",
      "Epoch 1188/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0169 - accuracy: 0.3542\n",
      "\n",
      "Epoch 01188: loss did not improve from 2.01539\n",
      "Epoch 1189/2000\n",
      "240/240 [==============================] - 0s 833us/step - loss: 2.0164 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01189: loss did not improve from 2.01539\n",
      "Epoch 1190/2000\n",
      "240/240 [==============================] - 0s 822us/step - loss: 2.0164 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01190: loss did not improve from 2.01539\n",
      "Epoch 1191/2000\n",
      "240/240 [==============================] - 0s 797us/step - loss: 2.0162 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01191: loss did not improve from 2.01539\n",
      "Epoch 1192/2000\n",
      "240/240 [==============================] - 0s 771us/step - loss: 2.0160 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01192: loss did not improve from 2.01539\n",
      "Epoch 1193/2000\n",
      "240/240 [==============================] - 0s 648us/step - loss: 2.0162 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01193: loss did not improve from 2.01539\n",
      "Epoch 1194/2000\n",
      "240/240 [==============================] - 0s 649us/step - loss: 2.0160 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01194: loss did not improve from 2.01539\n",
      "Epoch 1195/2000\n",
      "240/240 [==============================] - 0s 656us/step - loss: 2.0163 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01195: loss did not improve from 2.01539\n",
      "Epoch 1196/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0162 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01196: loss did not improve from 2.01539\n",
      "Epoch 1197/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0167 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01197: loss did not improve from 2.01539\n",
      "Epoch 1198/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 2.0166 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01198: loss did not improve from 2.01539\n",
      "Epoch 1199/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 2.0178 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01199: loss did not improve from 2.01539\n",
      "Epoch 1200/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 2.0165 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01200: loss did not improve from 2.01539\n",
      "Epoch 1201/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0358 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01201: loss did not improve from 2.01539\n",
      "Epoch 1202/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 2.0315 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01202: loss did not improve from 2.01539\n",
      "Epoch 1203/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 2.3713 - accuracy: 0.3125\n",
      "\n",
      "Epoch 01203: loss did not improve from 2.01539\n",
      "Epoch 1204/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0723 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01204: loss did not improve from 2.01539\n",
      "Epoch 1205/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 691us/step - loss: 2.0182 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01205: loss did not improve from 2.01539\n",
      "Epoch 1206/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0173 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01206: loss did not improve from 2.01539\n",
      "Epoch 1207/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 2.0170 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01207: loss did not improve from 2.01539\n",
      "Epoch 1208/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 2.0170 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01208: loss did not improve from 2.01539\n",
      "Epoch 1209/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0168 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01209: loss did not improve from 2.01539\n",
      "Epoch 1210/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0166 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01210: loss did not improve from 2.01539\n",
      "Epoch 1211/2000\n",
      "240/240 [==============================] - 0s 628us/step - loss: 2.0166 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01211: loss did not improve from 2.01539\n",
      "Epoch 1212/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 2.0165 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01212: loss did not improve from 2.01539\n",
      "Epoch 1213/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0164 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01213: loss did not improve from 2.01539\n",
      "Epoch 1214/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 2.0166 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01214: loss did not improve from 2.01539\n",
      "Epoch 1215/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 2.0165 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01215: loss did not improve from 2.01539\n",
      "Epoch 1216/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 2.0164 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01216: loss did not improve from 2.01539\n",
      "Epoch 1217/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0162 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01217: loss did not improve from 2.01539\n",
      "Epoch 1218/2000\n",
      "240/240 [==============================] - 0s 661us/step - loss: 2.0163 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01218: loss did not improve from 2.01539\n",
      "Epoch 1219/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 2.0164 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01219: loss did not improve from 2.01539\n",
      "Epoch 1220/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0161 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01220: loss did not improve from 2.01539\n",
      "Epoch 1221/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 2.0163 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01221: loss did not improve from 2.01539\n",
      "Epoch 1222/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 2.0161 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01222: loss did not improve from 2.01539\n",
      "Epoch 1223/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 2.0163 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01223: loss did not improve from 2.01539\n",
      "Epoch 1224/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0162 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01224: loss did not improve from 2.01539\n",
      "Epoch 1225/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0163 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01225: loss did not improve from 2.01539\n",
      "Epoch 1226/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 2.0161 - accuracy: 0.3542\n",
      "\n",
      "Epoch 01226: loss did not improve from 2.01539\n",
      "Epoch 1227/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 2.0164 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01227: loss did not improve from 2.01539\n",
      "Epoch 1228/2000\n",
      "240/240 [==============================] - 0s 672us/step - loss: 2.0163 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01228: loss did not improve from 2.01539\n",
      "Epoch 1229/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 2.0163 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01229: loss did not improve from 2.01539\n",
      "Epoch 1230/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 2.0162 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01230: loss did not improve from 2.01539\n",
      "Epoch 1231/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 2.0164 - accuracy: 0.3583\n",
      "\n",
      "Epoch 01231: loss did not improve from 2.01539\n",
      "Epoch 1232/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0162 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01232: loss did not improve from 2.01539\n",
      "Epoch 1233/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0164 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01233: loss did not improve from 2.01539\n",
      "Epoch 1234/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0164 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01234: loss did not improve from 2.01539\n",
      "Epoch 1235/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 2.0162 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01235: loss did not improve from 2.01539\n",
      "Epoch 1236/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 2.0163 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01236: loss did not improve from 2.01539\n",
      "Epoch 1237/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0163 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01237: loss did not improve from 2.01539\n",
      "Epoch 1238/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 2.0163 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01238: loss did not improve from 2.01539\n",
      "Epoch 1239/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 2.0161 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01239: loss did not improve from 2.01539\n",
      "Epoch 1240/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0161 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01240: loss did not improve from 2.01539\n",
      "Epoch 1241/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 2.0163 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01241: loss did not improve from 2.01539\n",
      "Epoch 1242/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 2.0175 - accuracy: 0.3625\n",
      "\n",
      "Epoch 01242: loss did not improve from 2.01539\n",
      "Epoch 1243/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 2.0190 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01243: loss did not improve from 2.01539\n",
      "Epoch 1244/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 2.0193 - accuracy: 0.3583\n",
      "\n",
      "Epoch 01244: loss did not improve from 2.01539\n",
      "Epoch 1245/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 2.0164 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01245: loss did not improve from 2.01539\n",
      "Epoch 1246/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 2.0162 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01246: loss did not improve from 2.01539\n",
      "Epoch 1247/2000\n",
      "240/240 [==============================] - 0s 667us/step - loss: 2.0158 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01247: loss did not improve from 2.01539\n",
      "Epoch 1248/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0160 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01248: loss did not improve from 2.01539\n",
      "Epoch 1249/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 2.0161 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01249: loss did not improve from 2.01539\n",
      "Epoch 1250/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0159 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01250: loss did not improve from 2.01539\n",
      "Epoch 1251/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 2.0159 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01251: loss did not improve from 2.01539\n",
      "Epoch 1252/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.0169 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01252: loss did not improve from 2.01539\n",
      "Epoch 1253/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 2.0164 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01253: loss did not improve from 2.01539\n",
      "Epoch 1254/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 2.0162 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01254: loss did not improve from 2.01539\n",
      "Epoch 1255/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 2.0160 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01255: loss did not improve from 2.01539\n",
      "Epoch 1256/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 2.0180 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01256: loss did not improve from 2.01539\n",
      "Epoch 1257/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0166 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01257: loss did not improve from 2.01539\n",
      "Epoch 1258/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 678us/step - loss: 2.0166 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01258: loss did not improve from 2.01539\n",
      "Epoch 1259/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0162 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01259: loss did not improve from 2.01539\n",
      "Epoch 1260/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0170 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01260: loss did not improve from 2.01539\n",
      "Epoch 1261/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 2.0163 - accuracy: 0.3083\n",
      "\n",
      "Epoch 01261: loss did not improve from 2.01539\n",
      "Epoch 1262/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 2.0169 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01262: loss did not improve from 2.01539\n",
      "Epoch 1263/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 2.0166 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01263: loss did not improve from 2.01539\n",
      "Epoch 1264/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0162 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01264: loss did not improve from 2.01539\n",
      "Epoch 1265/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 2.0162 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01265: loss did not improve from 2.01539\n",
      "Epoch 1266/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 2.0164 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01266: loss did not improve from 2.01539\n",
      "Epoch 1267/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 2.0272 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01267: loss did not improve from 2.01539\n",
      "Epoch 1268/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 2.0169 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01268: loss did not improve from 2.01539\n",
      "Epoch 1269/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 2.0173 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01269: loss did not improve from 2.01539\n",
      "Epoch 1270/2000\n",
      "240/240 [==============================] - 0s 666us/step - loss: 2.0164 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01270: loss did not improve from 2.01539\n",
      "Epoch 1271/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 2.0161 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01271: loss did not improve from 2.01539\n",
      "Epoch 1272/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 2.0164 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01272: loss did not improve from 2.01539\n",
      "Epoch 1273/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0163 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01273: loss did not improve from 2.01539\n",
      "Epoch 1274/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 2.0163 - accuracy: 0.3125\n",
      "\n",
      "Epoch 01274: loss did not improve from 2.01539\n",
      "Epoch 1275/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0162 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01275: loss did not improve from 2.01539\n",
      "Epoch 1276/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 2.0160 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01276: loss did not improve from 2.01539\n",
      "Epoch 1277/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 2.0158 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01277: loss did not improve from 2.01539\n",
      "Epoch 1278/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 2.0161 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01278: loss did not improve from 2.01539\n",
      "Epoch 1279/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 2.0166 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01279: loss did not improve from 2.01539\n",
      "Epoch 1280/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0162 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01280: loss did not improve from 2.01539\n",
      "Epoch 1281/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 2.0163 - accuracy: 0.3542\n",
      "\n",
      "Epoch 01281: loss did not improve from 2.01539\n",
      "Epoch 1282/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 2.0161 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01282: loss did not improve from 2.01539\n",
      "Epoch 1283/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 2.0162 - accuracy: 0.3042\n",
      "\n",
      "Epoch 01283: loss did not improve from 2.01539\n",
      "Epoch 1284/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0165 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01284: loss did not improve from 2.01539\n",
      "Epoch 1285/2000\n",
      "240/240 [==============================] - 0s 811us/step - loss: 2.0160 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01285: loss did not improve from 2.01539\n",
      "Epoch 1286/2000\n",
      "240/240 [==============================] - 0s 839us/step - loss: 2.0161 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01286: loss did not improve from 2.01539\n",
      "Epoch 1287/2000\n",
      "240/240 [==============================] - 0s 817us/step - loss: 2.0159 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01287: loss did not improve from 2.01539\n",
      "Epoch 1288/2000\n",
      "240/240 [==============================] - 0s 780us/step - loss: 2.0164 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01288: loss did not improve from 2.01539\n",
      "Epoch 1289/2000\n",
      "240/240 [==============================] - 0s 643us/step - loss: 2.0170 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01289: loss did not improve from 2.01539\n",
      "Epoch 1290/2000\n",
      "240/240 [==============================] - 0s 620us/step - loss: 2.0162 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01290: loss did not improve from 2.01539\n",
      "Epoch 1291/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 2.0161 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01291: loss did not improve from 2.01539\n",
      "Epoch 1292/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 2.0157 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01292: loss did not improve from 2.01539\n",
      "Epoch 1293/2000\n",
      "240/240 [==============================] - 0s 637us/step - loss: 2.0164 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01293: loss did not improve from 2.01539\n",
      "Epoch 1294/2000\n",
      "240/240 [==============================] - 0s 631us/step - loss: 2.0162 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01294: loss did not improve from 2.01539\n",
      "Epoch 1295/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 2.0167 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01295: loss did not improve from 2.01539\n",
      "Epoch 1296/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 2.0162 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01296: loss did not improve from 2.01539\n",
      "Epoch 1297/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 2.0162 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01297: loss did not improve from 2.01539\n",
      "Epoch 1298/2000\n",
      "240/240 [==============================] - 0s 651us/step - loss: 2.0160 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01298: loss did not improve from 2.01539\n",
      "Epoch 1299/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 2.0159 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01299: loss did not improve from 2.01539\n",
      "Epoch 1300/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 2.0157 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01300: loss did not improve from 2.01539\n",
      "Epoch 1301/2000\n",
      "240/240 [==============================] - 0s 649us/step - loss: 2.0159 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01301: loss did not improve from 2.01539\n",
      "Epoch 1302/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 2.0162 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01302: loss did not improve from 2.01539\n",
      "Epoch 1303/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 2.0162 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01303: loss did not improve from 2.01539\n",
      "Epoch 1304/2000\n",
      "240/240 [==============================] - 0s 667us/step - loss: 2.0162 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01304: loss did not improve from 2.01539\n",
      "Epoch 1305/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 2.0160 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01305: loss did not improve from 2.01539\n",
      "Epoch 1306/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0160 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01306: loss did not improve from 2.01539\n",
      "Epoch 1307/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 2.0159 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01307: loss did not improve from 2.01539\n",
      "Epoch 1308/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0427 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01308: loss did not improve from 2.01539\n",
      "Epoch 1309/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 2.0171 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01309: loss did not improve from 2.01539\n",
      "Epoch 1310/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 2.0180 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01310: loss did not improve from 2.01539\n",
      "Epoch 1311/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 687us/step - loss: 2.0203 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01311: loss did not improve from 2.01539\n",
      "Epoch 1312/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0166 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01312: loss did not improve from 2.01539\n",
      "Epoch 1313/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 2.0163 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01313: loss did not improve from 2.01539\n",
      "Epoch 1314/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0162 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01314: loss did not improve from 2.01539\n",
      "Epoch 1315/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 2.0162 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01315: loss did not improve from 2.01539\n",
      "Epoch 1316/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0163 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01316: loss did not improve from 2.01539\n",
      "Epoch 1317/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 2.0162 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01317: loss did not improve from 2.01539\n",
      "Epoch 1318/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 2.0159 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01318: loss did not improve from 2.01539\n",
      "Epoch 1319/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0159 - accuracy: 0.3125\n",
      "\n",
      "Epoch 01319: loss did not improve from 2.01539\n",
      "Epoch 1320/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 2.0158 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01320: loss did not improve from 2.01539\n",
      "Epoch 1321/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0159 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01321: loss did not improve from 2.01539\n",
      "Epoch 1322/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 2.0160 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01322: loss did not improve from 2.01539\n",
      "Epoch 1323/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 2.1039 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01323: loss did not improve from 2.01539\n",
      "Epoch 1324/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 2.0178 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01324: loss did not improve from 2.01539\n",
      "Epoch 1325/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0163 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01325: loss did not improve from 2.01539\n",
      "Epoch 1326/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0162 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01326: loss did not improve from 2.01539\n",
      "Epoch 1327/2000\n",
      "240/240 [==============================] - 0s 632us/step - loss: 2.0161 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01327: loss did not improve from 2.01539\n",
      "Epoch 1328/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 2.0161 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01328: loss did not improve from 2.01539\n",
      "Epoch 1329/2000\n",
      "240/240 [==============================] - 0s 745us/step - loss: 2.0159 - accuracy: 0.3083\n",
      "\n",
      "Epoch 01329: loss did not improve from 2.01539\n",
      "Epoch 1330/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 2.0167 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01330: loss did not improve from 2.01539\n",
      "Epoch 1331/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 2.0160 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01331: loss did not improve from 2.01539\n",
      "Epoch 1332/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 2.0164 - accuracy: 0.3542\n",
      "\n",
      "Epoch 01332: loss did not improve from 2.01539\n",
      "Epoch 1333/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 2.0164 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01333: loss did not improve from 2.01539\n",
      "Epoch 1334/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 2.0164 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01334: loss did not improve from 2.01539\n",
      "Epoch 1335/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 2.0163 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01335: loss did not improve from 2.01539\n",
      "Epoch 1336/2000\n",
      "240/240 [==============================] - 0s 794us/step - loss: 2.0169 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01336: loss did not improve from 2.01539\n",
      "Epoch 1337/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0164 - accuracy: 0.3583\n",
      "\n",
      "Epoch 01337: loss did not improve from 2.01539\n",
      "Epoch 1338/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0185 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01338: loss did not improve from 2.01539\n",
      "Epoch 1339/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 2.0169 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01339: loss did not improve from 2.01539\n",
      "Epoch 1340/2000\n",
      "240/240 [==============================] - ETA: 0s - loss: 2.0037 - accuracy: 0.33 - 0s 712us/step - loss: 2.0161 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01340: loss did not improve from 2.01539\n",
      "Epoch 1341/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 2.0161 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01341: loss did not improve from 2.01539\n",
      "Epoch 1342/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.0159 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01342: loss did not improve from 2.01539\n",
      "Epoch 1343/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 2.0158 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01343: loss did not improve from 2.01539\n",
      "Epoch 1344/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.0161 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01344: loss did not improve from 2.01539\n",
      "Epoch 1345/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0160 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01345: loss did not improve from 2.01539\n",
      "Epoch 1346/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0160 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01346: loss did not improve from 2.01539\n",
      "Epoch 1347/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 2.0160 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01347: loss did not improve from 2.01539\n",
      "Epoch 1348/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0158 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01348: loss did not improve from 2.01539\n",
      "Epoch 1349/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0158 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01349: loss did not improve from 2.01539\n",
      "Epoch 1350/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.0159 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01350: loss did not improve from 2.01539\n",
      "Epoch 1351/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 2.0158 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01351: loss did not improve from 2.01539\n",
      "Epoch 1352/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0177 - accuracy: 0.3542\n",
      "\n",
      "Epoch 01352: loss did not improve from 2.01539\n",
      "Epoch 1353/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 2.0168 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01353: loss did not improve from 2.01539\n",
      "Epoch 1354/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 2.0332 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01354: loss did not improve from 2.01539\n",
      "Epoch 1355/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0166 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01355: loss did not improve from 2.01539\n",
      "Epoch 1356/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0164 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01356: loss did not improve from 2.01539\n",
      "Epoch 1357/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0161 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01357: loss did not improve from 2.01539\n",
      "Epoch 1358/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 2.0162 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01358: loss did not improve from 2.01539\n",
      "Epoch 1359/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0161 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01359: loss did not improve from 2.01539\n",
      "Epoch 1360/2000\n",
      "240/240 [==============================] - 0s 662us/step - loss: 2.0161 - accuracy: 0.3125\n",
      "\n",
      "Epoch 01360: loss did not improve from 2.01539\n",
      "Epoch 1361/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 2.0162 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01361: loss did not improve from 2.01539\n",
      "Epoch 1362/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 2.0164 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01362: loss did not improve from 2.01539\n",
      "Epoch 1363/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 2.0162 - accuracy: 0.3417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01363: loss did not improve from 2.01539\n",
      "Epoch 1364/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0160 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01364: loss did not improve from 2.01539\n",
      "Epoch 1365/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 2.0161 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01365: loss did not improve from 2.01539\n",
      "Epoch 1366/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 2.0175 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01366: loss did not improve from 2.01539\n",
      "Epoch 1367/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0221 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01367: loss did not improve from 2.01539\n",
      "Epoch 1368/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 2.0182 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01368: loss did not improve from 2.01539\n",
      "Epoch 1369/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 2.0179 - accuracy: 0.3042\n",
      "\n",
      "Epoch 01369: loss did not improve from 2.01539\n",
      "Epoch 1370/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0169 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01370: loss did not improve from 2.01539\n",
      "Epoch 1371/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 2.0158 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01371: loss did not improve from 2.01539\n",
      "Epoch 1372/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 2.0161 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01372: loss did not improve from 2.01539\n",
      "Epoch 1373/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 2.0158 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01373: loss did not improve from 2.01539\n",
      "Epoch 1374/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 2.0157 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01374: loss did not improve from 2.01539\n",
      "Epoch 1375/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0160 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01375: loss did not improve from 2.01539\n",
      "Epoch 1376/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 2.0158 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01376: loss did not improve from 2.01539\n",
      "Epoch 1377/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 2.0158 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01377: loss did not improve from 2.01539\n",
      "Epoch 1378/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 2.0160 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01378: loss did not improve from 2.01539\n",
      "Epoch 1379/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0159 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01379: loss did not improve from 2.01539\n",
      "Epoch 1380/2000\n",
      "240/240 [==============================] - 0s 667us/step - loss: 2.0158 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01380: loss did not improve from 2.01539\n",
      "Epoch 1381/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0157 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01381: loss did not improve from 2.01539\n",
      "Epoch 1382/2000\n",
      "240/240 [==============================] - 0s 794us/step - loss: 2.0161 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01382: loss did not improve from 2.01539\n",
      "Epoch 1383/2000\n",
      "240/240 [==============================] - 0s 813us/step - loss: 2.0157 - accuracy: 0.3625\n",
      "\n",
      "Epoch 01383: loss did not improve from 2.01539\n",
      "Epoch 1384/2000\n",
      "240/240 [==============================] - 0s 845us/step - loss: 2.0157 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01384: loss did not improve from 2.01539\n",
      "Epoch 1385/2000\n",
      "240/240 [==============================] - 0s 983us/step - loss: 2.0157 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01385: loss did not improve from 2.01539\n",
      "Epoch 1386/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 2.0161 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01386: loss did not improve from 2.01539\n",
      "Epoch 1387/2000\n",
      "240/240 [==============================] - 0s 697us/step - loss: 2.0157 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01387: loss did not improve from 2.01539\n",
      "Epoch 1388/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 2.0157 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01388: loss did not improve from 2.01539\n",
      "Epoch 1389/2000\n",
      "240/240 [==============================] - 0s 658us/step - loss: 2.0158 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01389: loss did not improve from 2.01539\n",
      "Epoch 1390/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 2.0155 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01390: loss did not improve from 2.01539\n",
      "Epoch 1391/2000\n",
      "240/240 [==============================] - 0s 643us/step - loss: 2.0485 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01391: loss did not improve from 2.01539\n",
      "Epoch 1392/2000\n",
      "240/240 [==============================] - 0s 624us/step - loss: 2.0184 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01392: loss did not improve from 2.01539\n",
      "Epoch 1393/2000\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 2.0203 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01393: loss did not improve from 2.01539\n",
      "Epoch 1394/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0170 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01394: loss did not improve from 2.01539\n",
      "Epoch 1395/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 2.0161 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01395: loss did not improve from 2.01539\n",
      "Epoch 1396/2000\n",
      "240/240 [==============================] - 0s 654us/step - loss: 2.0163 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01396: loss did not improve from 2.01539\n",
      "Epoch 1397/2000\n",
      "240/240 [==============================] - 0s 643us/step - loss: 2.0164 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01397: loss did not improve from 2.01539\n",
      "Epoch 1398/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 2.0164 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01398: loss did not improve from 2.01539\n",
      "Epoch 1399/2000\n",
      "240/240 [==============================] - 0s 639us/step - loss: 2.0160 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01399: loss did not improve from 2.01539\n",
      "Epoch 1400/2000\n",
      "240/240 [==============================] - 0s 776us/step - loss: 2.0160 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01400: loss did not improve from 2.01539\n",
      "Epoch 1401/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 2.0163 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01401: loss did not improve from 2.01539\n",
      "Epoch 1402/2000\n",
      "240/240 [==============================] - 0s 649us/step - loss: 2.0163 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01402: loss did not improve from 2.01539\n",
      "Epoch 1403/2000\n",
      "240/240 [==============================] - 0s 640us/step - loss: 2.0161 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01403: loss did not improve from 2.01539\n",
      "Epoch 1404/2000\n",
      "240/240 [==============================] - 0s 629us/step - loss: 2.0160 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01404: loss did not improve from 2.01539\n",
      "Epoch 1405/2000\n",
      "240/240 [==============================] - 0s 615us/step - loss: 2.0170 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01405: loss did not improve from 2.01539\n",
      "Epoch 1406/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0242 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01406: loss did not improve from 2.01539\n",
      "Epoch 1407/2000\n",
      "240/240 [==============================] - 0s 769us/step - loss: 2.0186 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01407: loss did not improve from 2.01539\n",
      "Epoch 1408/2000\n",
      "240/240 [==============================] - 0s 655us/step - loss: 2.0178 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01408: loss did not improve from 2.01539\n",
      "Epoch 1409/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 2.0170 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01409: loss did not improve from 2.01539\n",
      "Epoch 1410/2000\n",
      "240/240 [==============================] - 0s 837us/step - loss: 2.0165 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01410: loss did not improve from 2.01539\n",
      "Epoch 1411/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 2.0162 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01411: loss did not improve from 2.01539\n",
      "Epoch 1412/2000\n",
      "240/240 [==============================] - 0s 807us/step - loss: 2.0159 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01412: loss did not improve from 2.01539\n",
      "Epoch 1413/2000\n",
      "240/240 [==============================] - 0s 816us/step - loss: 2.0159 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01413: loss did not improve from 2.01539\n",
      "Epoch 1414/2000\n",
      "240/240 [==============================] - 0s 827us/step - loss: 2.0159 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01414: loss did not improve from 2.01539\n",
      "Epoch 1415/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 2.0160 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01415: loss did not improve from 2.01539\n",
      "Epoch 1416/2000\n",
      "240/240 [==============================] - 0s 796us/step - loss: 2.0158 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01416: loss did not improve from 2.01539\n",
      "Epoch 1417/2000\n",
      "240/240 [==============================] - 0s 852us/step - loss: 2.0158 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01417: loss did not improve from 2.01539\n",
      "Epoch 1418/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 2.0161 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01418: loss did not improve from 2.01539\n",
      "Epoch 1419/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 2.0159 - accuracy: 0.3583\n",
      "\n",
      "Epoch 01419: loss did not improve from 2.01539\n",
      "Epoch 1420/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0159 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01420: loss did not improve from 2.01539\n",
      "Epoch 1421/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 2.0159 - accuracy: 0.3125\n",
      "\n",
      "Epoch 01421: loss did not improve from 2.01539\n",
      "Epoch 1422/2000\n",
      "240/240 [==============================] - 0s 740us/step - loss: 2.0158 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01422: loss did not improve from 2.01539\n",
      "Epoch 1423/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 2.0169 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01423: loss did not improve from 2.01539\n",
      "Epoch 1424/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 2.0162 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01424: loss did not improve from 2.01539\n",
      "Epoch 1425/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 2.0163 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01425: loss did not improve from 2.01539\n",
      "Epoch 1426/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 2.0162 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01426: loss did not improve from 2.01539\n",
      "Epoch 1427/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0161 - accuracy: 0.3583\n",
      "\n",
      "Epoch 01427: loss did not improve from 2.01539\n",
      "Epoch 1428/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 2.0160 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01428: loss did not improve from 2.01539\n",
      "Epoch 1429/2000\n",
      "240/240 [==============================] - 0s 667us/step - loss: 2.0163 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01429: loss did not improve from 2.01539\n",
      "Epoch 1430/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 2.0158 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01430: loss did not improve from 2.01539\n",
      "Epoch 1431/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 2.0158 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01431: loss did not improve from 2.01539\n",
      "Epoch 1432/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 2.0158 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01432: loss did not improve from 2.01539\n",
      "Epoch 1433/2000\n",
      "240/240 [==============================] - 0s 667us/step - loss: 2.0163 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01433: loss did not improve from 2.01539\n",
      "Epoch 1434/2000\n",
      "240/240 [==============================] - 0s 620us/step - loss: 2.0157 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01434: loss did not improve from 2.01539\n",
      "Epoch 1435/2000\n",
      "240/240 [==============================] - 0s 756us/step - loss: 2.0158 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01435: loss did not improve from 2.01539\n",
      "Epoch 1436/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 2.0157 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01436: loss did not improve from 2.01539\n",
      "Epoch 1437/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0160 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01437: loss did not improve from 2.01539\n",
      "Epoch 1438/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 2.0159 - accuracy: 0.3542\n",
      "\n",
      "Epoch 01438: loss did not improve from 2.01539\n",
      "Epoch 1439/2000\n",
      "240/240 [==============================] - 0s 718us/step - loss: 2.0163 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01439: loss did not improve from 2.01539\n",
      "Epoch 1440/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 2.0183 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01440: loss did not improve from 2.01539\n",
      "Epoch 1441/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 2.0198 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01441: loss did not improve from 2.01539\n",
      "Epoch 1442/2000\n",
      "240/240 [==============================] - 0s 697us/step - loss: 2.0176 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01442: loss did not improve from 2.01539\n",
      "Epoch 1443/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 2.0168 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01443: loss did not improve from 2.01539\n",
      "Epoch 1444/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0163 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01444: loss did not improve from 2.01539\n",
      "Epoch 1445/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0164 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01445: loss did not improve from 2.01539\n",
      "Epoch 1446/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 2.0158 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01446: loss did not improve from 2.01539\n",
      "Epoch 1447/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.0159 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01447: loss did not improve from 2.01539\n",
      "Epoch 1448/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0159 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01448: loss did not improve from 2.01539\n",
      "Epoch 1449/2000\n",
      "240/240 [==============================] - 0s 732us/step - loss: 2.0167 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01449: loss did not improve from 2.01539\n",
      "Epoch 1450/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 2.0166 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01450: loss did not improve from 2.01539\n",
      "Epoch 1451/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 2.0167 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01451: loss did not improve from 2.01539\n",
      "Epoch 1452/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0162 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01452: loss did not improve from 2.01539\n",
      "Epoch 1453/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0166 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01453: loss did not improve from 2.01539\n",
      "Epoch 1454/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 2.0160 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01454: loss did not improve from 2.01539\n",
      "Epoch 1455/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 2.0161 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01455: loss did not improve from 2.01539\n",
      "Epoch 1456/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.0190 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01456: loss did not improve from 2.01539\n",
      "Epoch 1457/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0178 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01457: loss did not improve from 2.01539\n",
      "Epoch 1458/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0166 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01458: loss did not improve from 2.01539\n",
      "Epoch 1459/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 2.0159 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01459: loss did not improve from 2.01539\n",
      "Epoch 1460/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 2.0159 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01460: loss did not improve from 2.01539\n",
      "Epoch 1461/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.0162 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01461: loss did not improve from 2.01539\n",
      "Epoch 1462/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 2.0161 - accuracy: 0.3083\n",
      "\n",
      "Epoch 01462: loss did not improve from 2.01539\n",
      "Epoch 1463/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 2.0190 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01463: loss did not improve from 2.01539\n",
      "Epoch 1464/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 2.0161 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01464: loss did not improve from 2.01539\n",
      "Epoch 1465/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0163 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01465: loss did not improve from 2.01539\n",
      "Epoch 1466/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0162 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01466: loss did not improve from 2.01539\n",
      "Epoch 1467/2000\n",
      "240/240 [==============================] - 0s 697us/step - loss: 2.0159 - accuracy: 0.31250s - loss: 2.0213 - accuracy: 0.31\n",
      "\n",
      "Epoch 01467: loss did not improve from 2.01539\n",
      "Epoch 1468/2000\n",
      "240/240 [==============================] - 0s 752us/step - loss: 2.0158 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01468: loss did not improve from 2.01539\n",
      "Epoch 1469/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 739us/step - loss: 2.0159 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01469: loss did not improve from 2.01539\n",
      "Epoch 1470/2000\n",
      "240/240 [==============================] - 0s 666us/step - loss: 2.0347 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01470: loss did not improve from 2.01539\n",
      "Epoch 1471/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 2.0186 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01471: loss did not improve from 2.01539\n",
      "Epoch 1472/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 2.0174 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01472: loss did not improve from 2.01539\n",
      "Epoch 1473/2000\n",
      "240/240 [==============================] - 0s 751us/step - loss: 2.0170 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01473: loss did not improve from 2.01539\n",
      "Epoch 1474/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 2.0162 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01474: loss did not improve from 2.01539\n",
      "Epoch 1475/2000\n",
      "240/240 [==============================] - 0s 762us/step - loss: 2.0167 - accuracy: 0.3542\n",
      "\n",
      "Epoch 01475: loss did not improve from 2.01539\n",
      "Epoch 1476/2000\n",
      "240/240 [==============================] - 0s 935us/step - loss: 2.0163 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01476: loss did not improve from 2.01539\n",
      "Epoch 1477/2000\n",
      "240/240 [==============================] - 0s 808us/step - loss: 2.0158 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01477: loss did not improve from 2.01539\n",
      "Epoch 1478/2000\n",
      "240/240 [==============================] - 0s 801us/step - loss: 2.0161 - accuracy: 0.3542\n",
      "\n",
      "Epoch 01478: loss did not improve from 2.01539\n",
      "Epoch 1479/2000\n",
      "240/240 [==============================] - 0s 773us/step - loss: 2.0160 - accuracy: 0.3583\n",
      "\n",
      "Epoch 01479: loss did not improve from 2.01539\n",
      "Epoch 1480/2000\n",
      "240/240 [==============================] - 0s 648us/step - loss: 2.0159 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01480: loss did not improve from 2.01539\n",
      "Epoch 1481/2000\n",
      "240/240 [==============================] - 0s 624us/step - loss: 2.0159 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01481: loss did not improve from 2.01539\n",
      "Epoch 1482/2000\n",
      "240/240 [==============================] - 0s 650us/step - loss: 2.0160 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01482: loss did not improve from 2.01539\n",
      "Epoch 1483/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 2.0157 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01483: loss did not improve from 2.01539\n",
      "Epoch 1484/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 2.0157 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01484: loss did not improve from 2.01539\n",
      "Epoch 1485/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0157 - accuracy: 0.3583\n",
      "\n",
      "Epoch 01485: loss did not improve from 2.01539\n",
      "Epoch 1486/2000\n",
      "240/240 [==============================] - 0s 672us/step - loss: 2.0158 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01486: loss did not improve from 2.01539\n",
      "Epoch 1487/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0157 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01487: loss did not improve from 2.01539\n",
      "Epoch 1488/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0160 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01488: loss did not improve from 2.01539\n",
      "Epoch 1489/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 2.0168 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01489: loss did not improve from 2.01539\n",
      "Epoch 1490/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0159 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01490: loss did not improve from 2.01539\n",
      "Epoch 1491/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0157 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01491: loss did not improve from 2.01539\n",
      "Epoch 1492/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 2.0158 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01492: loss did not improve from 2.01539\n",
      "Epoch 1493/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 2.0156 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01493: loss did not improve from 2.01539\n",
      "Epoch 1494/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 2.0158 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01494: loss did not improve from 2.01539\n",
      "Epoch 1495/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 2.0157 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01495: loss did not improve from 2.01539\n",
      "Epoch 1496/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 2.0156 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01496: loss did not improve from 2.01539\n",
      "Epoch 1497/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 2.0159 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01497: loss did not improve from 2.01539\n",
      "Epoch 1498/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 2.0157 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01498: loss did not improve from 2.01539\n",
      "Epoch 1499/2000\n",
      "240/240 [==============================] - 0s 733us/step - loss: 2.0156 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01499: loss did not improve from 2.01539\n",
      "Epoch 1500/2000\n",
      "240/240 [==============================] - 0s 649us/step - loss: 2.0177 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01500: loss did not improve from 2.01539\n",
      "Epoch 1501/2000\n",
      "240/240 [==============================] - 0s 629us/step - loss: 2.0163 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01501: loss did not improve from 2.01539\n",
      "Epoch 1502/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 2.0161 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01502: loss did not improve from 2.01539\n",
      "Epoch 1503/2000\n",
      "240/240 [==============================] - 0s 624us/step - loss: 2.0163 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01503: loss did not improve from 2.01539\n",
      "Epoch 1504/2000\n",
      "240/240 [==============================] - 0s 637us/step - loss: 2.0159 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01504: loss did not improve from 2.01539\n",
      "Epoch 1505/2000\n",
      "240/240 [==============================] - 0s 615us/step - loss: 2.0158 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01505: loss did not improve from 2.01539\n",
      "Epoch 1506/2000\n",
      "240/240 [==============================] - 0s 645us/step - loss: 2.0157 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01506: loss did not improve from 2.01539\n",
      "Epoch 1507/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0159 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01507: loss did not improve from 2.01539\n",
      "Epoch 1508/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0157 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01508: loss did not improve from 2.01539\n",
      "Epoch 1509/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0171 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01509: loss did not improve from 2.01539\n",
      "Epoch 1510/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 2.0165 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01510: loss did not improve from 2.01539\n",
      "Epoch 1511/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0158 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01511: loss did not improve from 2.01539\n",
      "Epoch 1512/2000\n",
      "240/240 [==============================] - 0s 659us/step - loss: 2.0165 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01512: loss did not improve from 2.01539\n",
      "Epoch 1513/2000\n",
      "240/240 [==============================] - 0s 628us/step - loss: 2.0179 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01513: loss did not improve from 2.01539\n",
      "Epoch 1514/2000\n",
      "240/240 [==============================] - 0s 613us/step - loss: 2.0160 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01514: loss did not improve from 2.01539\n",
      "Epoch 1515/2000\n",
      "240/240 [==============================] - 0s 629us/step - loss: 2.0155 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01515: loss did not improve from 2.01539\n",
      "Epoch 1516/2000\n",
      "240/240 [==============================] - 0s 635us/step - loss: 2.0157 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01516: loss did not improve from 2.01539\n",
      "Epoch 1517/2000\n",
      "240/240 [==============================] - 0s 664us/step - loss: 2.0156 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01517: loss did not improve from 2.01539\n",
      "Epoch 1518/2000\n",
      "240/240 [==============================] - 0s 623us/step - loss: 2.0158 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01518: loss did not improve from 2.01539\n",
      "Epoch 1519/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 2.0159 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01519: loss did not improve from 2.01539\n",
      "Epoch 1520/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 2.0998 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01520: loss did not improve from 2.01539\n",
      "Epoch 1521/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 2.0181 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01521: loss did not improve from 2.01539\n",
      "Epoch 1522/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 651us/step - loss: 2.0164 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01522: loss did not improve from 2.01539\n",
      "Epoch 1523/2000\n",
      "240/240 [==============================] - 0s 613us/step - loss: 2.0164 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01523: loss did not improve from 2.01539\n",
      "Epoch 1524/2000\n",
      "240/240 [==============================] - 0s 632us/step - loss: 2.0158 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01524: loss did not improve from 2.01539\n",
      "Epoch 1525/2000\n",
      "240/240 [==============================] - 0s 631us/step - loss: 2.0161 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01525: loss did not improve from 2.01539\n",
      "Epoch 1526/2000\n",
      "240/240 [==============================] - 0s 817us/step - loss: 2.0156 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01526: loss did not improve from 2.01539\n",
      "Epoch 1527/2000\n",
      "240/240 [==============================] - 0s 917us/step - loss: 2.0155 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01527: loss did not improve from 2.01539\n",
      "Epoch 1528/2000\n",
      "240/240 [==============================] - 0s 802us/step - loss: 2.0200 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01528: loss did not improve from 2.01539\n",
      "Epoch 1529/2000\n",
      "240/240 [==============================] - 0s 757us/step - loss: 2.0446 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01529: loss did not improve from 2.01539\n",
      "Epoch 1530/2000\n",
      "240/240 [==============================] - 0s 881us/step - loss: 2.0187 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01530: loss did not improve from 2.01539\n",
      "Epoch 1531/2000\n",
      "240/240 [==============================] - 0s 759us/step - loss: 2.0166 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01531: loss did not improve from 2.01539\n",
      "Epoch 1532/2000\n",
      "240/240 [==============================] - 0s 744us/step - loss: 2.0163 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01532: loss did not improve from 2.01539\n",
      "Epoch 1533/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 2.0160 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01533: loss did not improve from 2.01539\n",
      "Epoch 1534/2000\n",
      "240/240 [==============================] - 0s 757us/step - loss: 2.0159 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01534: loss did not improve from 2.01539\n",
      "Epoch 1535/2000\n",
      "240/240 [==============================] - 0s 773us/step - loss: 2.0159 - accuracy: 0.3542\n",
      "\n",
      "Epoch 01535: loss did not improve from 2.01539\n",
      "Epoch 1536/2000\n",
      "240/240 [==============================] - 0s 804us/step - loss: 2.0159 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01536: loss did not improve from 2.01539\n",
      "Epoch 1537/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 2.0160 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01537: loss did not improve from 2.01539\n",
      "Epoch 1538/2000\n",
      "240/240 [==============================] - 0s 793us/step - loss: 2.0159 - accuracy: 0.3083\n",
      "\n",
      "Epoch 01538: loss did not improve from 2.01539\n",
      "Epoch 1539/2000\n",
      "240/240 [==============================] - 0s 798us/step - loss: 2.0159 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01539: loss did not improve from 2.01539\n",
      "Epoch 1540/2000\n",
      "240/240 [==============================] - 0s 773us/step - loss: 2.0157 - accuracy: 0.3125\n",
      "\n",
      "Epoch 01540: loss did not improve from 2.01539\n",
      "Epoch 1541/2000\n",
      "240/240 [==============================] - 0s 806us/step - loss: 2.0162 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01541: loss did not improve from 2.01539\n",
      "Epoch 1542/2000\n",
      "240/240 [==============================] - 0s 797us/step - loss: 2.0155 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01542: loss did not improve from 2.01539\n",
      "Epoch 1543/2000\n",
      "240/240 [==============================] - 0s 644us/step - loss: 2.3836 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01543: loss did not improve from 2.01539\n",
      "Epoch 1544/2000\n",
      "240/240 [==============================] - 0s 627us/step - loss: 2.0299 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01544: loss did not improve from 2.01539\n",
      "Epoch 1545/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 2.0168 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01545: loss did not improve from 2.01539\n",
      "Epoch 1546/2000\n",
      "240/240 [==============================] - 0s 635us/step - loss: 2.0161 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01546: loss did not improve from 2.01539\n",
      "Epoch 1547/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 2.0160 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01547: loss did not improve from 2.01539\n",
      "Epoch 1548/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 2.0158 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01548: loss did not improve from 2.01539\n",
      "Epoch 1549/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 2.0154 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01549: loss did not improve from 2.01539\n",
      "Epoch 1550/2000\n",
      "240/240 [==============================] - 0s 737us/step - loss: 2.0155 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01550: loss did not improve from 2.01539\n",
      "Epoch 1551/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 2.0157 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01551: loss did not improve from 2.01539\n",
      "Epoch 1552/2000\n",
      "240/240 [==============================] - 0s 618us/step - loss: 2.0156 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01552: loss did not improve from 2.01539\n",
      "Epoch 1553/2000\n",
      "240/240 [==============================] - 0s 614us/step - loss: 2.0156 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01553: loss did not improve from 2.01539\n",
      "Epoch 1554/2000\n",
      "240/240 [==============================] - 0s 663us/step - loss: 2.0155 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01554: loss did not improve from 2.01539\n",
      "Epoch 1555/2000\n",
      "240/240 [==============================] - 0s 788us/step - loss: 2.0163 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01555: loss did not improve from 2.01539\n",
      "Epoch 1556/2000\n",
      "240/240 [==============================] - 0s 631us/step - loss: 2.0158 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01556: loss did not improve from 2.01539\n",
      "Epoch 1557/2000\n",
      "240/240 [==============================] - 0s 647us/step - loss: 2.0156 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01557: loss did not improve from 2.01539\n",
      "Epoch 1558/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0156 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01558: loss did not improve from 2.01539\n",
      "Epoch 1559/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 2.0156 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01559: loss did not improve from 2.01539\n",
      "Epoch 1560/2000\n",
      "240/240 [==============================] - 0s 762us/step - loss: 2.0155 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01560: loss did not improve from 2.01539\n",
      "Epoch 1561/2000\n",
      "240/240 [==============================] - 0s 757us/step - loss: 2.0157 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01561: loss did not improve from 2.01539\n",
      "Epoch 1562/2000\n",
      "240/240 [==============================] - 0s 618us/step - loss: 2.0156 - accuracy: 0.3625\n",
      "\n",
      "Epoch 01562: loss did not improve from 2.01539\n",
      "Epoch 1563/2000\n",
      "240/240 [==============================] - 0s 667us/step - loss: 2.0156 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01563: loss did not improve from 2.01539\n",
      "Epoch 1564/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 2.0156 - accuracy: 0.3125\n",
      "\n",
      "Epoch 01564: loss did not improve from 2.01539\n",
      "Epoch 1565/2000\n",
      "240/240 [==============================] - 0s 766us/step - loss: 2.0171 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01565: loss did not improve from 2.01539\n",
      "Epoch 1566/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 2.0164 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01566: loss did not improve from 2.01539\n",
      "Epoch 1567/2000\n",
      "240/240 [==============================] - 0s 652us/step - loss: 2.0156 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01567: loss did not improve from 2.01539\n",
      "Epoch 1568/2000\n",
      "240/240 [==============================] - 0s 643us/step - loss: 2.0156 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01568: loss did not improve from 2.01539\n",
      "Epoch 1569/2000\n",
      "240/240 [==============================] - 0s 640us/step - loss: 2.0157 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01569: loss did not improve from 2.01539\n",
      "Epoch 1570/2000\n",
      "240/240 [==============================] - 0s 608us/step - loss: 2.0155 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01570: loss did not improve from 2.01539\n",
      "Epoch 1571/2000\n",
      "240/240 [==============================] - 0s 963us/step - loss: 2.0155 - accuracy: 0.3542\n",
      "\n",
      "Epoch 01571: loss did not improve from 2.01539\n",
      "Epoch 1572/2000\n",
      "240/240 [==============================] - 0s 830us/step - loss: 2.0158 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01572: loss did not improve from 2.01539\n",
      "Epoch 1573/2000\n",
      "240/240 [==============================] - 0s 851us/step - loss: 2.0155 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01573: loss did not improve from 2.01539\n",
      "Epoch 1574/2000\n",
      "240/240 [==============================] - 0s 806us/step - loss: 2.0154 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01574: loss did not improve from 2.01539\n",
      "Epoch 1575/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 688us/step - loss: 2.0155 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01575: loss did not improve from 2.01539\n",
      "Epoch 1576/2000\n",
      "240/240 [==============================] - 0s 787us/step - loss: 2.0154 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01576: loss did not improve from 2.01539\n",
      "Epoch 1577/2000\n",
      "240/240 [==============================] - 0s 786us/step - loss: 2.0157 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01577: loss did not improve from 2.01539\n",
      "Epoch 1578/2000\n",
      "240/240 [==============================] - 0s 744us/step - loss: 2.0159 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01578: loss did not improve from 2.01539\n",
      "Epoch 1579/2000\n",
      "240/240 [==============================] - 0s 777us/step - loss: 2.0158 - accuracy: 0.3542\n",
      "\n",
      "Epoch 01579: loss did not improve from 2.01539\n",
      "Epoch 1580/2000\n",
      "240/240 [==============================] - 0s 747us/step - loss: 2.0158 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01580: loss did not improve from 2.01539\n",
      "Epoch 1581/2000\n",
      "240/240 [==============================] - 0s 777us/step - loss: 2.0158 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01581: loss did not improve from 2.01539\n",
      "Epoch 1582/2000\n",
      "240/240 [==============================] - 0s 810us/step - loss: 2.0157 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01582: loss did not improve from 2.01539\n",
      "Epoch 1583/2000\n",
      "240/240 [==============================] - 0s 839us/step - loss: 2.0156 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01583: loss did not improve from 2.01539\n",
      "Epoch 1584/2000\n",
      "240/240 [==============================] - 0s 805us/step - loss: 2.0157 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01584: loss did not improve from 2.01539\n",
      "Epoch 1585/2000\n",
      "240/240 [==============================] - 0s 648us/step - loss: 2.0154 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01585: loss did not improve from 2.01539\n",
      "Epoch 1586/2000\n",
      "240/240 [==============================] - 0s 618us/step - loss: 2.0157 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01586: loss did not improve from 2.01539\n",
      "Epoch 1587/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0162 - accuracy: 0.3542\n",
      "\n",
      "Epoch 01587: loss did not improve from 2.01539\n",
      "Epoch 1588/2000\n",
      "240/240 [==============================] - 0s 625us/step - loss: 2.0399 - accuracy: 0.3125\n",
      "\n",
      "Epoch 01588: loss did not improve from 2.01539\n",
      "Epoch 1589/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 2.0189 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01589: loss did not improve from 2.01539\n",
      "Epoch 1590/2000\n",
      "240/240 [==============================] - 0s 651us/step - loss: 2.0172 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01590: loss did not improve from 2.01539\n",
      "Epoch 1591/2000\n",
      "240/240 [==============================] - 0s 625us/step - loss: 2.0167 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01591: loss did not improve from 2.01539\n",
      "Epoch 1592/2000\n",
      "240/240 [==============================] - 0s 638us/step - loss: 2.0163 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01592: loss did not improve from 2.01539\n",
      "Epoch 1593/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 2.0171 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01593: loss did not improve from 2.01539\n",
      "Epoch 1594/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 2.0160 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01594: loss did not improve from 2.01539\n",
      "Epoch 1595/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 2.0167 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01595: loss did not improve from 2.01539\n",
      "Epoch 1596/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 2.0163 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01596: loss did not improve from 2.01539\n",
      "Epoch 1597/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 2.0162 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01597: loss did not improve from 2.01539\n",
      "Epoch 1598/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 2.0159 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01598: loss did not improve from 2.01539\n",
      "Epoch 1599/2000\n",
      "240/240 [==============================] - 0s 623us/step - loss: 2.0159 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01599: loss did not improve from 2.01539\n",
      "Epoch 1600/2000\n",
      "240/240 [==============================] - 0s 613us/step - loss: 2.0158 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01600: loss did not improve from 2.01539\n",
      "Epoch 1601/2000\n",
      "240/240 [==============================] - 0s 613us/step - loss: 2.0158 - accuracy: 0.3083\n",
      "\n",
      "Epoch 01601: loss did not improve from 2.01539\n",
      "Epoch 1602/2000\n",
      "240/240 [==============================] - 0s 629us/step - loss: 2.0160 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01602: loss did not improve from 2.01539\n",
      "Epoch 1603/2000\n",
      "240/240 [==============================] - 0s 642us/step - loss: 2.0158 - accuracy: 0.3542\n",
      "\n",
      "Epoch 01603: loss did not improve from 2.01539\n",
      "Epoch 1604/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0156 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01604: loss did not improve from 2.01539\n",
      "Epoch 1605/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 2.0156 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01605: loss did not improve from 2.01539\n",
      "Epoch 1606/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 2.0157 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01606: loss did not improve from 2.01539\n",
      "Epoch 1607/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 2.0158 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01607: loss did not improve from 2.01539\n",
      "Epoch 1608/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 2.0160 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01608: loss did not improve from 2.01539\n",
      "Epoch 1609/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 2.0159 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01609: loss did not improve from 2.01539\n",
      "Epoch 1610/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 2.0154 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01610: loss did not improve from 2.01539\n",
      "Epoch 1611/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0154 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01611: loss did not improve from 2.01539\n",
      "Epoch 1612/2000\n",
      "240/240 [==============================] - 0s 697us/step - loss: 2.0157 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01612: loss did not improve from 2.01539\n",
      "Epoch 1613/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 2.0155 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01613: loss did not improve from 2.01539\n",
      "Epoch 1614/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0153 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01614: loss improved from 2.01539 to 2.01528, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 1615/2000\n",
      "240/240 [==============================] - 0s 697us/step - loss: 2.0156 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01615: loss did not improve from 2.01528\n",
      "Epoch 1616/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0156 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01616: loss did not improve from 2.01528\n",
      "Epoch 1617/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0154 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01617: loss did not improve from 2.01528\n",
      "Epoch 1618/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 2.0155 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01618: loss did not improve from 2.01528\n",
      "Epoch 1619/2000\n",
      "240/240 [==============================] - 0s 756us/step - loss: 2.0154 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01619: loss did not improve from 2.01528\n",
      "Epoch 1620/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 2.0156 - accuracy: 0.3542\n",
      "\n",
      "Epoch 01620: loss did not improve from 2.01528\n",
      "Epoch 1621/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0154 - accuracy: 0.3542\n",
      "\n",
      "Epoch 01621: loss did not improve from 2.01528\n",
      "Epoch 1622/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 2.0156 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01622: loss did not improve from 2.01528\n",
      "Epoch 1623/2000\n",
      "240/240 [==============================] - 0s 697us/step - loss: 2.0161 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01623: loss did not improve from 2.01528\n",
      "Epoch 1624/2000\n",
      "240/240 [==============================] - 0s 741us/step - loss: 2.0163 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01624: loss did not improve from 2.01528\n",
      "Epoch 1625/2000\n",
      "240/240 [==============================] - 0s 749us/step - loss: 2.0173 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01625: loss did not improve from 2.01528\n",
      "Epoch 1626/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 2.0162 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01626: loss did not improve from 2.01528\n",
      "Epoch 1627/2000\n",
      "240/240 [==============================] - 0s 721us/step - loss: 2.0156 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01627: loss did not improve from 2.01528\n",
      "Epoch 1628/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 2.0154 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01628: loss did not improve from 2.01528\n",
      "Epoch 1629/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 2.0153 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01629: loss did not improve from 2.01528\n",
      "Epoch 1630/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 2.0160 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01630: loss did not improve from 2.01528\n",
      "Epoch 1631/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 2.0155 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01631: loss did not improve from 2.01528\n",
      "Epoch 1632/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 2.0152 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01632: loss improved from 2.01528 to 2.01523, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 1633/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0154 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01633: loss did not improve from 2.01523\n",
      "Epoch 1634/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0155 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01634: loss did not improve from 2.01523\n",
      "Epoch 1635/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 2.0154 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01635: loss did not improve from 2.01523\n",
      "Epoch 1636/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0155 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01636: loss did not improve from 2.01523\n",
      "Epoch 1637/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0154 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01637: loss did not improve from 2.01523\n",
      "Epoch 1638/2000\n",
      "240/240 [==============================] - 0s 745us/step - loss: 2.0154 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01638: loss did not improve from 2.01523\n",
      "Epoch 1639/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 2.0158 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01639: loss did not improve from 2.01523\n",
      "Epoch 1640/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 2.0156 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01640: loss did not improve from 2.01523\n",
      "Epoch 1641/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 2.0156 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01641: loss did not improve from 2.01523\n",
      "Epoch 1642/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 2.0154 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01642: loss did not improve from 2.01523\n",
      "Epoch 1643/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 2.0168 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01643: loss did not improve from 2.01523\n",
      "Epoch 1644/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 2.0154 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01644: loss did not improve from 2.01523\n",
      "Epoch 1645/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 2.0197 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01645: loss did not improve from 2.01523\n",
      "Epoch 1646/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 2.0168 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01646: loss did not improve from 2.01523\n",
      "Epoch 1647/2000\n",
      "240/240 [==============================] - 0s 734us/step - loss: 2.0156 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01647: loss did not improve from 2.01523\n",
      "Epoch 1648/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 2.0155 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01648: loss did not improve from 2.01523\n",
      "Epoch 1649/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.0156 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01649: loss did not improve from 2.01523\n",
      "Epoch 1650/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 2.0154 - accuracy: 0.3542\n",
      "\n",
      "Epoch 01650: loss did not improve from 2.01523\n",
      "Epoch 1651/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 2.0156 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01651: loss did not improve from 2.01523\n",
      "Epoch 1652/2000\n",
      "240/240 [==============================] - 0s 742us/step - loss: 2.0163 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01652: loss did not improve from 2.01523\n",
      "Epoch 1653/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 2.0160 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01653: loss did not improve from 2.01523\n",
      "Epoch 1654/2000\n",
      "240/240 [==============================] - 0s 737us/step - loss: 2.0983 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01654: loss did not improve from 2.01523\n",
      "Epoch 1655/2000\n",
      "240/240 [==============================] - 0s 778us/step - loss: 2.0818 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01655: loss did not improve from 2.01523\n",
      "Epoch 1656/2000\n",
      "240/240 [==============================] - 0s 718us/step - loss: 2.0171 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01656: loss did not improve from 2.01523\n",
      "Epoch 1657/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 2.0166 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01657: loss did not improve from 2.01523\n",
      "Epoch 1658/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 2.0173 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01658: loss did not improve from 2.01523\n",
      "Epoch 1659/2000\n",
      "240/240 [==============================] - 0s 747us/step - loss: 2.0158 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01659: loss did not improve from 2.01523\n",
      "Epoch 1660/2000\n",
      "240/240 [==============================] - 0s 752us/step - loss: 2.0157 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01660: loss did not improve from 2.01523\n",
      "Epoch 1661/2000\n",
      "240/240 [==============================] - 0s 745us/step - loss: 2.0155 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01661: loss did not improve from 2.01523\n",
      "Epoch 1662/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0157 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01662: loss did not improve from 2.01523\n",
      "Epoch 1663/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0155 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01663: loss did not improve from 2.01523\n",
      "Epoch 1664/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 2.0154 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01664: loss did not improve from 2.01523\n",
      "Epoch 1665/2000\n",
      "240/240 [==============================] - 0s 837us/step - loss: 2.0155 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01665: loss did not improve from 2.01523\n",
      "Epoch 1666/2000\n",
      "240/240 [==============================] - 0s 876us/step - loss: 2.0156 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01666: loss did not improve from 2.01523\n",
      "Epoch 1667/2000\n",
      "240/240 [==============================] - 0s 931us/step - loss: 2.0154 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01667: loss did not improve from 2.01523\n",
      "Epoch 1668/2000\n",
      "240/240 [==============================] - 0s 822us/step - loss: 2.0157 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01668: loss did not improve from 2.01523\n",
      "Epoch 1669/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 2.0155 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01669: loss did not improve from 2.01523\n",
      "Epoch 1670/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 2.0155 - accuracy: 0.3542\n",
      "\n",
      "Epoch 01670: loss did not improve from 2.01523\n",
      "Epoch 1671/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 2.0156 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01671: loss did not improve from 2.01523\n",
      "Epoch 1672/2000\n",
      "240/240 [==============================] - 0s 633us/step - loss: 2.0156 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01672: loss did not improve from 2.01523\n",
      "Epoch 1673/2000\n",
      "240/240 [==============================] - 0s 642us/step - loss: 2.0156 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01673: loss did not improve from 2.01523\n",
      "Epoch 1674/2000\n",
      "240/240 [==============================] - 0s 672us/step - loss: 2.0154 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01674: loss did not improve from 2.01523\n",
      "Epoch 1675/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 2.0259 - accuracy: 0.3542\n",
      "\n",
      "Epoch 01675: loss did not improve from 2.01523\n",
      "Epoch 1676/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0176 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01676: loss did not improve from 2.01523\n",
      "Epoch 1677/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 2.0164 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01677: loss did not improve from 2.01523\n",
      "Epoch 1678/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0159 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01678: loss did not improve from 2.01523\n",
      "Epoch 1679/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 697us/step - loss: 2.0157 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01679: loss did not improve from 2.01523\n",
      "Epoch 1680/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 2.0156 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01680: loss did not improve from 2.01523\n",
      "Epoch 1681/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 2.0156 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01681: loss did not improve from 2.01523\n",
      "Epoch 1682/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 2.0154 - accuracy: 0.3583\n",
      "\n",
      "Epoch 01682: loss did not improve from 2.01523\n",
      "Epoch 1683/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 2.0152 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01683: loss did not improve from 2.01523\n",
      "Epoch 1684/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 2.0155 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01684: loss did not improve from 2.01523\n",
      "Epoch 1685/2000\n",
      "240/240 [==============================] - 0s 733us/step - loss: 2.0153 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01685: loss did not improve from 2.01523\n",
      "Epoch 1686/2000\n",
      "240/240 [==============================] - 0s 595us/step - loss: 2.0159 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01686: loss did not improve from 2.01523\n",
      "Epoch 1687/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 2.0158 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01687: loss did not improve from 2.01523\n",
      "Epoch 1688/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 2.0156 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01688: loss did not improve from 2.01523\n",
      "Epoch 1689/2000\n",
      "240/240 [==============================] - 0s 631us/step - loss: 2.0153 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01689: loss did not improve from 2.01523\n",
      "Epoch 1690/2000\n",
      "240/240 [==============================] - 0s 628us/step - loss: 2.0153 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01690: loss did not improve from 2.01523\n",
      "Epoch 1691/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 2.0152 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01691: loss did not improve from 2.01523\n",
      "Epoch 1692/2000\n",
      "240/240 [==============================] - 0s 609us/step - loss: 2.0153 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01692: loss did not improve from 2.01523\n",
      "Epoch 1693/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 2.0153 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01693: loss did not improve from 2.01523\n",
      "Epoch 1694/2000\n",
      "240/240 [==============================] - 0s 638us/step - loss: 2.0154 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01694: loss did not improve from 2.01523\n",
      "Epoch 1695/2000\n",
      "240/240 [==============================] - 0s 827us/step - loss: 2.0156 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01695: loss did not improve from 2.01523\n",
      "Epoch 1696/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 2.0153 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01696: loss did not improve from 2.01523\n",
      "Epoch 1697/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 2.0161 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01697: loss did not improve from 2.01523\n",
      "Epoch 1698/2000\n",
      "240/240 [==============================] - 0s 640us/step - loss: 2.0156 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01698: loss did not improve from 2.01523\n",
      "Epoch 1699/2000\n",
      "240/240 [==============================] - 0s 635us/step - loss: 2.0155 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01699: loss did not improve from 2.01523\n",
      "Epoch 1700/2000\n",
      "240/240 [==============================] - 0s 644us/step - loss: 2.0152 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01700: loss did not improve from 2.01523\n",
      "Epoch 1701/2000\n",
      "240/240 [==============================] - 0s 672us/step - loss: 2.0153 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01701: loss did not improve from 2.01523\n",
      "Epoch 1702/2000\n",
      "240/240 [==============================] - 0s 666us/step - loss: 2.0154 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01702: loss did not improve from 2.01523\n",
      "Epoch 1703/2000\n",
      "240/240 [==============================] - 0s 624us/step - loss: 2.0153 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01703: loss did not improve from 2.01523\n",
      "Epoch 1704/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 2.0153 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01704: loss did not improve from 2.01523\n",
      "Epoch 1705/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 2.0154 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01705: loss did not improve from 2.01523\n",
      "Epoch 1706/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 2.0154 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01706: loss did not improve from 2.01523\n",
      "Epoch 1707/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 2.0155 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01707: loss did not improve from 2.01523\n",
      "Epoch 1708/2000\n",
      "240/240 [==============================] - 0s 661us/step - loss: 2.0152 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01708: loss improved from 2.01523 to 2.01521, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 1709/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0167 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01709: loss did not improve from 2.01521\n",
      "Epoch 1710/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 2.0159 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01710: loss did not improve from 2.01521\n",
      "Epoch 1711/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0164 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01711: loss did not improve from 2.01521\n",
      "Epoch 1712/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0176 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01712: loss did not improve from 2.01521\n",
      "Epoch 1713/2000\n",
      "240/240 [==============================] - 0s 667us/step - loss: 2.0155 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01713: loss did not improve from 2.01521\n",
      "Epoch 1714/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 2.0155 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01714: loss did not improve from 2.01521\n",
      "Epoch 1715/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0154 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01715: loss did not improve from 2.01521\n",
      "Epoch 1716/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.0153 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01716: loss did not improve from 2.01521\n",
      "Epoch 1717/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0154 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01717: loss did not improve from 2.01521\n",
      "Epoch 1718/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0165 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01718: loss did not improve from 2.01521\n",
      "Epoch 1719/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 2.0158 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01719: loss did not improve from 2.01521\n",
      "Epoch 1720/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 2.0160 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01720: loss did not improve from 2.01521\n",
      "Epoch 1721/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0158 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01721: loss did not improve from 2.01521\n",
      "Epoch 1722/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0173 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01722: loss did not improve from 2.01521\n",
      "Epoch 1723/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 2.0194 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01723: loss did not improve from 2.01521\n",
      "Epoch 1724/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0510 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01724: loss did not improve from 2.01521\n",
      "Epoch 1725/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.2595 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01725: loss did not improve from 2.01521\n",
      "Epoch 1726/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 2.0174 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01726: loss did not improve from 2.01521\n",
      "Epoch 1727/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0156 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01727: loss did not improve from 2.01521\n",
      "Epoch 1728/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 2.0154 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01728: loss did not improve from 2.01521\n",
      "Epoch 1729/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0154 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01729: loss did not improve from 2.01521\n",
      "Epoch 1730/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0155 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01730: loss did not improve from 2.01521\n",
      "Epoch 1731/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0154 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01731: loss did not improve from 2.01521\n",
      "Epoch 1732/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 2.0154 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01732: loss did not improve from 2.01521\n",
      "Epoch 1733/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 2.0153 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01733: loss did not improve from 2.01521\n",
      "Epoch 1734/2000\n",
      "240/240 [==============================] - 0s 627us/step - loss: 2.0152 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01734: loss did not improve from 2.01521\n",
      "Epoch 1735/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 2.0153 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01735: loss did not improve from 2.01521\n",
      "Epoch 1736/2000\n",
      "240/240 [==============================] - 0s 667us/step - loss: 2.0153 - accuracy: 0.3542\n",
      "\n",
      "Epoch 01736: loss did not improve from 2.01521\n",
      "Epoch 1737/2000\n",
      "240/240 [==============================] - 0s 790us/step - loss: 2.0152 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01737: loss improved from 2.01521 to 2.01516, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 1738/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 2.0152 - accuracy: 0.3542\n",
      "\n",
      "Epoch 01738: loss did not improve from 2.01516\n",
      "Epoch 1739/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 2.0152 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01739: loss did not improve from 2.01516\n",
      "Epoch 1740/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 2.0154 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01740: loss did not improve from 2.01516\n",
      "Epoch 1741/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 2.0151 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01741: loss improved from 2.01516 to 2.01506, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 1742/2000\n",
      "240/240 [==============================] - 0s 831us/step - loss: 2.0153 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01742: loss did not improve from 2.01506\n",
      "Epoch 1743/2000\n",
      "240/240 [==============================] - 0s 752us/step - loss: 2.0153 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01743: loss did not improve from 2.01506\n",
      "Epoch 1744/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 2.0155 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01744: loss did not improve from 2.01506\n",
      "Epoch 1745/2000\n",
      "240/240 [==============================] - 0s 885us/step - loss: 2.0153 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01745: loss did not improve from 2.01506\n",
      "Epoch 1746/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 2.0160 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01746: loss did not improve from 2.01506\n",
      "Epoch 1747/2000\n",
      "240/240 [==============================] - 0s 860us/step - loss: 2.0197 - accuracy: 0.3583\n",
      "\n",
      "Epoch 01747: loss did not improve from 2.01506\n",
      "Epoch 1748/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 2.0176 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01748: loss did not improve from 2.01506\n",
      "Epoch 1749/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0158 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01749: loss did not improve from 2.01506\n",
      "Epoch 1750/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 2.0154 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01750: loss did not improve from 2.01506\n",
      "Epoch 1751/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0157 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01751: loss did not improve from 2.01506\n",
      "Epoch 1752/2000\n",
      "240/240 [==============================] - 0s 885us/step - loss: 2.0155 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01752: loss did not improve from 2.01506\n",
      "Epoch 1753/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 2.0155 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01753: loss did not improve from 2.01506\n",
      "Epoch 1754/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0153 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01754: loss did not improve from 2.01506\n",
      "Epoch 1755/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0152 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01755: loss did not improve from 2.01506\n",
      "Epoch 1756/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 2.0152 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01756: loss did not improve from 2.01506\n",
      "Epoch 1757/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 2.0154 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01757: loss did not improve from 2.01506\n",
      "Epoch 1758/2000\n",
      "240/240 [==============================] - 0s 910us/step - loss: 2.0153 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01758: loss did not improve from 2.01506\n",
      "Epoch 1759/2000\n",
      "240/240 [==============================] - 0s 910us/step - loss: 2.0154 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01759: loss did not improve from 2.01506\n",
      "Epoch 1760/2000\n",
      "240/240 [==============================] - 0s 823us/step - loss: 2.0154 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01760: loss did not improve from 2.01506\n",
      "Epoch 1761/2000\n",
      "240/240 [==============================] - 0s 824us/step - loss: 2.0154 - accuracy: 0.3125\n",
      "\n",
      "Epoch 01761: loss did not improve from 2.01506\n",
      "Epoch 1762/2000\n",
      "240/240 [==============================] - 0s 819us/step - loss: 2.0153 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01762: loss did not improve from 2.01506\n",
      "Epoch 1763/2000\n",
      "240/240 [==============================] - 0s 798us/step - loss: 2.0153 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01763: loss did not improve from 2.01506\n",
      "Epoch 1764/2000\n",
      "240/240 [==============================] - 0s 629us/step - loss: 2.0157 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01764: loss did not improve from 2.01506\n",
      "Epoch 1765/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 2.0159 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01765: loss did not improve from 2.01506\n",
      "Epoch 1766/2000\n",
      "240/240 [==============================] - 0s 828us/step - loss: 2.0156 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01766: loss did not improve from 2.01506\n",
      "Epoch 1767/2000\n",
      "240/240 [==============================] - 0s 761us/step - loss: 2.0154 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01767: loss did not improve from 2.01506\n",
      "Epoch 1768/2000\n",
      "240/240 [==============================] - 0s 827us/step - loss: 2.0153 - accuracy: 0.3542\n",
      "\n",
      "Epoch 01768: loss did not improve from 2.01506\n",
      "Epoch 1769/2000\n",
      "240/240 [==============================] - 0s 747us/step - loss: 2.0282 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01769: loss did not improve from 2.01506\n",
      "Epoch 1770/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 2.0157 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01770: loss did not improve from 2.01506\n",
      "Epoch 1771/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 2.0155 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01771: loss did not improve from 2.01506\n",
      "Epoch 1772/2000\n",
      "240/240 [==============================] - 0s 893us/step - loss: 2.0167 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01772: loss did not improve from 2.01506\n",
      "Epoch 1773/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 2.0160 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01773: loss did not improve from 2.01506\n",
      "Epoch 1774/2000\n",
      "240/240 [==============================] - 0s 870us/step - loss: 2.0157 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01774: loss did not improve from 2.01506\n",
      "Epoch 1775/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 2.0156 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01775: loss did not improve from 2.01506\n",
      "Epoch 1776/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 2.0156 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01776: loss did not improve from 2.01506\n",
      "Epoch 1777/2000\n",
      "240/240 [==============================] - 0s 835us/step - loss: 2.0155 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01777: loss did not improve from 2.01506\n",
      "Epoch 1778/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 2.0155 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01778: loss did not improve from 2.01506\n",
      "Epoch 1779/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 2.0150 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01779: loss improved from 2.01506 to 2.01501, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 1780/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 2.0153 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01780: loss did not improve from 2.01501\n",
      "Epoch 1781/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 2.0152 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01781: loss did not improve from 2.01501\n",
      "Epoch 1782/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 2.0155 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01782: loss did not improve from 2.01501\n",
      "Epoch 1783/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 694us/step - loss: 2.0154 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01783: loss did not improve from 2.01501\n",
      "Epoch 1784/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 2.0154 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01784: loss did not improve from 2.01501\n",
      "Epoch 1785/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0162 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01785: loss did not improve from 2.01501\n",
      "Epoch 1786/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 2.0172 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01786: loss did not improve from 2.01501\n",
      "Epoch 1787/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0158 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01787: loss did not improve from 2.01501\n",
      "Epoch 1788/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 2.0158 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01788: loss did not improve from 2.01501\n",
      "Epoch 1789/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0154 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01789: loss did not improve from 2.01501\n",
      "Epoch 1790/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 2.0153 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01790: loss did not improve from 2.01501\n",
      "Epoch 1791/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 2.0153 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01791: loss did not improve from 2.01501\n",
      "Epoch 1792/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 2.0149 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01792: loss improved from 2.01501 to 2.01492, saving model to ./model/1000001000100010003.hdf5\n",
      "Epoch 1793/2000\n",
      "240/240 [==============================] - 0s 742us/step - loss: 2.0152 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01793: loss did not improve from 2.01492\n",
      "Epoch 1794/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 2.0158 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01794: loss did not improve from 2.01492\n",
      "Epoch 1795/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 2.0159 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01795: loss did not improve from 2.01492\n",
      "Epoch 1796/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 2.0156 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01796: loss did not improve from 2.01492\n",
      "Epoch 1797/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 2.0152 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01797: loss did not improve from 2.01492\n",
      "Epoch 1798/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 2.0151 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01798: loss did not improve from 2.01492\n",
      "Epoch 1799/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 2.0152 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01799: loss did not improve from 2.01492\n",
      "Epoch 1800/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 2.0152 - accuracy: 0.3125\n",
      "\n",
      "Epoch 01800: loss did not improve from 2.01492\n",
      "Epoch 1801/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 2.0152 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01801: loss did not improve from 2.01492\n",
      "Epoch 1802/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 2.0152 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01802: loss did not improve from 2.01492\n",
      "Epoch 1803/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 2.0151 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01803: loss did not improve from 2.01492\n",
      "Epoch 1804/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 2.0154 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01804: loss did not improve from 2.01492\n",
      "Epoch 1805/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 2.0158 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01805: loss did not improve from 2.01492\n",
      "Epoch 1806/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 2.0154 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01806: loss did not improve from 2.01492\n",
      "Epoch 1807/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.0153 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01807: loss did not improve from 2.01492\n",
      "Epoch 1808/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 2.0153 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01808: loss did not improve from 2.01492\n",
      "Epoch 1809/2000\n",
      "240/240 [==============================] - 0s 902us/step - loss: 2.0152 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01809: loss did not improve from 2.01492\n",
      "Epoch 1810/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 2.0154 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01810: loss did not improve from 2.01492\n",
      "Epoch 1811/2000\n",
      "240/240 [==============================] - 0s 771us/step - loss: 2.0154 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01811: loss did not improve from 2.01492\n",
      "Epoch 1812/2000\n",
      "240/240 [==============================] - 0s 883us/step - loss: 2.0154 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01812: loss did not improve from 2.01492\n",
      "Epoch 1813/2000\n",
      "240/240 [==============================] - 0s 697us/step - loss: 2.0153 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01813: loss did not improve from 2.01492\n",
      "Epoch 1814/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 2.0153 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01814: loss did not improve from 2.01492\n",
      "Epoch 1815/2000\n",
      "240/240 [==============================] - 0s 853us/step - loss: 2.0156 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01815: loss did not improve from 2.01492\n",
      "Epoch 1816/2000\n",
      "240/240 [==============================] - 0s 780us/step - loss: 2.0154 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01816: loss did not improve from 2.01492\n",
      "Epoch 1817/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 2.0168 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01817: loss did not improve from 2.01492\n",
      "Epoch 1818/2000\n",
      "240/240 [==============================] - 0s 663us/step - loss: 2.0172 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01818: loss did not improve from 2.01492\n",
      "Epoch 1819/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 2.0608 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01819: loss did not improve from 2.01492\n",
      "Epoch 1820/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0182 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01820: loss did not improve from 2.01492\n",
      "Epoch 1821/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 2.0161 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01821: loss did not improve from 2.01492\n",
      "Epoch 1822/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 2.0161 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01822: loss did not improve from 2.01492\n",
      "Epoch 1823/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 2.0157 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01823: loss did not improve from 2.01492\n",
      "Epoch 1824/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0155 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01824: loss did not improve from 2.01492\n",
      "Epoch 1825/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 2.0155 - accuracy: 0.3667\n",
      "\n",
      "Epoch 01825: loss did not improve from 2.01492\n",
      "Epoch 1826/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0169 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01826: loss did not improve from 2.01492\n",
      "Epoch 1827/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 2.0163 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01827: loss did not improve from 2.01492\n",
      "Epoch 1828/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 2.0183 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01828: loss did not improve from 2.01492\n",
      "Epoch 1829/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 2.0154 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01829: loss did not improve from 2.01492\n",
      "Epoch 1830/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 2.0156 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01830: loss did not improve from 2.01492\n",
      "Epoch 1831/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 2.0155 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01831: loss did not improve from 2.01492\n",
      "Epoch 1832/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0154 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01832: loss did not improve from 2.01492\n",
      "Epoch 1833/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0155 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01833: loss did not improve from 2.01492\n",
      "Epoch 1834/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0157 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01834: loss did not improve from 2.01492\n",
      "Epoch 1835/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 2.0218 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01835: loss did not improve from 2.01492\n",
      "Epoch 1836/2000\n",
      "240/240 [==============================] - 0s 672us/step - loss: 2.0174 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01836: loss did not improve from 2.01492\n",
      "Epoch 1837/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0160 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01837: loss did not improve from 2.01492\n",
      "Epoch 1838/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 2.2699 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01838: loss did not improve from 2.01492\n",
      "Epoch 1839/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 2.0168 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01839: loss did not improve from 2.01492\n",
      "Epoch 1840/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 2.0159 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01840: loss did not improve from 2.01492\n",
      "Epoch 1841/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0155 - accuracy: 0.3583\n",
      "\n",
      "Epoch 01841: loss did not improve from 2.01492\n",
      "Epoch 1842/2000\n",
      "240/240 [==============================] - 0s 742us/step - loss: 2.0158 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01842: loss did not improve from 2.01492\n",
      "Epoch 1843/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0158 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01843: loss did not improve from 2.01492\n",
      "Epoch 1844/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 2.0157 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01844: loss did not improve from 2.01492\n",
      "Epoch 1845/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 2.0157 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01845: loss did not improve from 2.01492\n",
      "Epoch 1846/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0156 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01846: loss did not improve from 2.01492\n",
      "Epoch 1847/2000\n",
      "240/240 [==============================] - 0s 667us/step - loss: 2.0155 - accuracy: 0.3542\n",
      "\n",
      "Epoch 01847: loss did not improve from 2.01492\n",
      "Epoch 1848/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0157 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01848: loss did not improve from 2.01492\n",
      "Epoch 1849/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 2.0157 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01849: loss did not improve from 2.01492\n",
      "Epoch 1850/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 2.0155 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01850: loss did not improve from 2.01492\n",
      "Epoch 1851/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 2.0155 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01851: loss did not improve from 2.01492\n",
      "Epoch 1852/2000\n",
      "240/240 [==============================] - 0s 792us/step - loss: 2.0154 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01852: loss did not improve from 2.01492\n",
      "Epoch 1853/2000\n",
      "240/240 [==============================] - 0s 870us/step - loss: 2.0155 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01853: loss did not improve from 2.01492\n",
      "Epoch 1854/2000\n",
      "240/240 [==============================] - 0s 837us/step - loss: 2.0155 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01854: loss did not improve from 2.01492\n",
      "Epoch 1855/2000\n",
      "240/240 [==============================] - 0s 800us/step - loss: 2.0159 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01855: loss did not improve from 2.01492\n",
      "Epoch 1856/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 2.0167 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01856: loss did not improve from 2.01492\n",
      "Epoch 1857/2000\n",
      "240/240 [==============================] - 0s 652us/step - loss: 2.0171 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01857: loss did not improve from 2.01492\n",
      "Epoch 1858/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0161 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01858: loss did not improve from 2.01492\n",
      "Epoch 1859/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 2.0157 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01859: loss did not improve from 2.01492\n",
      "Epoch 1860/2000\n",
      "240/240 [==============================] - 0s 740us/step - loss: 2.0153 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01860: loss did not improve from 2.01492\n",
      "Epoch 1861/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 2.0153 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01861: loss did not improve from 2.01492\n",
      "Epoch 1862/2000\n",
      "240/240 [==============================] - 0s 756us/step - loss: 2.0154 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01862: loss did not improve from 2.01492\n",
      "Epoch 1863/2000\n",
      "240/240 [==============================] - 0s 744us/step - loss: 2.0156 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01863: loss did not improve from 2.01492\n",
      "Epoch 1864/2000\n",
      "240/240 [==============================] - 0s 756us/step - loss: 2.0162 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01864: loss did not improve from 2.01492\n",
      "Epoch 1865/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 2.0156 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01865: loss did not improve from 2.01492\n",
      "Epoch 1866/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0154 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01866: loss did not improve from 2.01492\n",
      "Epoch 1867/2000\n",
      "240/240 [==============================] - 0s 739us/step - loss: 2.0156 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01867: loss did not improve from 2.01492\n",
      "Epoch 1868/2000\n",
      "240/240 [==============================] - 0s 776us/step - loss: 2.0434 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01868: loss did not improve from 2.01492\n",
      "Epoch 1869/2000\n",
      "240/240 [==============================] - 0s 726us/step - loss: 2.0162 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01869: loss did not improve from 2.01492\n",
      "Epoch 1870/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 2.0157 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01870: loss did not improve from 2.01492\n",
      "Epoch 1871/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0154 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01871: loss did not improve from 2.01492\n",
      "Epoch 1872/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 2.0153 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01872: loss did not improve from 2.01492\n",
      "Epoch 1873/2000\n",
      "240/240 [==============================] - 0s 750us/step - loss: 2.0154 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01873: loss did not improve from 2.01492\n",
      "Epoch 1874/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 2.0155 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01874: loss did not improve from 2.01492\n",
      "Epoch 1875/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 2.0184 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01875: loss did not improve from 2.01492\n",
      "Epoch 1876/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0161 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01876: loss did not improve from 2.01492\n",
      "Epoch 1877/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 2.0158 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01877: loss did not improve from 2.01492\n",
      "Epoch 1878/2000\n",
      "240/240 [==============================] - 0s 750us/step - loss: 2.0159 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01878: loss did not improve from 2.01492\n",
      "Epoch 1879/2000\n",
      "240/240 [==============================] - 0s 763us/step - loss: 2.0154 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01879: loss did not improve from 2.01492\n",
      "Epoch 1880/2000\n",
      "240/240 [==============================] - 0s 765us/step - loss: 2.0155 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01880: loss did not improve from 2.01492\n",
      "Epoch 1881/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 2.0155 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01881: loss did not improve from 2.01492\n",
      "Epoch 1882/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 2.0154 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01882: loss did not improve from 2.01492\n",
      "Epoch 1883/2000\n",
      "240/240 [==============================] - 0s 765us/step - loss: 2.0153 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01883: loss did not improve from 2.01492\n",
      "Epoch 1884/2000\n",
      "240/240 [==============================] - 0s 752us/step - loss: 2.0154 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01884: loss did not improve from 2.01492\n",
      "Epoch 1885/2000\n",
      "240/240 [==============================] - 0s 740us/step - loss: 2.0155 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01885: loss did not improve from 2.01492\n",
      "Epoch 1886/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.0153 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01886: loss did not improve from 2.01492\n",
      "Epoch 1887/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 2.0155 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01887: loss did not improve from 2.01492\n",
      "Epoch 1888/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 712us/step - loss: 2.0153 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01888: loss did not improve from 2.01492\n",
      "Epoch 1889/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 2.0157 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01889: loss did not improve from 2.01492\n",
      "Epoch 1890/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 2.0153 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01890: loss did not improve from 2.01492\n",
      "Epoch 1891/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 2.0160 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01891: loss did not improve from 2.01492\n",
      "Epoch 1892/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 2.0167 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01892: loss did not improve from 2.01492\n",
      "Epoch 1893/2000\n",
      "240/240 [==============================] - 0s 737us/step - loss: 2.0173 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01893: loss did not improve from 2.01492\n",
      "Epoch 1894/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 2.0158 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01894: loss did not improve from 2.01492\n",
      "Epoch 1895/2000\n",
      "240/240 [==============================] - 0s 754us/step - loss: 2.0154 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01895: loss did not improve from 2.01492\n",
      "Epoch 1896/2000\n",
      "240/240 [==============================] - 0s 762us/step - loss: 2.0155 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01896: loss did not improve from 2.01492\n",
      "Epoch 1897/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 2.0155 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01897: loss did not improve from 2.01492\n",
      "Epoch 1898/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0152 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01898: loss did not improve from 2.01492\n",
      "Epoch 1899/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 2.0153 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01899: loss did not improve from 2.01492\n",
      "Epoch 1900/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 2.0152 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01900: loss did not improve from 2.01492\n",
      "Epoch 1901/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0152 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01901: loss did not improve from 2.01492\n",
      "Epoch 1902/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 2.0156 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01902: loss did not improve from 2.01492\n",
      "Epoch 1903/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 2.0155 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01903: loss did not improve from 2.01492\n",
      "Epoch 1904/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 2.0499 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01904: loss did not improve from 2.01492\n",
      "Epoch 1905/2000\n",
      "240/240 [==============================] - 0s 757us/step - loss: 2.0394 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01905: loss did not improve from 2.01492\n",
      "Epoch 1906/2000\n",
      "240/240 [==============================] - 0s 752us/step - loss: 2.0198 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01906: loss did not improve from 2.01492\n",
      "Epoch 1907/2000\n",
      "240/240 [==============================] - ETA: 0s - loss: 1.9963 - accuracy: 0.33 - 0s 771us/step - loss: 2.0172 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01907: loss did not improve from 2.01492\n",
      "Epoch 1908/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 2.0164 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01908: loss did not improve from 2.01492\n",
      "Epoch 1909/2000\n",
      "240/240 [==============================] - 0s 760us/step - loss: 2.0162 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01909: loss did not improve from 2.01492\n",
      "Epoch 1910/2000\n",
      "240/240 [==============================] - 0s 741us/step - loss: 2.0160 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01910: loss did not improve from 2.01492\n",
      "Epoch 1911/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 2.0162 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01911: loss did not improve from 2.01492\n",
      "Epoch 1912/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 2.0164 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01912: loss did not improve from 2.01492\n",
      "Epoch 1913/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 2.0158 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01913: loss did not improve from 2.01492\n",
      "Epoch 1914/2000\n",
      "240/240 [==============================] - 0s 667us/step - loss: 2.0155 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01914: loss did not improve from 2.01492\n",
      "Epoch 1915/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 2.0153 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01915: loss did not improve from 2.01492\n",
      "Epoch 1916/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 2.0153 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01916: loss did not improve from 2.01492\n",
      "Epoch 1917/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 2.0153 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01917: loss did not improve from 2.01492\n",
      "Epoch 1918/2000\n",
      "240/240 [==============================] - 0s 718us/step - loss: 2.0151 - accuracy: 0.3125\n",
      "\n",
      "Epoch 01918: loss did not improve from 2.01492\n",
      "Epoch 1919/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 2.0155 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01919: loss did not improve from 2.01492\n",
      "Epoch 1920/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0158 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01920: loss did not improve from 2.01492\n",
      "Epoch 1921/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0157 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01921: loss did not improve from 2.01492\n",
      "Epoch 1922/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 2.0156 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01922: loss did not improve from 2.01492\n",
      "Epoch 1923/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0156 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01923: loss did not improve from 2.01492\n",
      "Epoch 1924/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0167 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01924: loss did not improve from 2.01492\n",
      "Epoch 1925/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 2.0152 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01925: loss did not improve from 2.01492\n",
      "Epoch 1926/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 2.0152 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01926: loss did not improve from 2.01492\n",
      "Epoch 1927/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 2.0156 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01927: loss did not improve from 2.01492\n",
      "Epoch 1928/2000\n",
      "240/240 [==============================] - 0s 666us/step - loss: 2.0152 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01928: loss did not improve from 2.01492\n",
      "Epoch 1929/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 2.0164 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01929: loss did not improve from 2.01492\n",
      "Epoch 1930/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 2.0160 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01930: loss did not improve from 2.01492\n",
      "Epoch 1931/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 2.0159 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01931: loss did not improve from 2.01492\n",
      "Epoch 1932/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 2.0153 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01932: loss did not improve from 2.01492\n",
      "Epoch 1933/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0152 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01933: loss did not improve from 2.01492\n",
      "Epoch 1934/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0152 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01934: loss did not improve from 2.01492\n",
      "Epoch 1935/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 2.0151 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01935: loss did not improve from 2.01492\n",
      "Epoch 1936/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0301 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01936: loss did not improve from 2.01492\n",
      "Epoch 1937/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.2505 - accuracy: 0.3042\n",
      "\n",
      "Epoch 01937: loss did not improve from 2.01492\n",
      "Epoch 1938/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 2.0199 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01938: loss did not improve from 2.01492\n",
      "Epoch 1939/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0172 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01939: loss did not improve from 2.01492\n",
      "Epoch 1940/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 2.0460 - accuracy: 0.3375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01940: loss did not improve from 2.01492\n",
      "Epoch 1941/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 2.0165 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01941: loss did not improve from 2.01492\n",
      "Epoch 1942/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 2.0161 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01942: loss did not improve from 2.01492\n",
      "Epoch 1943/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0157 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01943: loss did not improve from 2.01492\n",
      "Epoch 1944/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0155 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01944: loss did not improve from 2.01492\n",
      "Epoch 1945/2000\n",
      "240/240 [==============================] - 0s 804us/step - loss: 2.0155 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01945: loss did not improve from 2.01492\n",
      "Epoch 1946/2000\n",
      "240/240 [==============================] - 0s 880us/step - loss: 2.0154 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01946: loss did not improve from 2.01492\n",
      "Epoch 1947/2000\n",
      "240/240 [==============================] - 0s 813us/step - loss: 2.0154 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01947: loss did not improve from 2.01492\n",
      "Epoch 1948/2000\n",
      "240/240 [==============================] - 0s 826us/step - loss: 2.0167 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01948: loss did not improve from 2.01492\n",
      "Epoch 1949/2000\n",
      "240/240 [==============================] - 0s 647us/step - loss: 2.0155 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01949: loss did not improve from 2.01492\n",
      "Epoch 1950/2000\n",
      "240/240 [==============================] - 0s 646us/step - loss: 2.0154 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01950: loss did not improve from 2.01492\n",
      "Epoch 1951/2000\n",
      "240/240 [==============================] - 0s 649us/step - loss: 2.0154 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01951: loss did not improve from 2.01492\n",
      "Epoch 1952/2000\n",
      "240/240 [==============================] - 0s 631us/step - loss: 2.0154 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01952: loss did not improve from 2.01492\n",
      "Epoch 1953/2000\n",
      "240/240 [==============================] - 0s 633us/step - loss: 2.0155 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01953: loss did not improve from 2.01492\n",
      "Epoch 1954/2000\n",
      "240/240 [==============================] - 0s 643us/step - loss: 2.0154 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01954: loss did not improve from 2.01492\n",
      "Epoch 1955/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 2.0153 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01955: loss did not improve from 2.01492\n",
      "Epoch 1956/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0154 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01956: loss did not improve from 2.01492\n",
      "Epoch 1957/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 2.0152 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01957: loss did not improve from 2.01492\n",
      "Epoch 1958/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 2.0156 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01958: loss did not improve from 2.01492\n",
      "Epoch 1959/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0178 - accuracy: 0.3542\n",
      "\n",
      "Epoch 01959: loss did not improve from 2.01492\n",
      "Epoch 1960/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 2.0166 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01960: loss did not improve from 2.01492\n",
      "Epoch 1961/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 2.0153 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01961: loss did not improve from 2.01492\n",
      "Epoch 1962/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 2.0152 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01962: loss did not improve from 2.01492\n",
      "Epoch 1963/2000\n",
      "240/240 [==============================] - 0s 666us/step - loss: 2.0150 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01963: loss did not improve from 2.01492\n",
      "Epoch 1964/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 2.0151 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01964: loss did not improve from 2.01492\n",
      "Epoch 1965/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 2.0151 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01965: loss did not improve from 2.01492\n",
      "Epoch 1966/2000\n",
      "240/240 [==============================] - 0s 666us/step - loss: 2.0151 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01966: loss did not improve from 2.01492\n",
      "Epoch 1967/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 2.0151 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01967: loss did not improve from 2.01492\n",
      "Epoch 1968/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 2.0151 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01968: loss did not improve from 2.01492\n",
      "Epoch 1969/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 2.0151 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01969: loss did not improve from 2.01492\n",
      "Epoch 1970/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 2.0151 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01970: loss did not improve from 2.01492\n",
      "Epoch 1971/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0151 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01971: loss did not improve from 2.01492\n",
      "Epoch 1972/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 2.0152 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01972: loss did not improve from 2.01492\n",
      "Epoch 1973/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 2.0150 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01973: loss did not improve from 2.01492\n",
      "Epoch 1974/2000\n",
      "240/240 [==============================] - 0s 672us/step - loss: 2.0153 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01974: loss did not improve from 2.01492\n",
      "Epoch 1975/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0151 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01975: loss did not improve from 2.01492\n",
      "Epoch 1976/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 2.0161 - accuracy: 0.3250\n",
      "\n",
      "Epoch 01976: loss did not improve from 2.01492\n",
      "Epoch 1977/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 2.0153 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01977: loss did not improve from 2.01492\n",
      "Epoch 1978/2000\n",
      "240/240 [==============================] - 0s 633us/step - loss: 2.0151 - accuracy: 0.3167\n",
      "\n",
      "Epoch 01978: loss did not improve from 2.01492\n",
      "Epoch 1979/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 2.0154 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01979: loss did not improve from 2.01492\n",
      "Epoch 1980/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 2.0167 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01980: loss did not improve from 2.01492\n",
      "Epoch 1981/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 2.0245 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01981: loss did not improve from 2.01492\n",
      "Epoch 1982/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0164 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01982: loss did not improve from 2.01492\n",
      "Epoch 1983/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 2.0159 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01983: loss did not improve from 2.01492\n",
      "Epoch 1984/2000\n",
      "240/240 [==============================] - 0s 664us/step - loss: 2.0155 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01984: loss did not improve from 2.01492\n",
      "Epoch 1985/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 2.0155 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01985: loss did not improve from 2.01492\n",
      "Epoch 1986/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0154 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01986: loss did not improve from 2.01492\n",
      "Epoch 1987/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 2.0153 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01987: loss did not improve from 2.01492\n",
      "Epoch 1988/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 2.0153 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01988: loss did not improve from 2.01492\n",
      "Epoch 1989/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 2.0153 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01989: loss did not improve from 2.01492\n",
      "Epoch 1990/2000\n",
      "240/240 [==============================] - 0s 672us/step - loss: 2.0153 - accuracy: 0.3417\n",
      "\n",
      "Epoch 01990: loss did not improve from 2.01492\n",
      "Epoch 1991/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 2.0153 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01991: loss did not improve from 2.01492\n",
      "Epoch 1992/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 2.0152 - accuracy: 0.3375\n",
      "\n",
      "Epoch 01992: loss did not improve from 2.01492\n",
      "Epoch 1993/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 2.0153 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01993: loss did not improve from 2.01492\n",
      "Epoch 1994/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 2.0150 - accuracy: 0.3208\n",
      "\n",
      "Epoch 01994: loss did not improve from 2.01492\n",
      "Epoch 1995/2000\n",
      "240/240 [==============================] - 0s 732us/step - loss: 2.0150 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01995: loss did not improve from 2.01492\n",
      "Epoch 1996/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 2.0150 - accuracy: 0.3500\n",
      "\n",
      "Epoch 01996: loss did not improve from 2.01492\n",
      "Epoch 1997/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 2.0158 - accuracy: 0.3542\n",
      "\n",
      "Epoch 01997: loss did not improve from 2.01492\n",
      "Epoch 1998/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 2.0366 - accuracy: 0.3292\n",
      "\n",
      "Epoch 01998: loss did not improve from 2.01492\n",
      "Epoch 1999/2000\n",
      "240/240 [==============================] - 0s 718us/step - loss: 2.0183 - accuracy: 0.3458\n",
      "\n",
      "Epoch 01999: loss did not improve from 2.01492\n",
      "Epoch 2000/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 2.0159 - accuracy: 0.3292\n",
      "\n",
      "Epoch 02000: loss did not improve from 2.01492\n"
     ]
    }
   ],
   "source": [
    "readdata_and_savemodel(\"1000001000100010003.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 5.5959 - accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00001: loss improved from inf to 5.59586, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 2/2000\n",
      "240/240 [==============================] - 0s 831us/step - loss: 5.2250 - accuracy: 0.0292\n",
      "\n",
      "Epoch 00002: loss improved from 5.59586 to 5.22500, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 3/2000\n",
      "240/240 [==============================] - 0s 826us/step - loss: 4.1193 - accuracy: 0.1000\n",
      "\n",
      "Epoch 00003: loss improved from 5.22500 to 4.11933, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 4/2000\n",
      "240/240 [==============================] - 0s 808us/step - loss: 2.8861 - accuracy: 0.2542\n",
      "\n",
      "Epoch 00004: loss improved from 4.11933 to 2.88613, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 5/2000\n",
      "240/240 [==============================] - 0s 803us/step - loss: 2.2068 - accuracy: 0.3875\n",
      "\n",
      "Epoch 00005: loss improved from 2.88613 to 2.20680, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 6/2000\n",
      "240/240 [==============================] - 0s 834us/step - loss: 1.9195 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00006: loss improved from 2.20680 to 1.91955, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 7/2000\n",
      "240/240 [==============================] - 0s 746us/step - loss: 1.7838 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00007: loss improved from 1.91955 to 1.78384, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 8/2000\n",
      "240/240 [==============================] - 0s 746us/step - loss: 1.7028 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00008: loss improved from 1.78384 to 1.70280, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 9/2000\n",
      "240/240 [==============================] - 0s 756us/step - loss: 1.6366 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00009: loss improved from 1.70280 to 1.63663, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 10/2000\n",
      "240/240 [==============================] - 0s 764us/step - loss: 1.6285 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00010: loss improved from 1.63663 to 1.62846, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 11/2000\n",
      "240/240 [==============================] - 0s 766us/step - loss: 1.5826 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00011: loss improved from 1.62846 to 1.58265, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 12/2000\n",
      "240/240 [==============================] - 0s 774us/step - loss: 1.5702 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00012: loss improved from 1.58265 to 1.57016, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 13/2000\n",
      "240/240 [==============================] - 0s 810us/step - loss: 1.5532 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00013: loss improved from 1.57016 to 1.55316, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 14/2000\n",
      "240/240 [==============================] - 0s 783us/step - loss: 1.5344 - accuracy: 0.4625\n",
      "\n",
      "Epoch 00014: loss improved from 1.55316 to 1.53438, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 15/2000\n",
      "240/240 [==============================] - 0s 762us/step - loss: 1.5192 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00015: loss improved from 1.53438 to 1.51922, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 16/2000\n",
      "240/240 [==============================] - 0s 752us/step - loss: 1.5127 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00016: loss improved from 1.51922 to 1.51267, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 17/2000\n",
      "240/240 [==============================] - 0s 761us/step - loss: 1.4970 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00017: loss improved from 1.51267 to 1.49696, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 18/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 1.5004 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00018: loss did not improve from 1.49696\n",
      "Epoch 19/2000\n",
      "240/240 [==============================] - 0s 753us/step - loss: 1.4858 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00019: loss improved from 1.49696 to 1.48580, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 20/2000\n",
      "240/240 [==============================] - 0s 764us/step - loss: 1.4812 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00020: loss improved from 1.48580 to 1.48119, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 21/2000\n",
      "240/240 [==============================] - 0s 757us/step - loss: 1.4756 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00021: loss improved from 1.48119 to 1.47564, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 22/2000\n",
      "240/240 [==============================] - 0s 754us/step - loss: 1.4674 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00022: loss improved from 1.47564 to 1.46736, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 23/2000\n",
      "240/240 [==============================] - 0s 782us/step - loss: 1.4631 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00023: loss improved from 1.46736 to 1.46311, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 24/2000\n",
      "240/240 [==============================] - 0s 739us/step - loss: 1.4584 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00024: loss improved from 1.46311 to 1.45841, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 25/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 1.4457 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00025: loss improved from 1.45841 to 1.44572, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 26/2000\n",
      "240/240 [==============================] - 0s 754us/step - loss: 1.4412 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00026: loss improved from 1.44572 to 1.44119, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 27/2000\n",
      "240/240 [==============================] - 0s 751us/step - loss: 1.4388 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00027: loss improved from 1.44119 to 1.43883, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 28/2000\n",
      "240/240 [==============================] - 0s 764us/step - loss: 1.4314 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00028: loss improved from 1.43883 to 1.43143, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 29/2000\n",
      "240/240 [==============================] - 0s 734us/step - loss: 1.4266 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00029: loss improved from 1.43143 to 1.42657, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 30/2000\n",
      "240/240 [==============================] - 0s 721us/step - loss: 1.4196 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00030: loss improved from 1.42657 to 1.41957, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 31/2000\n",
      "240/240 [==============================] - 0s 788us/step - loss: 1.4197 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00031: loss did not improve from 1.41957\n",
      "Epoch 32/2000\n",
      "240/240 [==============================] - 0s 830us/step - loss: 1.4124 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00032: loss improved from 1.41957 to 1.41238, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 33/2000\n",
      "240/240 [==============================] - 0s 800us/step - loss: 1.4045 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00033: loss improved from 1.41238 to 1.40452, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 34/2000\n",
      "240/240 [==============================] - 0s 790us/step - loss: 1.4030 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00034: loss improved from 1.40452 to 1.40298, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 35/2000\n",
      "240/240 [==============================] - 0s 643us/step - loss: 1.4013 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00035: loss improved from 1.40298 to 1.40126, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 36/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.3991 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00036: loss improved from 1.40126 to 1.39912, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 37/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.3903 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00037: loss improved from 1.39912 to 1.39026, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 38/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.3878 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00038: loss improved from 1.39026 to 1.38781, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 39/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 1.3829 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00039: loss improved from 1.38781 to 1.38289, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 40/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 1.3843 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00040: loss did not improve from 1.38289\n",
      "Epoch 41/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.3763 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00041: loss improved from 1.38289 to 1.37632, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 42/2000\n",
      "240/240 [==============================] - 0s 661us/step - loss: 1.3755 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00042: loss improved from 1.37632 to 1.37552, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 43/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 1.3716 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00043: loss improved from 1.37552 to 1.37161, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 44/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.3690 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00044: loss improved from 1.37161 to 1.36897, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 45/2000\n",
      "240/240 [==============================] - 0s 721us/step - loss: 1.3630 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00045: loss improved from 1.36897 to 1.36298, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 46/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 1.3636 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00046: loss did not improve from 1.36298\n",
      "Epoch 47/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 1.3613 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00047: loss improved from 1.36298 to 1.36125, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 48/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.3544 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00048: loss improved from 1.36125 to 1.35445, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 49/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.3542 - accuracy: 0.4000\n",
      "\n",
      "Epoch 00049: loss improved from 1.35445 to 1.35415, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 50/2000\n",
      "240/240 [==============================] - 0s 738us/step - loss: 1.3526 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00050: loss improved from 1.35415 to 1.35262, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 51/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.3472 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00051: loss improved from 1.35262 to 1.34720, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 52/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.3488 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00052: loss did not improve from 1.34720\n",
      "Epoch 53/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.3413 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00053: loss improved from 1.34720 to 1.34127, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 54/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.3401 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00054: loss improved from 1.34127 to 1.34005, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 55/2000\n",
      "240/240 [==============================] - 0s 733us/step - loss: 1.3407 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00055: loss did not improve from 1.34005\n",
      "Epoch 56/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 1.3370 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00056: loss improved from 1.34005 to 1.33698, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 57/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.3324 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00057: loss improved from 1.33698 to 1.33242, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 58/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.3338 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00058: loss did not improve from 1.33242\n",
      "Epoch 59/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 1.3334 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00059: loss did not improve from 1.33242\n",
      "Epoch 60/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.3279 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00060: loss improved from 1.33242 to 1.32791, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 61/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 1.3284 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00061: loss did not improve from 1.32791\n",
      "Epoch 62/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.3264 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00062: loss improved from 1.32791 to 1.32639, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 63/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.3213 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00063: loss improved from 1.32639 to 1.32127, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 64/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 1.3208 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00064: loss improved from 1.32127 to 1.32078, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 65/2000\n",
      "240/240 [==============================] - 0s 722us/step - loss: 1.3216 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00065: loss did not improve from 1.32078\n",
      "Epoch 66/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 1.3174 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00066: loss improved from 1.32078 to 1.31744, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 67/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.3187 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00067: loss did not improve from 1.31744\n",
      "Epoch 68/2000\n",
      "240/240 [==============================] - 0s 721us/step - loss: 1.3124 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00068: loss improved from 1.31744 to 1.31241, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 69/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 1.3135 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00069: loss did not improve from 1.31241\n",
      "Epoch 70/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.3113 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00070: loss improved from 1.31241 to 1.31132, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 71/2000\n",
      "240/240 [==============================] - 0s 722us/step - loss: 1.3105 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00071: loss improved from 1.31132 to 1.31049, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 72/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.3094 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00072: loss improved from 1.31049 to 1.30944, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 73/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 1.3067 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00073: loss improved from 1.30944 to 1.30672, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 74/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.3083 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00074: loss did not improve from 1.30672\n",
      "Epoch 75/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 1.3075 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00075: loss did not improve from 1.30672\n",
      "Epoch 76/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 1.3035 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00076: loss improved from 1.30672 to 1.30347, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 77/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.3058 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00077: loss did not improve from 1.30347\n",
      "Epoch 78/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.3048 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00078: loss did not improve from 1.30347\n",
      "Epoch 79/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.3019 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00079: loss improved from 1.30347 to 1.30194, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 80/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.3030 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00080: loss did not improve from 1.30194\n",
      "Epoch 81/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.2999 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00081: loss improved from 1.30194 to 1.29991, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 82/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.2996 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00082: loss improved from 1.29991 to 1.29960, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 83/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.3030 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00083: loss did not improve from 1.29960\n",
      "Epoch 84/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.3009 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00084: loss did not improve from 1.29960\n",
      "Epoch 85/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 703us/step - loss: 1.2957 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00085: loss improved from 1.29960 to 1.29572, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 86/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.2969 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00086: loss did not improve from 1.29572\n",
      "Epoch 87/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.2922 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00087: loss improved from 1.29572 to 1.29221, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 88/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.2933 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00088: loss did not improve from 1.29221\n",
      "Epoch 89/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.2921 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00089: loss improved from 1.29221 to 1.29212, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 90/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.2935 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00090: loss did not improve from 1.29212\n",
      "Epoch 91/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.2920 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00091: loss improved from 1.29212 to 1.29197, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 92/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 1.2893 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00092: loss improved from 1.29197 to 1.28932, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 93/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.2888 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00093: loss improved from 1.28932 to 1.28885, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 94/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 1.2888 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00094: loss improved from 1.28885 to 1.28881, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 95/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.2878 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00095: loss improved from 1.28881 to 1.28778, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 96/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 1.2890 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00096: loss did not improve from 1.28778\n",
      "Epoch 97/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.2892 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00097: loss did not improve from 1.28778\n",
      "Epoch 98/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.2892 - accuracy: 0.4667\n",
      "\n",
      "Epoch 00098: loss did not improve from 1.28778\n",
      "Epoch 99/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.2859 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00099: loss improved from 1.28778 to 1.28588, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 100/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.2858 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00100: loss improved from 1.28588 to 1.28578, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 101/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.2850 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00101: loss improved from 1.28578 to 1.28500, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 102/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.2814 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00102: loss improved from 1.28500 to 1.28136, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 103/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.2839 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00103: loss did not improve from 1.28136\n",
      "Epoch 104/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 1.2811 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00104: loss improved from 1.28136 to 1.28108, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 105/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.2836 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00105: loss did not improve from 1.28108\n",
      "Epoch 106/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.2801 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00106: loss improved from 1.28108 to 1.28005, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 107/2000\n",
      "240/240 [==============================] - 0s 750us/step - loss: 1.2824 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00107: loss did not improve from 1.28005\n",
      "Epoch 108/2000\n",
      "240/240 [==============================] - 0s 737us/step - loss: 1.2811 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00108: loss did not improve from 1.28005\n",
      "Epoch 109/2000\n",
      "240/240 [==============================] - 0s 733us/step - loss: 1.2803 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00109: loss did not improve from 1.28005\n",
      "Epoch 110/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.2789 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00110: loss improved from 1.28005 to 1.27889, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 111/2000\n",
      "240/240 [==============================] - 0s 966us/step - loss: 1.2779 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00111: loss improved from 1.27889 to 1.27790, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 112/2000\n",
      "240/240 [==============================] - 0s 745us/step - loss: 1.2800 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00112: loss did not improve from 1.27790\n",
      "Epoch 113/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.2791 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00113: loss did not improve from 1.27790\n",
      "Epoch 114/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 1.2788 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00114: loss did not improve from 1.27790\n",
      "Epoch 115/2000\n",
      "240/240 [==============================] - 0s 734us/step - loss: 1.2783 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00115: loss did not improve from 1.27790\n",
      "Epoch 116/2000\n",
      "240/240 [==============================] - 0s 743us/step - loss: 1.2751 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00116: loss improved from 1.27790 to 1.27506, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 117/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 1.2767 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00117: loss did not improve from 1.27506\n",
      "Epoch 118/2000\n",
      "240/240 [==============================] - 0s 860us/step - loss: 1.2787 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00118: loss did not improve from 1.27506\n",
      "Epoch 119/2000\n",
      "240/240 [==============================] - 0s 847us/step - loss: 1.2732 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00119: loss improved from 1.27506 to 1.27318, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 120/2000\n",
      "240/240 [==============================] - 0s 791us/step - loss: 1.2753 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00120: loss did not improve from 1.27318\n",
      "Epoch 121/2000\n",
      "240/240 [==============================] - 0s 740us/step - loss: 1.2733 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00121: loss did not improve from 1.27318\n",
      "Epoch 122/2000\n",
      "240/240 [==============================] - 0s 656us/step - loss: 1.2728 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00122: loss improved from 1.27318 to 1.27280, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 123/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.2724 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00123: loss improved from 1.27280 to 1.27244, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 124/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.2740 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00124: loss did not improve from 1.27244\n",
      "Epoch 125/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.2730 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00125: loss did not improve from 1.27244\n",
      "Epoch 126/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 1.2709 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00126: loss improved from 1.27244 to 1.27092, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 127/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.2738 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00127: loss did not improve from 1.27092\n",
      "Epoch 128/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 1.2711 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00128: loss did not improve from 1.27092\n",
      "Epoch 129/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.2726 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00129: loss did not improve from 1.27092\n",
      "Epoch 130/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.2695 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00130: loss improved from 1.27092 to 1.26948, saving model to ./model/1000001000100010004.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.2696 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00131: loss did not improve from 1.26948\n",
      "Epoch 132/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.2723 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00132: loss did not improve from 1.26948\n",
      "Epoch 133/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.2707 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00133: loss did not improve from 1.26948\n",
      "Epoch 134/2000\n",
      "240/240 [==============================] - 0s 747us/step - loss: 1.2713 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00134: loss did not improve from 1.26948\n",
      "Epoch 135/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.2696 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00135: loss did not improve from 1.26948\n",
      "Epoch 136/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.2699 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00136: loss did not improve from 1.26948\n",
      "Epoch 137/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.2669 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00137: loss improved from 1.26948 to 1.26694, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 138/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 1.2680 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00138: loss did not improve from 1.26694\n",
      "Epoch 139/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.2682 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00139: loss did not improve from 1.26694\n",
      "Epoch 140/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.2676 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00140: loss did not improve from 1.26694\n",
      "Epoch 141/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.2657 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00141: loss improved from 1.26694 to 1.26569, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 142/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 1.2674 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00142: loss did not improve from 1.26569\n",
      "Epoch 143/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 1.2674 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00143: loss did not improve from 1.26569\n",
      "Epoch 144/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.2658 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00144: loss did not improve from 1.26569\n",
      "Epoch 145/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.2655 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00145: loss improved from 1.26569 to 1.26554, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 146/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.2694 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00146: loss did not improve from 1.26554\n",
      "Epoch 147/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.2691 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00147: loss did not improve from 1.26554\n",
      "Epoch 148/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.2670 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00148: loss did not improve from 1.26554\n",
      "Epoch 149/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 1.2659 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00149: loss did not improve from 1.26554\n",
      "Epoch 150/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.2646 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00150: loss improved from 1.26554 to 1.26461, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 151/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.2642 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00151: loss improved from 1.26461 to 1.26417, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 152/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.2618 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00152: loss improved from 1.26417 to 1.26184, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 153/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.2628 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00153: loss did not improve from 1.26184\n",
      "Epoch 154/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.2642 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00154: loss did not improve from 1.26184\n",
      "Epoch 155/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 1.2628 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00155: loss did not improve from 1.26184\n",
      "Epoch 156/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.2616 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00156: loss improved from 1.26184 to 1.26159, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 157/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.2636 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00157: loss did not improve from 1.26159\n",
      "Epoch 158/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 1.2674 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00158: loss did not improve from 1.26159\n",
      "Epoch 159/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.2652 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00159: loss did not improve from 1.26159\n",
      "Epoch 160/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.2618 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00160: loss did not improve from 1.26159\n",
      "Epoch 161/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 1.2613 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00161: loss improved from 1.26159 to 1.26130, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 162/2000\n",
      "240/240 [==============================] - 0s 721us/step - loss: 1.2635 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00162: loss did not improve from 1.26130\n",
      "Epoch 163/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.2617 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00163: loss did not improve from 1.26130\n",
      "Epoch 164/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 1.2623 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00164: loss did not improve from 1.26130\n",
      "Epoch 165/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.2617 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00165: loss did not improve from 1.26130\n",
      "Epoch 166/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 1.2597 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00166: loss improved from 1.26130 to 1.25975, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 167/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.2595 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00167: loss improved from 1.25975 to 1.25955, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 168/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 1.2585 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00168: loss improved from 1.25955 to 1.25851, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 169/2000\n",
      "240/240 [==============================] - 0s 757us/step - loss: 1.2597 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00169: loss did not improve from 1.25851\n",
      "Epoch 170/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 1.2607 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00170: loss did not improve from 1.25851\n",
      "Epoch 171/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.2610 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00171: loss did not improve from 1.25851\n",
      "Epoch 172/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 1.2639 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00172: loss did not improve from 1.25851\n",
      "Epoch 173/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.2608 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00173: loss did not improve from 1.25851\n",
      "Epoch 174/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.2607 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00174: loss did not improve from 1.25851\n",
      "Epoch 175/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.2580 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00175: loss improved from 1.25851 to 1.25802, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 176/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.2578 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00176: loss improved from 1.25802 to 1.25785, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 177/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 1.2574 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00177: loss improved from 1.25785 to 1.25739, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 178/2000\n",
      "240/240 [==============================] - 0s 764us/step - loss: 1.2589 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00178: loss did not improve from 1.25739\n",
      "Epoch 179/2000\n",
      "240/240 [==============================] - 0s 774us/step - loss: 1.2596 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00179: loss did not improve from 1.25739\n",
      "Epoch 180/2000\n",
      "240/240 [==============================] - 0s 777us/step - loss: 1.2581 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00180: loss did not improve from 1.25739\n",
      "Epoch 181/2000\n",
      "240/240 [==============================] - 0s 797us/step - loss: 1.2585 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00181: loss did not improve from 1.25739\n",
      "Epoch 182/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.2576 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00182: loss did not improve from 1.25739\n",
      "Epoch 183/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 1.2563 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00183: loss improved from 1.25739 to 1.25633, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 184/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 1.2572 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00184: loss did not improve from 1.25633\n",
      "Epoch 185/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 1.2559 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00185: loss improved from 1.25633 to 1.25588, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 186/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.2585 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00186: loss did not improve from 1.25588\n",
      "Epoch 187/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.2565 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00187: loss did not improve from 1.25588\n",
      "Epoch 188/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.2566 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00188: loss did not improve from 1.25588\n",
      "Epoch 189/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.2567 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00189: loss did not improve from 1.25588\n",
      "Epoch 190/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 1.2570 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00190: loss did not improve from 1.25588\n",
      "Epoch 191/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.2581 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00191: loss did not improve from 1.25588\n",
      "Epoch 192/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.2592 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00192: loss did not improve from 1.25588\n",
      "Epoch 193/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 1.2606 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00193: loss did not improve from 1.25588\n",
      "Epoch 194/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.2563 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00194: loss did not improve from 1.25588\n",
      "Epoch 195/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.2568 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00195: loss did not improve from 1.25588\n",
      "Epoch 196/2000\n",
      "240/240 [==============================] - 0s 726us/step - loss: 1.2562 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00196: loss did not improve from 1.25588\n",
      "Epoch 197/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 1.2560 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00197: loss did not improve from 1.25588\n",
      "Epoch 198/2000\n",
      "240/240 [==============================] - 0s 789us/step - loss: 1.2547 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00198: loss improved from 1.25588 to 1.25473, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 199/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.2537 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00199: loss improved from 1.25473 to 1.25374, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 200/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.2547 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00200: loss did not improve from 1.25374\n",
      "Epoch 201/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.2548 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00201: loss did not improve from 1.25374\n",
      "Epoch 202/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.2539 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00202: loss did not improve from 1.25374\n",
      "Epoch 203/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.2538 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00203: loss did not improve from 1.25374\n",
      "Epoch 204/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.2612 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00204: loss did not improve from 1.25374\n",
      "Epoch 205/2000\n",
      "240/240 [==============================] - 0s 737us/step - loss: 1.2605 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00205: loss did not improve from 1.25374\n",
      "Epoch 206/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 1.2575 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00206: loss did not improve from 1.25374\n",
      "Epoch 207/2000\n",
      "240/240 [==============================] - 0s 806us/step - loss: 1.2554 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00207: loss did not improve from 1.25374\n",
      "Epoch 208/2000\n",
      "240/240 [==============================] - 0s 826us/step - loss: 1.2556 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00208: loss did not improve from 1.25374\n",
      "Epoch 209/2000\n",
      "240/240 [==============================] - 0s 822us/step - loss: 1.2547 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00209: loss did not improve from 1.25374\n",
      "Epoch 210/2000\n",
      "240/240 [==============================] - 0s 860us/step - loss: 1.2552 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00210: loss did not improve from 1.25374\n",
      "Epoch 211/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 1.2560 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00211: loss did not improve from 1.25374\n",
      "Epoch 212/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 1.2543 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00212: loss did not improve from 1.25374\n",
      "Epoch 213/2000\n",
      "240/240 [==============================] - 0s 633us/step - loss: 1.2529 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00213: loss improved from 1.25374 to 1.25292, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 214/2000\n",
      "240/240 [==============================] - 0s 661us/step - loss: 1.2525 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00214: loss improved from 1.25292 to 1.25248, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 215/2000\n",
      "240/240 [==============================] - 0s 631us/step - loss: 1.2519 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00215: loss improved from 1.25248 to 1.25186, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 216/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.2526 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00216: loss did not improve from 1.25186\n",
      "Epoch 217/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.2538 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00217: loss did not improve from 1.25186\n",
      "Epoch 218/2000\n",
      "240/240 [==============================] - 0s 619us/step - loss: 1.2537 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00218: loss did not improve from 1.25186\n",
      "Epoch 219/2000\n",
      "240/240 [==============================] - 0s 756us/step - loss: 1.2563 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00219: loss did not improve from 1.25186\n",
      "Epoch 220/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.2529 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00220: loss did not improve from 1.25186\n",
      "Epoch 221/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.2519 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00221: loss improved from 1.25186 to 1.25185, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 222/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.2527 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00222: loss did not improve from 1.25185\n",
      "Epoch 223/2000\n",
      "240/240 [==============================] - 0s 635us/step - loss: 1.2554 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00223: loss did not improve from 1.25185\n",
      "Epoch 224/2000\n",
      "240/240 [==============================] - 0s 739us/step - loss: 1.2538 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00224: loss did not improve from 1.25185\n",
      "Epoch 225/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.2541 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00225: loss did not improve from 1.25185\n",
      "Epoch 226/2000\n",
      "240/240 [==============================] - ETA: 0s - loss: 1.2516 - accuracy: 0.42 - 0s 696us/step - loss: 1.2515 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00226: loss improved from 1.25185 to 1.25153, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 227/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.2518 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00227: loss did not improve from 1.25153\n",
      "Epoch 228/2000\n",
      "240/240 [==============================] - 0s 718us/step - loss: 1.2515 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00228: loss did not improve from 1.25153\n",
      "Epoch 229/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 681us/step - loss: 1.2516 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00229: loss did not improve from 1.25153\n",
      "Epoch 230/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.2550 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00230: loss did not improve from 1.25153\n",
      "Epoch 231/2000\n",
      "240/240 [==============================] - 0s 667us/step - loss: 1.2533 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00231: loss did not improve from 1.25153\n",
      "Epoch 232/2000\n",
      "240/240 [==============================] - 0s 623us/step - loss: 1.2575 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00232: loss did not improve from 1.25153\n",
      "Epoch 233/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.2545 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00233: loss did not improve from 1.25153\n",
      "Epoch 234/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 1.2533 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00234: loss did not improve from 1.25153\n",
      "Epoch 235/2000\n",
      "240/240 [==============================] - 0s 741us/step - loss: 1.2515 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00235: loss improved from 1.25153 to 1.25146, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 236/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.2509 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00236: loss improved from 1.25146 to 1.25092, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 237/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.2502 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00237: loss improved from 1.25092 to 1.25024, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 238/2000\n",
      "240/240 [==============================] - 0s 750us/step - loss: 1.2500 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00238: loss improved from 1.25024 to 1.25004, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 239/2000\n",
      "240/240 [==============================] - 0s 743us/step - loss: 1.2499 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00239: loss improved from 1.25004 to 1.24994, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 240/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.2503 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00240: loss did not improve from 1.24994\n",
      "Epoch 241/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.2506 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00241: loss did not improve from 1.24994\n",
      "Epoch 242/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 1.2506 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00242: loss did not improve from 1.24994\n",
      "Epoch 243/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.2501 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00243: loss did not improve from 1.24994\n",
      "Epoch 244/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 1.2520 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00244: loss did not improve from 1.24994\n",
      "Epoch 245/2000\n",
      "240/240 [==============================] - 0s 666us/step - loss: 1.2514 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00245: loss did not improve from 1.24994\n",
      "Epoch 246/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.2538 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00246: loss did not improve from 1.24994\n",
      "Epoch 247/2000\n",
      "240/240 [==============================] - 0s 743us/step - loss: 1.2522 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00247: loss did not improve from 1.24994\n",
      "Epoch 248/2000\n",
      "240/240 [==============================] - 0s 816us/step - loss: 1.2532 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00248: loss did not improve from 1.24994\n",
      "Epoch 249/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.2494 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00249: loss improved from 1.24994 to 1.24937, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 250/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.2489 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00250: loss improved from 1.24937 to 1.24893, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 251/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.2505 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00251: loss did not improve from 1.24893\n",
      "Epoch 252/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.2539 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00252: loss did not improve from 1.24893\n",
      "Epoch 253/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.2527 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00253: loss did not improve from 1.24893\n",
      "Epoch 254/2000\n",
      "240/240 [==============================] - 0s 664us/step - loss: 1.2520 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00254: loss did not improve from 1.24893\n",
      "Epoch 255/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.2502 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00255: loss did not improve from 1.24893\n",
      "Epoch 256/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 1.2496 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00256: loss did not improve from 1.24893\n",
      "Epoch 257/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.2486 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00257: loss improved from 1.24893 to 1.24860, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 258/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.2487 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00258: loss did not improve from 1.24860\n",
      "Epoch 259/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 1.2493 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00259: loss did not improve from 1.24860\n",
      "Epoch 260/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.2500 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00260: loss did not improve from 1.24860\n",
      "Epoch 261/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 1.2504 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00261: loss did not improve from 1.24860\n",
      "Epoch 262/2000\n",
      "240/240 [==============================] - 0s 737us/step - loss: 1.2486 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00262: loss did not improve from 1.24860\n",
      "Epoch 263/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.2479 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00263: loss improved from 1.24860 to 1.24791, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 264/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.2487 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00264: loss did not improve from 1.24791\n",
      "Epoch 265/2000\n",
      "240/240 [==============================] - 0s 667us/step - loss: 1.2487 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00265: loss did not improve from 1.24791\n",
      "Epoch 266/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 1.2479 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00266: loss improved from 1.24791 to 1.24789, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 267/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 1.2491 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00267: loss did not improve from 1.24789\n",
      "Epoch 268/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.2490 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00268: loss did not improve from 1.24789\n",
      "Epoch 269/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.2490 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00269: loss did not improve from 1.24789\n",
      "Epoch 270/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2487 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00270: loss did not improve from 1.24789\n",
      "Epoch 271/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 1.2483 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00271: loss did not improve from 1.24789\n",
      "Epoch 272/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.2482 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00272: loss did not improve from 1.24789\n",
      "Epoch 273/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.2479 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00273: loss did not improve from 1.24789\n",
      "Epoch 274/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.2531 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00274: loss did not improve from 1.24789\n",
      "Epoch 275/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2606 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00275: loss did not improve from 1.24789\n",
      "Epoch 276/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.2586 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00276: loss did not improve from 1.24789\n",
      "Epoch 277/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.2508 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00277: loss did not improve from 1.24789\n",
      "Epoch 278/2000\n",
      "240/240 [==============================] - 0s 667us/step - loss: 1.2480 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00278: loss did not improve from 1.24789\n",
      "Epoch 279/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 686us/step - loss: 1.2474 - accuracy: 0.4625\n",
      "\n",
      "Epoch 00279: loss improved from 1.24789 to 1.24739, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 280/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.2483 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00280: loss did not improve from 1.24739\n",
      "Epoch 281/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 1.2482 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00281: loss did not improve from 1.24739\n",
      "Epoch 282/2000\n",
      "240/240 [==============================] - 0s 631us/step - loss: 1.2468 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00282: loss improved from 1.24739 to 1.24684, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 283/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.2471 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00283: loss did not improve from 1.24684\n",
      "Epoch 284/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.2475 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00284: loss did not improve from 1.24684\n",
      "Epoch 285/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.2480 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00285: loss did not improve from 1.24684\n",
      "Epoch 286/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 1.2484 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00286: loss did not improve from 1.24684\n",
      "Epoch 287/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.2474 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00287: loss did not improve from 1.24684\n",
      "Epoch 288/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.2477 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00288: loss did not improve from 1.24684\n",
      "Epoch 289/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.2481 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00289: loss did not improve from 1.24684\n",
      "Epoch 290/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.2471 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00290: loss did not improve from 1.24684\n",
      "Epoch 291/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.2476 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00291: loss did not improve from 1.24684\n",
      "Epoch 292/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 1.2501 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00292: loss did not improve from 1.24684\n",
      "Epoch 293/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.2505 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00293: loss did not improve from 1.24684\n",
      "Epoch 294/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.2500 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00294: loss did not improve from 1.24684\n",
      "Epoch 295/2000\n",
      "240/240 [==============================] - 0s 848us/step - loss: 1.2501 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00295: loss did not improve from 1.24684\n",
      "Epoch 296/2000\n",
      "240/240 [==============================] - 0s 793us/step - loss: 1.2460 - accuracy: 0.4625\n",
      "\n",
      "Epoch 00296: loss improved from 1.24684 to 1.24605, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 297/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.2458 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00297: loss improved from 1.24605 to 1.24584, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 298/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.2484 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00298: loss did not improve from 1.24584\n",
      "Epoch 299/2000\n",
      "240/240 [==============================] - 0s 804us/step - loss: 1.2473 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00299: loss did not improve from 1.24584\n",
      "Epoch 300/2000\n",
      "240/240 [==============================] - 0s 784us/step - loss: 1.2469 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00300: loss did not improve from 1.24584\n",
      "Epoch 301/2000\n",
      "240/240 [==============================] - 0s 846us/step - loss: 1.2465 - accuracy: 0.4667\n",
      "\n",
      "Epoch 00301: loss did not improve from 1.24584\n",
      "Epoch 302/2000\n",
      "240/240 [==============================] - 0s 827us/step - loss: 1.2457 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00302: loss improved from 1.24584 to 1.24571, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 303/2000\n",
      "240/240 [==============================] - 0s 819us/step - loss: 1.2455 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00303: loss improved from 1.24571 to 1.24554, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 304/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 1.2472 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00304: loss did not improve from 1.24554\n",
      "Epoch 305/2000\n",
      "240/240 [==============================] - 0s 616us/step - loss: 1.2497 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00305: loss did not improve from 1.24554\n",
      "Epoch 306/2000\n",
      "240/240 [==============================] - 0s 740us/step - loss: 1.2485 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00306: loss did not improve from 1.24554\n",
      "Epoch 307/2000\n",
      "240/240 [==============================] - 0s 856us/step - loss: 1.2484 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00307: loss did not improve from 1.24554\n",
      "Epoch 308/2000\n",
      "240/240 [==============================] - 0s 664us/step - loss: 1.2475 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00308: loss did not improve from 1.24554\n",
      "Epoch 309/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 1.2480 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00309: loss did not improve from 1.24554\n",
      "Epoch 310/2000\n",
      "240/240 [==============================] - 0s 790us/step - loss: 1.2458 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00310: loss did not improve from 1.24554\n",
      "Epoch 311/2000\n",
      "240/240 [==============================] - 0s 632us/step - loss: 1.2533 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00311: loss did not improve from 1.24554\n",
      "Epoch 312/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 1.2494 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00312: loss did not improve from 1.24554\n",
      "Epoch 313/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.2493 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00313: loss did not improve from 1.24554\n",
      "Epoch 314/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 1.2472 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00314: loss did not improve from 1.24554\n",
      "Epoch 315/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 1.2487 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00315: loss did not improve from 1.24554\n",
      "Epoch 316/2000\n",
      "240/240 [==============================] - 0s 620us/step - loss: 1.2462 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00316: loss did not improve from 1.24554\n",
      "Epoch 317/2000\n",
      "240/240 [==============================] - 0s 623us/step - loss: 1.2447 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00317: loss improved from 1.24554 to 1.24468, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 318/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 1.2454 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00318: loss did not improve from 1.24468\n",
      "Epoch 319/2000\n",
      "240/240 [==============================] - 0s 754us/step - loss: 1.2455 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00319: loss did not improve from 1.24468\n",
      "Epoch 320/2000\n",
      "240/240 [==============================] - 0s 741us/step - loss: 1.2453 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00320: loss did not improve from 1.24468\n",
      "Epoch 321/2000\n",
      "240/240 [==============================] - 0s 739us/step - loss: 1.2452 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00321: loss did not improve from 1.24468\n",
      "Epoch 322/2000\n",
      "240/240 [==============================] - 0s 734us/step - loss: 1.2460 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00322: loss did not improve from 1.24468\n",
      "Epoch 323/2000\n",
      "240/240 [==============================] - 0s 739us/step - loss: 1.2479 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00323: loss did not improve from 1.24468\n",
      "Epoch 324/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.2461 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00324: loss did not improve from 1.24468\n",
      "Epoch 325/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.2459 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00325: loss did not improve from 1.24468\n",
      "Epoch 326/2000\n",
      "240/240 [==============================] - 0s 861us/step - loss: 1.2453 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00326: loss did not improve from 1.24468\n",
      "Epoch 327/2000\n",
      "240/240 [==============================] - 0s 932us/step - loss: 1.2454 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00327: loss did not improve from 1.24468\n",
      "Epoch 328/2000\n",
      "240/240 [==============================] - 0s 868us/step - loss: 1.2459 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00328: loss did not improve from 1.24468\n",
      "Epoch 329/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.2447 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00329: loss did not improve from 1.24468\n",
      "Epoch 330/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 676us/step - loss: 1.2449 - accuracy: 0.4708\n",
      "\n",
      "Epoch 00330: loss did not improve from 1.24468\n",
      "Epoch 331/2000\n",
      "240/240 [==============================] - 0s 666us/step - loss: 1.2501 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00331: loss did not improve from 1.24468\n",
      "Epoch 332/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 1.2489 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00332: loss did not improve from 1.24468\n",
      "Epoch 333/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 1.2468 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00333: loss did not improve from 1.24468\n",
      "Epoch 334/2000\n",
      "240/240 [==============================] - 0s 661us/step - loss: 1.2481 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00334: loss did not improve from 1.24468\n",
      "Epoch 335/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 1.2496 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00335: loss did not improve from 1.24468\n",
      "Epoch 336/2000\n",
      "240/240 [==============================] - 0s 906us/step - loss: 1.2472 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00336: loss did not improve from 1.24468\n",
      "Epoch 337/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.2453 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00337: loss did not improve from 1.24468\n",
      "Epoch 338/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.2451 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00338: loss did not improve from 1.24468\n",
      "Epoch 339/2000\n",
      "240/240 [==============================] - 0s 881us/step - loss: 1.2452 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00339: loss did not improve from 1.24468\n",
      "Epoch 340/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.2448 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00340: loss did not improve from 1.24468\n",
      "Epoch 341/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.2447 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00341: loss did not improve from 1.24468\n",
      "Epoch 342/2000\n",
      "240/240 [==============================] - 0s 868us/step - loss: 1.2520 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00342: loss did not improve from 1.24468\n",
      "Epoch 343/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 1.2464 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00343: loss did not improve from 1.24468\n",
      "Epoch 344/2000\n",
      "240/240 [==============================] - 0s 623us/step - loss: 1.2449 - accuracy: 0.4625\n",
      "\n",
      "Epoch 00344: loss did not improve from 1.24468\n",
      "Epoch 345/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.2448 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00345: loss did not improve from 1.24468\n",
      "Epoch 346/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.2447 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00346: loss improved from 1.24468 to 1.24466, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 347/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.2442 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00347: loss improved from 1.24466 to 1.24424, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 348/2000\n",
      "240/240 [==============================] - 0s 893us/step - loss: 1.2447 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00348: loss did not improve from 1.24424\n",
      "Epoch 349/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.2446 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00349: loss did not improve from 1.24424\n",
      "Epoch 350/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2451 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00350: loss did not improve from 1.24424\n",
      "Epoch 351/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 1.2444 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00351: loss did not improve from 1.24424\n",
      "Epoch 352/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2445 - accuracy: 0.4708\n",
      "\n",
      "Epoch 00352: loss did not improve from 1.24424\n",
      "Epoch 353/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2440 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00353: loss improved from 1.24424 to 1.24396, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 354/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.2452 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00354: loss did not improve from 1.24396\n",
      "Epoch 355/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 1.2438 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00355: loss improved from 1.24396 to 1.24381, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 356/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.2443 - accuracy: 0.4625\n",
      "\n",
      "Epoch 00356: loss did not improve from 1.24381\n",
      "Epoch 357/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 1.2442 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00357: loss did not improve from 1.24381\n",
      "Epoch 358/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.2443 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00358: loss did not improve from 1.24381\n",
      "Epoch 359/2000\n",
      "240/240 [==============================] - 0s 658us/step - loss: 1.2445 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00359: loss did not improve from 1.24381\n",
      "Epoch 360/2000\n",
      "240/240 [==============================] - 0s 745us/step - loss: 1.2439 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00360: loss did not improve from 1.24381\n",
      "Epoch 361/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.2444 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00361: loss did not improve from 1.24381\n",
      "Epoch 362/2000\n",
      "240/240 [==============================] - 0s 662us/step - loss: 1.2457 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00362: loss did not improve from 1.24381\n",
      "Epoch 363/2000\n",
      "240/240 [==============================] - 0s 762us/step - loss: 1.2505 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00363: loss did not improve from 1.24381\n",
      "Epoch 364/2000\n",
      "240/240 [==============================] - 0s 743us/step - loss: 1.2459 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00364: loss did not improve from 1.24381\n",
      "Epoch 365/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 1.2442 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00365: loss did not improve from 1.24381\n",
      "Epoch 366/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.2441 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00366: loss did not improve from 1.24381\n",
      "Epoch 367/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.2446 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00367: loss did not improve from 1.24381\n",
      "Epoch 368/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.2439 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00368: loss did not improve from 1.24381\n",
      "Epoch 369/2000\n",
      "240/240 [==============================] - 0s 893us/step - loss: 1.2434 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00369: loss improved from 1.24381 to 1.24339, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 370/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 1.2436 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00370: loss did not improve from 1.24339\n",
      "Epoch 371/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 1.2438 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00371: loss did not improve from 1.24339\n",
      "Epoch 372/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.2436 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00372: loss did not improve from 1.24339\n",
      "Epoch 373/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 1.2449 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00373: loss did not improve from 1.24339\n",
      "Epoch 374/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.2450 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00374: loss did not improve from 1.24339\n",
      "Epoch 375/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.2445 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00375: loss did not improve from 1.24339\n",
      "Epoch 376/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.2455 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00376: loss did not improve from 1.24339\n",
      "Epoch 377/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.2453 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00377: loss did not improve from 1.24339\n",
      "Epoch 378/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.2441 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00378: loss did not improve from 1.24339\n",
      "Epoch 379/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.2456 - accuracy: 0.4417TA: 0s - loss: 1.1804 - accuracy: 0.\n",
      "\n",
      "Epoch 00379: loss did not improve from 1.24339\n",
      "Epoch 380/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.2477 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00380: loss did not improve from 1.24339\n",
      "Epoch 381/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.2451 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00381: loss did not improve from 1.24339\n",
      "Epoch 382/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 1.2442 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00382: loss did not improve from 1.24339\n",
      "Epoch 383/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 1.2431 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00383: loss improved from 1.24339 to 1.24312, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 384/2000\n",
      "240/240 [==============================] - 0s 726us/step - loss: 1.2443 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00384: loss did not improve from 1.24312\n",
      "Epoch 385/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.2430 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00385: loss improved from 1.24312 to 1.24304, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 386/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.2434 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00386: loss did not improve from 1.24304\n",
      "Epoch 387/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.2426 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00387: loss improved from 1.24304 to 1.24263, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 388/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.2437 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00388: loss did not improve from 1.24263\n",
      "Epoch 389/2000\n",
      "240/240 [==============================] - 0s 850us/step - loss: 1.2438 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00389: loss did not improve from 1.24263\n",
      "Epoch 390/2000\n",
      "240/240 [==============================] - 0s 803us/step - loss: 1.2433 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00390: loss did not improve from 1.24263\n",
      "Epoch 391/2000\n",
      "240/240 [==============================] - 0s 794us/step - loss: 1.2451 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00391: loss did not improve from 1.24263\n",
      "Epoch 392/2000\n",
      "240/240 [==============================] - 0s 791us/step - loss: 1.2448 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00392: loss did not improve from 1.24263\n",
      "Epoch 393/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 1.2476 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00393: loss did not improve from 1.24263\n",
      "Epoch 394/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.2452 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00394: loss did not improve from 1.24263\n",
      "Epoch 395/2000\n",
      "240/240 [==============================] - 0s 798us/step - loss: 1.2443 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00395: loss did not improve from 1.24263\n",
      "Epoch 396/2000\n",
      "240/240 [==============================] - 0s 810us/step - loss: 1.2442 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00396: loss did not improve from 1.24263\n",
      "Epoch 397/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2444 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00397: loss did not improve from 1.24263\n",
      "Epoch 398/2000\n",
      "240/240 [==============================] - 0s 819us/step - loss: 1.2443 - accuracy: 0.4625\n",
      "\n",
      "Epoch 00398: loss did not improve from 1.24263\n",
      "Epoch 399/2000\n",
      "240/240 [==============================] - 0s 662us/step - loss: 1.2438 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00399: loss did not improve from 1.24263\n",
      "Epoch 400/2000\n",
      "240/240 [==============================] - 0s 639us/step - loss: 1.2434 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00400: loss did not improve from 1.24263\n",
      "Epoch 401/2000\n",
      "240/240 [==============================] - 0s 794us/step - loss: 1.2447 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00401: loss did not improve from 1.24263\n",
      "Epoch 402/2000\n",
      "240/240 [==============================] - 0s 643us/step - loss: 1.2432 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00402: loss did not improve from 1.24263\n",
      "Epoch 403/2000\n",
      "240/240 [==============================] - 0s 633us/step - loss: 1.2430 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00403: loss did not improve from 1.24263\n",
      "Epoch 404/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.2433 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00404: loss did not improve from 1.24263\n",
      "Epoch 405/2000\n",
      "240/240 [==============================] - 0s 802us/step - loss: 1.2476 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00405: loss did not improve from 1.24263\n",
      "Epoch 406/2000\n",
      "240/240 [==============================] - 0s 629us/step - loss: 1.2449 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00406: loss did not improve from 1.24263\n",
      "Epoch 407/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 1.2442 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00407: loss did not improve from 1.24263\n",
      "Epoch 408/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.2429 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00408: loss did not improve from 1.24263\n",
      "Epoch 409/2000\n",
      "240/240 [==============================] - 0s 661us/step - loss: 1.2422 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00409: loss improved from 1.24263 to 1.24222, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 410/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.2431 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00410: loss did not improve from 1.24222\n",
      "Epoch 411/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 1.2442 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00411: loss did not improve from 1.24222\n",
      "Epoch 412/2000\n",
      "240/240 [==============================] - ETA: 0s - loss: 1.2320 - accuracy: 0.43 - 0s 687us/step - loss: 1.2431 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00412: loss did not improve from 1.24222\n",
      "Epoch 413/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.2425 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00413: loss did not improve from 1.24222\n",
      "Epoch 414/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 1.2422 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00414: loss improved from 1.24222 to 1.24216, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 415/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.2428 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00415: loss did not improve from 1.24216\n",
      "Epoch 416/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2434 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00416: loss did not improve from 1.24216\n",
      "Epoch 417/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 1.2421 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00417: loss improved from 1.24216 to 1.24212, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 418/2000\n",
      "240/240 [==============================] - 0s 806us/step - loss: 1.2423 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00418: loss did not improve from 1.24212\n",
      "Epoch 419/2000\n",
      "240/240 [==============================] - 0s 798us/step - loss: 1.2446 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00419: loss did not improve from 1.24212\n",
      "Epoch 420/2000\n",
      "240/240 [==============================] - 0s 898us/step - loss: 1.2555 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00420: loss did not improve from 1.24212\n",
      "Epoch 421/2000\n",
      "240/240 [==============================] - 0s 643us/step - loss: 1.2480 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00421: loss did not improve from 1.24212\n",
      "Epoch 422/2000\n",
      "240/240 [==============================] - 0s 616us/step - loss: 1.2460 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00422: loss did not improve from 1.24212\n",
      "Epoch 423/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.2438 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00423: loss did not improve from 1.24212\n",
      "Epoch 424/2000\n",
      "240/240 [==============================] - 0s 744us/step - loss: 1.2441 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00424: loss did not improve from 1.24212\n",
      "Epoch 425/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.2430 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00425: loss did not improve from 1.24212\n",
      "Epoch 426/2000\n",
      "240/240 [==============================] - 0s 640us/step - loss: 1.2432 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00426: loss did not improve from 1.24212\n",
      "Epoch 427/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.2434 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00427: loss did not improve from 1.24212\n",
      "Epoch 428/2000\n",
      "240/240 [==============================] - 0s 656us/step - loss: 1.2425 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00428: loss did not improve from 1.24212\n",
      "Epoch 429/2000\n",
      "240/240 [==============================] - 0s 629us/step - loss: 1.2432 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00429: loss did not improve from 1.24212\n",
      "Epoch 430/2000\n",
      "240/240 [==============================] - 0s 618us/step - loss: 1.2434 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00430: loss did not improve from 1.24212\n",
      "Epoch 431/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.2438 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00431: loss did not improve from 1.24212\n",
      "Epoch 432/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 626us/step - loss: 1.2431 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00432: loss did not improve from 1.24212\n",
      "Epoch 433/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 1.2437 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00433: loss did not improve from 1.24212\n",
      "Epoch 434/2000\n",
      "240/240 [==============================] - 0s 628us/step - loss: 1.2431 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00434: loss did not improve from 1.24212\n",
      "Epoch 435/2000\n",
      "240/240 [==============================] - 0s 633us/step - loss: 1.2420 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00435: loss improved from 1.24212 to 1.24201, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 436/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 1.2419 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00436: loss improved from 1.24201 to 1.24188, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 437/2000\n",
      "240/240 [==============================] - 0s 848us/step - loss: 1.2442 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00437: loss did not improve from 1.24188\n",
      "Epoch 438/2000\n",
      "240/240 [==============================] - 0s 753us/step - loss: 1.2429 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00438: loss did not improve from 1.24188\n",
      "Epoch 439/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.2435 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00439: loss did not improve from 1.24188\n",
      "Epoch 440/2000\n",
      "240/240 [==============================] - 0s 765us/step - loss: 1.2429 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00440: loss did not improve from 1.24188\n",
      "Epoch 441/2000\n",
      "240/240 [==============================] - 0s 819us/step - loss: 1.2428 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00441: loss did not improve from 1.24188\n",
      "Epoch 442/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.2422 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00442: loss did not improve from 1.24188\n",
      "Epoch 443/2000\n",
      "240/240 [==============================] - 0s 619us/step - loss: 1.2422 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00443: loss did not improve from 1.24188\n",
      "Epoch 444/2000\n",
      "240/240 [==============================] - 0s 625us/step - loss: 1.2427 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00444: loss did not improve from 1.24188\n",
      "Epoch 445/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2441 - accuracy: 0.4667\n",
      "\n",
      "Epoch 00445: loss did not improve from 1.24188\n",
      "Epoch 446/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 1.2445 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00446: loss did not improve from 1.24188\n",
      "Epoch 447/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.2430 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00447: loss did not improve from 1.24188\n",
      "Epoch 448/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 1.2429 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00448: loss did not improve from 1.24188\n",
      "Epoch 449/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.2419 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00449: loss did not improve from 1.24188\n",
      "Epoch 450/2000\n",
      "240/240 [==============================] - 0s 785us/step - loss: 1.2414 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00450: loss improved from 1.24188 to 1.24144, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 451/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.2415 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00451: loss did not improve from 1.24144\n",
      "Epoch 452/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.2416 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00452: loss did not improve from 1.24144\n",
      "Epoch 453/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.2449 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00453: loss did not improve from 1.24144\n",
      "Epoch 454/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.2625 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00454: loss did not improve from 1.24144\n",
      "Epoch 455/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 1.2479 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00455: loss did not improve from 1.24144\n",
      "Epoch 456/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.2461 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00456: loss did not improve from 1.24144\n",
      "Epoch 457/2000\n",
      "240/240 [==============================] - 0s 874us/step - loss: 1.2439 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00457: loss did not improve from 1.24144\n",
      "Epoch 458/2000\n",
      "240/240 [==============================] - 0s 722us/step - loss: 1.2442 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00458: loss did not improve from 1.24144\n",
      "Epoch 459/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.2422 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00459: loss did not improve from 1.24144\n",
      "Epoch 460/2000\n",
      "240/240 [==============================] - 0s 741us/step - loss: 1.2423 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00460: loss did not improve from 1.24144\n",
      "Epoch 461/2000\n",
      "240/240 [==============================] - 0s 744us/step - loss: 1.2427 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00461: loss did not improve from 1.24144\n",
      "Epoch 462/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.2422 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00462: loss did not improve from 1.24144\n",
      "Epoch 463/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 1.2425 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00463: loss did not improve from 1.24144\n",
      "Epoch 464/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.2420 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00464: loss did not improve from 1.24144\n",
      "Epoch 465/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 1.2421 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00465: loss did not improve from 1.24144\n",
      "Epoch 466/2000\n",
      "240/240 [==============================] - 0s 844us/step - loss: 1.2418 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00466: loss did not improve from 1.24144\n",
      "Epoch 467/2000\n",
      "240/240 [==============================] - 0s 749us/step - loss: 1.2423 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00467: loss did not improve from 1.24144\n",
      "Epoch 468/2000\n",
      "240/240 [==============================] - 0s 812us/step - loss: 1.2421 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00468: loss did not improve from 1.24144\n",
      "Epoch 469/2000\n",
      "240/240 [==============================] - 0s 796us/step - loss: 1.2425 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00469: loss did not improve from 1.24144\n",
      "Epoch 470/2000\n",
      "240/240 [==============================] - 0s 756us/step - loss: 1.2448 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00470: loss did not improve from 1.24144\n",
      "Epoch 471/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 1.2417 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00471: loss did not improve from 1.24144\n",
      "Epoch 472/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.2412 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00472: loss improved from 1.24144 to 1.24124, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 473/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.2411 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00473: loss improved from 1.24124 to 1.24109, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 474/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.2414 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00474: loss did not improve from 1.24109\n",
      "Epoch 475/2000\n",
      "240/240 [==============================] - 0s 827us/step - loss: 1.2417 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00475: loss did not improve from 1.24109\n",
      "Epoch 476/2000\n",
      "240/240 [==============================] - 0s 753us/step - loss: 1.2420 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00476: loss did not improve from 1.24109\n",
      "Epoch 477/2000\n",
      "240/240 [==============================] - 0s 873us/step - loss: 1.2418 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00477: loss did not improve from 1.24109\n",
      "Epoch 478/2000\n",
      "240/240 [==============================] - 0s 660us/step - loss: 1.2422 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00478: loss did not improve from 1.24109\n",
      "Epoch 479/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.2411 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00479: loss improved from 1.24109 to 1.24109, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 480/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.2408 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00480: loss improved from 1.24109 to 1.24082, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 481/2000\n",
      "240/240 [==============================] - 0s 783us/step - loss: 1.2414 - accuracy: 0.4625\n",
      "\n",
      "Epoch 00481: loss did not improve from 1.24082\n",
      "Epoch 482/2000\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 1.2418 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00482: loss did not improve from 1.24082\n",
      "Epoch 483/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 1ms/step - loss: 1.2440 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00483: loss did not improve from 1.24082\n",
      "Epoch 484/2000\n",
      "240/240 [==============================] - 0s 850us/step - loss: 1.2419 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00484: loss did not improve from 1.24082\n",
      "Epoch 485/2000\n",
      "240/240 [==============================] - 0s 656us/step - loss: 1.2408 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00485: loss improved from 1.24082 to 1.24081, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 486/2000\n",
      "240/240 [==============================] - 0s 662us/step - loss: 1.2408 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00486: loss improved from 1.24081 to 1.24077, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 487/2000\n",
      "240/240 [==============================] - 0s 884us/step - loss: 1.2410 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00487: loss did not improve from 1.24077\n",
      "Epoch 488/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 1.2410 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00488: loss did not improve from 1.24077\n",
      "Epoch 489/2000\n",
      "240/240 [==============================] - 0s 631us/step - loss: 1.2436 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00489: loss did not improve from 1.24077\n",
      "Epoch 490/2000\n",
      "240/240 [==============================] - 0s 733us/step - loss: 1.2505 - accuracy: 0.4625\n",
      "\n",
      "Epoch 00490: loss did not improve from 1.24077\n",
      "Epoch 491/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 1.2462 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00491: loss did not improve from 1.24077\n",
      "Epoch 492/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2449 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00492: loss did not improve from 1.24077\n",
      "Epoch 493/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 1.2441 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00493: loss did not improve from 1.24077\n",
      "Epoch 494/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.2417 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00494: loss did not improve from 1.24077\n",
      "Epoch 495/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.2410 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00495: loss did not improve from 1.24077\n",
      "Epoch 496/2000\n",
      "240/240 [==============================] - 0s 612us/step - loss: 1.2407 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00496: loss improved from 1.24077 to 1.24068, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 497/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 1.2404 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00497: loss improved from 1.24068 to 1.24044, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 498/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 1.2405 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00498: loss did not improve from 1.24044\n",
      "Epoch 499/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 1.2405 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00499: loss did not improve from 1.24044\n",
      "Epoch 500/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.2407 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00500: loss did not improve from 1.24044\n",
      "Epoch 501/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 1.2408 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00501: loss did not improve from 1.24044\n",
      "Epoch 502/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 1.2415 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00502: loss did not improve from 1.24044\n",
      "Epoch 503/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.2411 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00503: loss did not improve from 1.24044\n",
      "Epoch 504/2000\n",
      "240/240 [==============================] - 0s 718us/step - loss: 1.2432 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00504: loss did not improve from 1.24044\n",
      "Epoch 505/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.2418 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00505: loss did not improve from 1.24044\n",
      "Epoch 506/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.2423 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00506: loss did not improve from 1.24044\n",
      "Epoch 507/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.2443 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00507: loss did not improve from 1.24044\n",
      "Epoch 508/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.2413 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00508: loss did not improve from 1.24044\n",
      "Epoch 509/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.2409 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00509: loss did not improve from 1.24044\n",
      "Epoch 510/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 1.2413 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00510: loss did not improve from 1.24044\n",
      "Epoch 511/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.2419 - accuracy: 0.4750\n",
      "\n",
      "Epoch 00511: loss did not improve from 1.24044\n",
      "Epoch 512/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.2403 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00512: loss improved from 1.24044 to 1.24025, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 513/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 1.2418 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00513: loss did not improve from 1.24025\n",
      "Epoch 514/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.2417 - accuracy: 0.4625\n",
      "\n",
      "Epoch 00514: loss did not improve from 1.24025\n",
      "Epoch 515/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.2406 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00515: loss did not improve from 1.24025\n",
      "Epoch 516/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.2410 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00516: loss did not improve from 1.24025\n",
      "Epoch 517/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.2413 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00517: loss did not improve from 1.24025\n",
      "Epoch 518/2000\n",
      "240/240 [==============================] - 0s 740us/step - loss: 1.2420 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00518: loss did not improve from 1.24025\n",
      "Epoch 519/2000\n",
      "240/240 [==============================] - 0s 923us/step - loss: 1.2404 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00519: loss did not improve from 1.24025\n",
      "Epoch 520/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.2417 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00520: loss did not improve from 1.24025\n",
      "Epoch 521/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.2416 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00521: loss did not improve from 1.24025\n",
      "Epoch 522/2000\n",
      "240/240 [==============================] - 0s 746us/step - loss: 1.2417 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00522: loss did not improve from 1.24025\n",
      "Epoch 523/2000\n",
      "240/240 [==============================] - 0s 815us/step - loss: 1.2403 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00523: loss did not improve from 1.24025\n",
      "Epoch 524/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.2404 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00524: loss did not improve from 1.24025\n",
      "Epoch 525/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.2405 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00525: loss did not improve from 1.24025\n",
      "Epoch 526/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.2415 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00526: loss did not improve from 1.24025\n",
      "Epoch 527/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.2434 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00527: loss did not improve from 1.24025\n",
      "Epoch 528/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 1.2438 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00528: loss did not improve from 1.24025\n",
      "Epoch 529/2000\n",
      "240/240 [==============================] - 0s 752us/step - loss: 1.2422 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00529: loss did not improve from 1.24025\n",
      "Epoch 530/2000\n",
      "240/240 [==============================] - 0s 923us/step - loss: 1.2416 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00530: loss did not improve from 1.24025\n",
      "Epoch 531/2000\n",
      "240/240 [==============================] - 0s 792us/step - loss: 1.2408 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00531: loss did not improve from 1.24025\n",
      "Epoch 532/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 1.2414 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00532: loss did not improve from 1.24025\n",
      "Epoch 533/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.2407 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00533: loss did not improve from 1.24025\n",
      "Epoch 534/2000\n",
      "240/240 [==============================] - 0s 697us/step - loss: 1.2407 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00534: loss did not improve from 1.24025\n",
      "Epoch 535/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 1.2402 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00535: loss improved from 1.24025 to 1.24024, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 536/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.2398 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00536: loss improved from 1.24024 to 1.23979, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 537/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.2401 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00537: loss did not improve from 1.23979\n",
      "Epoch 538/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 1.2408 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00538: loss did not improve from 1.23979\n",
      "Epoch 539/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.2408 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00539: loss did not improve from 1.23979\n",
      "Epoch 540/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 1.2763 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00540: loss did not improve from 1.23979\n",
      "Epoch 541/2000\n",
      "240/240 [==============================] - 0s 754us/step - loss: 1.2482 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00541: loss did not improve from 1.23979\n",
      "Epoch 542/2000\n",
      "240/240 [==============================] - 0s 756us/step - loss: 1.2451 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00542: loss did not improve from 1.23979\n",
      "Epoch 543/2000\n",
      "240/240 [==============================] - 0s 632us/step - loss: 1.2414 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00543: loss did not improve from 1.23979\n",
      "Epoch 544/2000\n",
      "240/240 [==============================] - 0s 637us/step - loss: 1.2413 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00544: loss did not improve from 1.23979\n",
      "Epoch 545/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 1.2408 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00545: loss did not improve from 1.23979\n",
      "Epoch 546/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 1.2404 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00546: loss did not improve from 1.23979\n",
      "Epoch 547/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2403 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00547: loss did not improve from 1.23979\n",
      "Epoch 548/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.2402 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00548: loss did not improve from 1.23979\n",
      "Epoch 549/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 1.2397 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00549: loss improved from 1.23979 to 1.23971, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 550/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.2400 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00550: loss did not improve from 1.23971\n",
      "Epoch 551/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.2401 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00551: loss did not improve from 1.23971\n",
      "Epoch 552/2000\n",
      "240/240 [==============================] - 0s 819us/step - loss: 1.2479 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00552: loss did not improve from 1.23971\n",
      "Epoch 553/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 1.2421 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00553: loss did not improve from 1.23971\n",
      "Epoch 554/2000\n",
      "240/240 [==============================] - 0s 869us/step - loss: 1.2411 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00554: loss did not improve from 1.23971\n",
      "Epoch 555/2000\n",
      "240/240 [==============================] - 0s 765us/step - loss: 1.2403 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00555: loss did not improve from 1.23971\n",
      "Epoch 556/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 1.2403 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00556: loss did not improve from 1.23971\n",
      "Epoch 557/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 1.2404 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00557: loss did not improve from 1.23971\n",
      "Epoch 558/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2403 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00558: loss did not improve from 1.23971\n",
      "Epoch 559/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.2402 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00559: loss did not improve from 1.23971\n",
      "Epoch 560/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 1.2398 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00560: loss did not improve from 1.23971\n",
      "Epoch 561/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.2399 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00561: loss did not improve from 1.23971\n",
      "Epoch 562/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2409 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00562: loss did not improve from 1.23971\n",
      "Epoch 563/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.2398 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00563: loss did not improve from 1.23971\n",
      "Epoch 564/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2400 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00564: loss did not improve from 1.23971\n",
      "Epoch 565/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2397 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00565: loss improved from 1.23971 to 1.23969, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 566/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 1.2395 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00566: loss improved from 1.23969 to 1.23954, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 567/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.2402 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00567: loss did not improve from 1.23954\n",
      "Epoch 568/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.2402 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00568: loss did not improve from 1.23954\n",
      "Epoch 569/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 1.2405 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00569: loss did not improve from 1.23954\n",
      "Epoch 570/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 1.2410 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00570: loss did not improve from 1.23954\n",
      "Epoch 571/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.2402 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00571: loss did not improve from 1.23954\n",
      "Epoch 572/2000\n",
      "240/240 [==============================] - 0s 975us/step - loss: 1.2403 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00572: loss did not improve from 1.23954\n",
      "Epoch 573/2000\n",
      "240/240 [==============================] - 0s 810us/step - loss: 1.2398 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00573: loss did not improve from 1.23954\n",
      "Epoch 574/2000\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 1.2398 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00574: loss did not improve from 1.23954\n",
      "Epoch 575/2000\n",
      "240/240 [==============================] - 0s 787us/step - loss: 1.2405 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00575: loss did not improve from 1.23954\n",
      "Epoch 576/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.2401 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00576: loss did not improve from 1.23954\n",
      "Epoch 577/2000\n",
      "240/240 [==============================] - 0s 640us/step - loss: 1.2398 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00577: loss did not improve from 1.23954\n",
      "Epoch 578/2000\n",
      "240/240 [==============================] - 0s 627us/step - loss: 1.2410 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00578: loss did not improve from 1.23954\n",
      "Epoch 579/2000\n",
      "240/240 [==============================] - 0s 613us/step - loss: 1.2408 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00579: loss did not improve from 1.23954\n",
      "Epoch 580/2000\n",
      "240/240 [==============================] - 0s 625us/step - loss: 1.2417 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00580: loss did not improve from 1.23954\n",
      "Epoch 581/2000\n",
      "240/240 [==============================] - 0s 627us/step - loss: 1.2414 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00581: loss did not improve from 1.23954\n",
      "Epoch 582/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2409 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00582: loss did not improve from 1.23954\n",
      "Epoch 583/2000\n",
      "240/240 [==============================] - 0s 652us/step - loss: 1.2426 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00583: loss did not improve from 1.23954\n",
      "Epoch 584/2000\n",
      "240/240 [==============================] - 0s 652us/step - loss: 1.2415 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00584: loss did not improve from 1.23954\n",
      "Epoch 585/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 642us/step - loss: 1.2399 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00585: loss did not improve from 1.23954\n",
      "Epoch 586/2000\n",
      "240/240 [==============================] - 0s 633us/step - loss: 1.2396 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00586: loss did not improve from 1.23954\n",
      "Epoch 587/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.2396 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00587: loss did not improve from 1.23954\n",
      "Epoch 588/2000\n",
      "240/240 [==============================] - 0s 650us/step - loss: 1.2394 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00588: loss improved from 1.23954 to 1.23942, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 589/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2398 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00589: loss did not improve from 1.23942\n",
      "Epoch 590/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.2396 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00590: loss did not improve from 1.23942\n",
      "Epoch 591/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 1.2400 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00591: loss did not improve from 1.23942\n",
      "Epoch 592/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.2396 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00592: loss did not improve from 1.23942\n",
      "Epoch 593/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.2402 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00593: loss did not improve from 1.23942\n",
      "Epoch 594/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2410 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00594: loss did not improve from 1.23942\n",
      "Epoch 595/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.2468 - accuracy: 0.4708\n",
      "\n",
      "Epoch 00595: loss did not improve from 1.23942\n",
      "Epoch 596/2000\n",
      "240/240 [==============================] - 0s 780us/step - loss: 1.2456 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00596: loss did not improve from 1.23942\n",
      "Epoch 597/2000\n",
      "240/240 [==============================] - 0s 780us/step - loss: 1.2461 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00597: loss did not improve from 1.23942\n",
      "Epoch 598/2000\n",
      "240/240 [==============================] - 0s 818us/step - loss: 1.2446 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00598: loss did not improve from 1.23942\n",
      "Epoch 599/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.2409 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00599: loss did not improve from 1.23942\n",
      "Epoch 600/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.2391 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00600: loss improved from 1.23942 to 1.23915, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 601/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.2393 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00601: loss did not improve from 1.23915\n",
      "Epoch 602/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.2392 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00602: loss did not improve from 1.23915\n",
      "Epoch 603/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.2390 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00603: loss improved from 1.23915 to 1.23895, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 604/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.2391 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00604: loss did not improve from 1.23895\n",
      "Epoch 605/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 1.2392 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00605: loss did not improve from 1.23895\n",
      "Epoch 606/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.2390 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00606: loss did not improve from 1.23895\n",
      "Epoch 607/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 1.2403 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00607: loss did not improve from 1.23895\n",
      "Epoch 608/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.2397 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00608: loss did not improve from 1.23895\n",
      "Epoch 609/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2402 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00609: loss did not improve from 1.23895\n",
      "Epoch 610/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.2391 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00610: loss did not improve from 1.23895\n",
      "Epoch 611/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2390 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00611: loss did not improve from 1.23895\n",
      "Epoch 612/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.2393 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00612: loss did not improve from 1.23895\n",
      "Epoch 613/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.2387 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00613: loss improved from 1.23895 to 1.23872, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 614/2000\n",
      "240/240 [==============================] - 0s 844us/step - loss: 1.2395 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00614: loss did not improve from 1.23872\n",
      "Epoch 615/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.2386 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00615: loss improved from 1.23872 to 1.23855, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 616/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 1.2396 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00616: loss did not improve from 1.23855\n",
      "Epoch 617/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 1.2402 - accuracy: 0.4625\n",
      "\n",
      "Epoch 00617: loss did not improve from 1.23855\n",
      "Epoch 618/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.2413 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00618: loss did not improve from 1.23855\n",
      "Epoch 619/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 1.2406 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00619: loss did not improve from 1.23855\n",
      "Epoch 620/2000\n",
      "240/240 [==============================] - 0s 732us/step - loss: 1.2392 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00620: loss did not improve from 1.23855\n",
      "Epoch 621/2000\n",
      "240/240 [==============================] - 0s 733us/step - loss: 1.2411 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00621: loss did not improve from 1.23855\n",
      "Epoch 622/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.2435 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00622: loss did not improve from 1.23855\n",
      "Epoch 623/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.2455 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00623: loss did not improve from 1.23855\n",
      "Epoch 624/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 1.2397 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00624: loss did not improve from 1.23855\n",
      "Epoch 625/2000\n",
      "240/240 [==============================] - 0s 734us/step - loss: 1.2393 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00625: loss did not improve from 1.23855\n",
      "Epoch 626/2000\n",
      "240/240 [==============================] - 0s 737us/step - loss: 1.2584 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00626: loss did not improve from 1.23855\n",
      "Epoch 627/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.2415 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00627: loss did not improve from 1.23855\n",
      "Epoch 628/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 1.2425 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00628: loss did not improve from 1.23855\n",
      "Epoch 629/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.2396 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00629: loss did not improve from 1.23855\n",
      "Epoch 630/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.2396 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00630: loss did not improve from 1.23855\n",
      "Epoch 631/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.2388 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00631: loss did not improve from 1.23855\n",
      "Epoch 632/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.2391 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00632: loss did not improve from 1.23855\n",
      "Epoch 633/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.2391 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00633: loss did not improve from 1.23855\n",
      "Epoch 634/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2392 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00634: loss did not improve from 1.23855\n",
      "Epoch 635/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 1.2803 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00635: loss did not improve from 1.23855\n",
      "Epoch 636/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.2466 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00636: loss did not improve from 1.23855\n",
      "Epoch 637/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 1.2435 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00637: loss did not improve from 1.23855\n",
      "Epoch 638/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.2411 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00638: loss did not improve from 1.23855\n",
      "Epoch 639/2000\n",
      "240/240 [==============================] - 0s 632us/step - loss: 1.2400 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00639: loss did not improve from 1.23855\n",
      "Epoch 640/2000\n",
      "240/240 [==============================] - 0s 744us/step - loss: 1.2394 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00640: loss did not improve from 1.23855\n",
      "Epoch 641/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.2394 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00641: loss did not improve from 1.23855\n",
      "Epoch 642/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.2391 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00642: loss did not improve from 1.23855\n",
      "Epoch 643/2000\n",
      "240/240 [==============================] - 0s 852us/step - loss: 1.2390 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00643: loss did not improve from 1.23855\n",
      "Epoch 644/2000\n",
      "240/240 [==============================] - ETA: 0s - loss: 1.2122 - accuracy: 0.43 - 0s 744us/step - loss: 1.2389 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00644: loss did not improve from 1.23855\n",
      "Epoch 645/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2392 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00645: loss did not improve from 1.23855\n",
      "Epoch 646/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.2398 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00646: loss did not improve from 1.23855\n",
      "Epoch 647/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2390 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00647: loss did not improve from 1.23855\n",
      "Epoch 648/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.2391 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00648: loss did not improve from 1.23855\n",
      "Epoch 649/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 1.2394 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00649: loss did not improve from 1.23855\n",
      "Epoch 650/2000\n",
      "240/240 [==============================] - 0s 664us/step - loss: 1.2397 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00650: loss did not improve from 1.23855\n",
      "Epoch 651/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.2391 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00651: loss did not improve from 1.23855\n",
      "Epoch 652/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 1.2396 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00652: loss did not improve from 1.23855\n",
      "Epoch 653/2000\n",
      "240/240 [==============================] - 0s 839us/step - loss: 1.2392 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00653: loss did not improve from 1.23855\n",
      "Epoch 654/2000\n",
      "240/240 [==============================] - 0s 740us/step - loss: 1.2390 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00654: loss did not improve from 1.23855\n",
      "Epoch 655/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 1.2388 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00655: loss did not improve from 1.23855\n",
      "Epoch 656/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.2387 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00656: loss did not improve from 1.23855\n",
      "Epoch 657/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.2389 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00657: loss did not improve from 1.23855\n",
      "Epoch 658/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.2395 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00658: loss did not improve from 1.23855\n",
      "Epoch 659/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.2396 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00659: loss did not improve from 1.23855\n",
      "Epoch 660/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.2389 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00660: loss did not improve from 1.23855\n",
      "Epoch 661/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 1.2389 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00661: loss did not improve from 1.23855\n",
      "Epoch 662/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.2385 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00662: loss improved from 1.23855 to 1.23854, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 663/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.2397 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00663: loss did not improve from 1.23854\n",
      "Epoch 664/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.2387 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00664: loss did not improve from 1.23854\n",
      "Epoch 665/2000\n",
      "240/240 [==============================] - 0s 808us/step - loss: 1.2388 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00665: loss did not improve from 1.23854\n",
      "Epoch 666/2000\n",
      "240/240 [==============================] - 0s 826us/step - loss: 1.2392 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00666: loss did not improve from 1.23854\n",
      "Epoch 667/2000\n",
      "240/240 [==============================] - 0s 838us/step - loss: 1.2408 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00667: loss did not improve from 1.23854\n",
      "Epoch 668/2000\n",
      "240/240 [==============================] - 0s 787us/step - loss: 1.2398 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00668: loss did not improve from 1.23854\n",
      "Epoch 669/2000\n",
      "240/240 [==============================] - 0s 658us/step - loss: 1.2432 - accuracy: 0.4625\n",
      "\n",
      "Epoch 00669: loss did not improve from 1.23854\n",
      "Epoch 670/2000\n",
      "240/240 [==============================] - 0s 645us/step - loss: 1.2395 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00670: loss did not improve from 1.23854\n",
      "Epoch 671/2000\n",
      "240/240 [==============================] - 0s 594us/step - loss: 1.2390 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00671: loss did not improve from 1.23854\n",
      "Epoch 672/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.2385 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00672: loss improved from 1.23854 to 1.23848, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 673/2000\n",
      "240/240 [==============================] - 0s 739us/step - loss: 1.2390 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00673: loss did not improve from 1.23848\n",
      "Epoch 674/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 1.2420 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00674: loss did not improve from 1.23848\n",
      "Epoch 675/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 1.2392 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00675: loss did not improve from 1.23848\n",
      "Epoch 676/2000\n",
      "240/240 [==============================] - 0s 722us/step - loss: 1.2386 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00676: loss did not improve from 1.23848\n",
      "Epoch 677/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 1.2395 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00677: loss did not improve from 1.23848\n",
      "Epoch 678/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.2389 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00678: loss did not improve from 1.23848\n",
      "Epoch 679/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 1.2397 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00679: loss did not improve from 1.23848\n",
      "Epoch 680/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.2389 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00680: loss did not improve from 1.23848\n",
      "Epoch 681/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.2423 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00681: loss did not improve from 1.23848\n",
      "Epoch 682/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.2405 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00682: loss did not improve from 1.23848\n",
      "Epoch 683/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.2396 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00683: loss did not improve from 1.23848\n",
      "Epoch 684/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2398 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00684: loss did not improve from 1.23848\n",
      "Epoch 685/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2394 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00685: loss did not improve from 1.23848\n",
      "Epoch 686/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.2390 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00686: loss did not improve from 1.23848\n",
      "Epoch 687/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.2390 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00687: loss did not improve from 1.23848\n",
      "Epoch 688/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 703us/step - loss: 1.2390 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00688: loss did not improve from 1.23848\n",
      "Epoch 689/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 1.2387 - accuracy: 0.4625\n",
      "\n",
      "Epoch 00689: loss did not improve from 1.23848\n",
      "Epoch 690/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.2392 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00690: loss did not improve from 1.23848\n",
      "Epoch 691/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.2388 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00691: loss did not improve from 1.23848\n",
      "Epoch 692/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.2399 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00692: loss did not improve from 1.23848\n",
      "Epoch 693/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 1.2387 - accuracy: 0.4667\n",
      "\n",
      "Epoch 00693: loss did not improve from 1.23848\n",
      "Epoch 694/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.2399 - accuracy: 0.4625\n",
      "\n",
      "Epoch 00694: loss did not improve from 1.23848\n",
      "Epoch 695/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.2412 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00695: loss did not improve from 1.23848\n",
      "Epoch 696/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2430 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00696: loss did not improve from 1.23848\n",
      "Epoch 697/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 1.2401 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00697: loss did not improve from 1.23848\n",
      "Epoch 698/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.2385 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00698: loss improved from 1.23848 to 1.23848, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 699/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.2386 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00699: loss did not improve from 1.23848\n",
      "Epoch 700/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.2395 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00700: loss did not improve from 1.23848\n",
      "Epoch 701/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.2387 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00701: loss did not improve from 1.23848\n",
      "Epoch 702/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.2388 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00702: loss did not improve from 1.23848\n",
      "Epoch 703/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2440 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00703: loss did not improve from 1.23848\n",
      "Epoch 704/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.2429 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00704: loss did not improve from 1.23848\n",
      "Epoch 705/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.2416 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00705: loss did not improve from 1.23848\n",
      "Epoch 706/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.2398 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00706: loss did not improve from 1.23848\n",
      "Epoch 707/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.2393 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00707: loss did not improve from 1.23848\n",
      "Epoch 708/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2385 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00708: loss did not improve from 1.23848\n",
      "Epoch 709/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.2386 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00709: loss did not improve from 1.23848\n",
      "Epoch 710/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.2388 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00710: loss did not improve from 1.23848\n",
      "Epoch 711/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.2390 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00711: loss did not improve from 1.23848\n",
      "Epoch 712/2000\n",
      "240/240 [==============================] - 0s 877us/step - loss: 1.2395 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00712: loss did not improve from 1.23848\n",
      "Epoch 713/2000\n",
      "240/240 [==============================] - 0s 755us/step - loss: 1.2407 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00713: loss did not improve from 1.23848\n",
      "Epoch 714/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2402 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00714: loss did not improve from 1.23848\n",
      "Epoch 715/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.2407 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00715: loss did not improve from 1.23848\n",
      "Epoch 716/2000\n",
      "240/240 [==============================] - 0s 672us/step - loss: 1.2395 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00716: loss did not improve from 1.23848\n",
      "Epoch 717/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.2394 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00717: loss did not improve from 1.23848\n",
      "Epoch 718/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.2389 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00718: loss did not improve from 1.23848\n",
      "Epoch 719/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.2385 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00719: loss did not improve from 1.23848\n",
      "Epoch 720/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.2389 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00720: loss did not improve from 1.23848\n",
      "Epoch 721/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.2411 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00721: loss did not improve from 1.23848\n",
      "Epoch 722/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.2397 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00722: loss did not improve from 1.23848\n",
      "Epoch 723/2000\n",
      "240/240 [==============================] - 0s 769us/step - loss: 1.2489 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00723: loss did not improve from 1.23848\n",
      "Epoch 724/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.4508 - accuracy: 0.4000\n",
      "\n",
      "Epoch 00724: loss did not improve from 1.23848\n",
      "Epoch 725/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.2446 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00725: loss did not improve from 1.23848\n",
      "Epoch 726/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2415 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00726: loss did not improve from 1.23848\n",
      "Epoch 727/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.2404 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00727: loss did not improve from 1.23848\n",
      "Epoch 728/2000\n",
      "240/240 [==============================] - 0s 625us/step - loss: 1.2399 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00728: loss did not improve from 1.23848\n",
      "Epoch 729/2000\n",
      "240/240 [==============================] - 0s 785us/step - loss: 1.2393 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00729: loss did not improve from 1.23848\n",
      "Epoch 730/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 1.2388 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00730: loss did not improve from 1.23848\n",
      "Epoch 731/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 1.2390 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00731: loss did not improve from 1.23848\n",
      "Epoch 732/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.2396 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00732: loss did not improve from 1.23848\n",
      "Epoch 733/2000\n",
      "240/240 [==============================] - 0s 726us/step - loss: 1.2387 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00733: loss did not improve from 1.23848\n",
      "Epoch 734/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 1.2381 - accuracy: 0.4667\n",
      "\n",
      "Epoch 00734: loss improved from 1.23848 to 1.23814, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 735/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.2384 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00735: loss did not improve from 1.23814\n",
      "Epoch 736/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.2383 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00736: loss did not improve from 1.23814\n",
      "Epoch 737/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2387 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00737: loss did not improve from 1.23814\n",
      "Epoch 738/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 1.2397 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00738: loss did not improve from 1.23814\n",
      "Epoch 739/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.2385 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00739: loss did not improve from 1.23814\n",
      "Epoch 740/2000\n",
      "240/240 [==============================] - 0s 672us/step - loss: 1.2388 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00740: loss did not improve from 1.23814\n",
      "Epoch 741/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.2385 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00741: loss did not improve from 1.23814\n",
      "Epoch 742/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.2380 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00742: loss improved from 1.23814 to 1.23800, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 743/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.2381 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00743: loss did not improve from 1.23800\n",
      "Epoch 744/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 1.2379 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00744: loss improved from 1.23800 to 1.23795, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 745/2000\n",
      "240/240 [==============================] - 0s 718us/step - loss: 1.2383 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00745: loss did not improve from 1.23795\n",
      "Epoch 746/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.2385 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00746: loss did not improve from 1.23795\n",
      "Epoch 747/2000\n",
      "240/240 [==============================] - 0s 722us/step - loss: 1.2385 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00747: loss did not improve from 1.23795\n",
      "Epoch 748/2000\n",
      "240/240 [==============================] - 0s 722us/step - loss: 1.2384 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00748: loss did not improve from 1.23795\n",
      "Epoch 749/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.2382 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00749: loss did not improve from 1.23795\n",
      "Epoch 750/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.2382 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00750: loss did not improve from 1.23795\n",
      "Epoch 751/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.2401 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00751: loss did not improve from 1.23795\n",
      "Epoch 752/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.2409 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00752: loss did not improve from 1.23795\n",
      "Epoch 753/2000\n",
      "240/240 [==============================] - 0s 739us/step - loss: 1.2406 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00753: loss did not improve from 1.23795\n",
      "Epoch 754/2000\n",
      "240/240 [==============================] - 0s 722us/step - loss: 1.2392 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00754: loss did not improve from 1.23795\n",
      "Epoch 755/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.2385 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00755: loss did not improve from 1.23795\n",
      "Epoch 756/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 1.2385 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00756: loss did not improve from 1.23795\n",
      "Epoch 757/2000\n",
      "240/240 [==============================] - 0s 718us/step - loss: 1.2381 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00757: loss did not improve from 1.23795\n",
      "Epoch 758/2000\n",
      "240/240 [==============================] - 0s 780us/step - loss: 1.2379 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00758: loss improved from 1.23795 to 1.23793, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 759/2000\n",
      "240/240 [==============================] - 0s 795us/step - loss: 1.2384 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00759: loss did not improve from 1.23793\n",
      "Epoch 760/2000\n",
      "240/240 [==============================] - 0s 824us/step - loss: 1.2381 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00760: loss did not improve from 1.23793\n",
      "Epoch 761/2000\n",
      "240/240 [==============================] - 0s 785us/step - loss: 1.2392 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00761: loss did not improve from 1.23793\n",
      "Epoch 762/2000\n",
      "240/240 [==============================] - 0s 660us/step - loss: 1.2383 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00762: loss did not improve from 1.23793\n",
      "Epoch 763/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2388 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00763: loss did not improve from 1.23793\n",
      "Epoch 764/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 1.2398 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00764: loss did not improve from 1.23793\n",
      "Epoch 765/2000\n",
      "240/240 [==============================] - 0s 624us/step - loss: 1.2402 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00765: loss did not improve from 1.23793\n",
      "Epoch 766/2000\n",
      "240/240 [==============================] - 0s 614us/step - loss: 1.2387 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00766: loss did not improve from 1.23793\n",
      "Epoch 767/2000\n",
      "240/240 [==============================] - 0s 652us/step - loss: 1.2381 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00767: loss did not improve from 1.23793\n",
      "Epoch 768/2000\n",
      "240/240 [==============================] - 0s 633us/step - loss: 1.2382 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00768: loss did not improve from 1.23793\n",
      "Epoch 769/2000\n",
      "240/240 [==============================] - 0s 733us/step - loss: 1.2387 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00769: loss did not improve from 1.23793\n",
      "Epoch 770/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.2388 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00770: loss did not improve from 1.23793\n",
      "Epoch 771/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.2384 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00771: loss did not improve from 1.23793\n",
      "Epoch 772/2000\n",
      "240/240 [==============================] - 0s 627us/step - loss: 1.2394 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00772: loss did not improve from 1.23793\n",
      "Epoch 773/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.2418 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00773: loss did not improve from 1.23793\n",
      "Epoch 774/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.2388 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00774: loss did not improve from 1.23793\n",
      "Epoch 775/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.2431 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00775: loss did not improve from 1.23793\n",
      "Epoch 776/2000\n",
      "240/240 [==============================] - 0s 625us/step - loss: 1.2398 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00776: loss did not improve from 1.23793\n",
      "Epoch 777/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 1.2387 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00777: loss did not improve from 1.23793\n",
      "Epoch 778/2000\n",
      "240/240 [==============================] - 0s 608us/step - loss: 1.2384 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00778: loss did not improve from 1.23793\n",
      "Epoch 779/2000\n",
      "240/240 [==============================] - 0s 641us/step - loss: 1.2383 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00779: loss did not improve from 1.23793\n",
      "Epoch 780/2000\n",
      "240/240 [==============================] - 0s 615us/step - loss: 1.2378 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00780: loss improved from 1.23793 to 1.23785, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 781/2000\n",
      "240/240 [==============================] - 0s 662us/step - loss: 1.2385 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00781: loss did not improve from 1.23785\n",
      "Epoch 782/2000\n",
      "240/240 [==============================] - 0s 632us/step - loss: 1.2378 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00782: loss improved from 1.23785 to 1.23784, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 783/2000\n",
      "240/240 [==============================] - 0s 772us/step - loss: 1.2380 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00783: loss did not improve from 1.23784\n",
      "Epoch 784/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.2383 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00784: loss did not improve from 1.23784\n",
      "Epoch 785/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 1.2389 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00785: loss did not improve from 1.23784\n",
      "Epoch 786/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.2389 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00786: loss did not improve from 1.23784\n",
      "Epoch 787/2000\n",
      "240/240 [==============================] - 0s 672us/step - loss: 1.2462 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00787: loss did not improve from 1.23784\n",
      "Epoch 788/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 1.2461 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00788: loss did not improve from 1.23784\n",
      "Epoch 789/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.2407 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00789: loss did not improve from 1.23784\n",
      "Epoch 790/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.2387 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00790: loss did not improve from 1.23784\n",
      "Epoch 791/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 712us/step - loss: 1.2384 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00791: loss did not improve from 1.23784\n",
      "Epoch 792/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.2382 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00792: loss did not improve from 1.23784\n",
      "Epoch 793/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.2379 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00793: loss did not improve from 1.23784\n",
      "Epoch 794/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.2377 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00794: loss improved from 1.23784 to 1.23773, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 795/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.2379 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00795: loss did not improve from 1.23773\n",
      "Epoch 796/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.2409 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00796: loss did not improve from 1.23773\n",
      "Epoch 797/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 1.2406 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00797: loss did not improve from 1.23773\n",
      "Epoch 798/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.2418 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00798: loss did not improve from 1.23773\n",
      "Epoch 799/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.2392 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00799: loss did not improve from 1.23773\n",
      "Epoch 800/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.2380 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00800: loss did not improve from 1.23773\n",
      "Epoch 801/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.2379 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00801: loss did not improve from 1.23773\n",
      "Epoch 802/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.2376 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00802: loss improved from 1.23773 to 1.23758, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 803/2000\n",
      "240/240 [==============================] - 0s 757us/step - loss: 1.2377 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00803: loss did not improve from 1.23758\n",
      "Epoch 804/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 1.2377 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00804: loss did not improve from 1.23758\n",
      "Epoch 805/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.2383 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00805: loss did not improve from 1.23758\n",
      "Epoch 806/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.2382 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00806: loss did not improve from 1.23758\n",
      "Epoch 807/2000\n",
      "240/240 [==============================] - 0s 750us/step - loss: 1.2399 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00807: loss did not improve from 1.23758\n",
      "Epoch 808/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.2398 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00808: loss did not improve from 1.23758\n",
      "Epoch 809/2000\n",
      "240/240 [==============================] - 0s 732us/step - loss: 1.2401 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00809: loss did not improve from 1.23758\n",
      "Epoch 810/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 1.2396 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00810: loss did not improve from 1.23758\n",
      "Epoch 811/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.2385 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00811: loss did not improve from 1.23758\n",
      "Epoch 812/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 1.2383 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00812: loss did not improve from 1.23758\n",
      "Epoch 813/2000\n",
      "240/240 [==============================] - 0s 729us/step - loss: 1.2377 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00813: loss did not improve from 1.23758\n",
      "Epoch 814/2000\n",
      "240/240 [==============================] - 0s 733us/step - loss: 1.2377 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00814: loss did not improve from 1.23758\n",
      "Epoch 815/2000\n",
      "240/240 [==============================] - 0s 734us/step - loss: 1.2377 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00815: loss did not improve from 1.23758\n",
      "Epoch 816/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.2378 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00816: loss did not improve from 1.23758\n",
      "Epoch 817/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.2385 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00817: loss did not improve from 1.23758\n",
      "Epoch 818/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.2392 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00818: loss did not improve from 1.23758\n",
      "Epoch 819/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.2438 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00819: loss did not improve from 1.23758\n",
      "Epoch 820/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.2408 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00820: loss did not improve from 1.23758\n",
      "Epoch 821/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.2400 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00821: loss did not improve from 1.23758\n",
      "Epoch 822/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.2386 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00822: loss did not improve from 1.23758\n",
      "Epoch 823/2000\n",
      "240/240 [==============================] - 0s 743us/step - loss: 1.2382 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00823: loss did not improve from 1.23758\n",
      "Epoch 824/2000\n",
      "240/240 [==============================] - 0s 753us/step - loss: 1.2376 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00824: loss did not improve from 1.23758\n",
      "Epoch 825/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.2378 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00825: loss did not improve from 1.23758\n",
      "Epoch 826/2000\n",
      "240/240 [==============================] - 0s 740us/step - loss: 1.2376 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00826: loss did not improve from 1.23758\n",
      "Epoch 827/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 1.2382 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00827: loss did not improve from 1.23758\n",
      "Epoch 828/2000\n",
      "240/240 [==============================] - 0s 737us/step - loss: 1.2428 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00828: loss did not improve from 1.23758\n",
      "Epoch 829/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.2429 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00829: loss did not improve from 1.23758\n",
      "Epoch 830/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.2414 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00830: loss did not improve from 1.23758\n",
      "Epoch 831/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.2387 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00831: loss did not improve from 1.23758\n",
      "Epoch 832/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 1.2479 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00832: loss did not improve from 1.23758\n",
      "Epoch 833/2000\n",
      "240/240 [==============================] - 0s 789us/step - loss: 1.2393 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00833: loss did not improve from 1.23758\n",
      "Epoch 834/2000\n",
      "240/240 [==============================] - 0s 759us/step - loss: 1.2384 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00834: loss did not improve from 1.23758\n",
      "Epoch 835/2000\n",
      "240/240 [==============================] - 0s 815us/step - loss: 1.2381 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00835: loss did not improve from 1.23758\n",
      "Epoch 836/2000\n",
      "240/240 [==============================] - 0s 751us/step - loss: 1.2378 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00836: loss did not improve from 1.23758\n",
      "Epoch 837/2000\n",
      "240/240 [==============================] - 0s 785us/step - loss: 1.2394 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00837: loss did not improve from 1.23758\n",
      "Epoch 838/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.2389 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00838: loss did not improve from 1.23758\n",
      "Epoch 839/2000\n",
      "240/240 [==============================] - 0s 697us/step - loss: 1.2379 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00839: loss did not improve from 1.23758\n",
      "Epoch 840/2000\n",
      "240/240 [==============================] - 0s 655us/step - loss: 1.2373 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00840: loss improved from 1.23758 to 1.23734, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 841/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.2377 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00841: loss did not improve from 1.23734\n",
      "Epoch 842/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.2377 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00842: loss did not improve from 1.23734\n",
      "Epoch 843/2000\n",
      "240/240 [==============================] - 0s 718us/step - loss: 1.2379 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00843: loss did not improve from 1.23734\n",
      "Epoch 844/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.2377 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00844: loss did not improve from 1.23734\n",
      "Epoch 845/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 1.2379 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00845: loss did not improve from 1.23734\n",
      "Epoch 846/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.2379 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00846: loss did not improve from 1.23734\n",
      "Epoch 847/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.2395 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00847: loss did not improve from 1.23734\n",
      "Epoch 848/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 1.2400 - accuracy: 0.4667\n",
      "\n",
      "Epoch 00848: loss did not improve from 1.23734\n",
      "Epoch 849/2000\n",
      "240/240 [==============================] - 0s 740us/step - loss: 1.2383 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00849: loss did not improve from 1.23734\n",
      "Epoch 850/2000\n",
      "240/240 [==============================] - 0s 745us/step - loss: 1.2378 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00850: loss did not improve from 1.23734\n",
      "Epoch 851/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 1.2377 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00851: loss did not improve from 1.23734\n",
      "Epoch 852/2000\n",
      "240/240 [==============================] - 0s 800us/step - loss: 1.2379 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00852: loss did not improve from 1.23734\n",
      "Epoch 853/2000\n",
      "240/240 [==============================] - 0s 873us/step - loss: 1.2381 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00853: loss did not improve from 1.23734\n",
      "Epoch 854/2000\n",
      "240/240 [==============================] - 0s 796us/step - loss: 1.2387 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00854: loss did not improve from 1.23734\n",
      "Epoch 855/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 1.2399 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00855: loss did not improve from 1.23734\n",
      "Epoch 856/2000\n",
      "240/240 [==============================] - 0s 633us/step - loss: 1.2511 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00856: loss did not improve from 1.23734\n",
      "Epoch 857/2000\n",
      "240/240 [==============================] - 0s 650us/step - loss: 1.2397 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00857: loss did not improve from 1.23734\n",
      "Epoch 858/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2385 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00858: loss did not improve from 1.23734\n",
      "Epoch 859/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.2382 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00859: loss did not improve from 1.23734\n",
      "Epoch 860/2000\n",
      "240/240 [==============================] - ETA: 0s - loss: 1.2610 - accuracy: 0.43 - 0s 690us/step - loss: 1.2378 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00860: loss did not improve from 1.23734\n",
      "Epoch 861/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.2402 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00861: loss did not improve from 1.23734\n",
      "Epoch 862/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 1.2408 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00862: loss did not improve from 1.23734\n",
      "Epoch 863/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.2384 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00863: loss did not improve from 1.23734\n",
      "Epoch 864/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.2382 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00864: loss did not improve from 1.23734\n",
      "Epoch 865/2000\n",
      "240/240 [==============================] - 0s 745us/step - loss: 1.2379 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00865: loss did not improve from 1.23734\n",
      "Epoch 866/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.2374 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00866: loss did not improve from 1.23734\n",
      "Epoch 867/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 1.2405 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00867: loss did not improve from 1.23734\n",
      "Epoch 868/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.2384 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00868: loss did not improve from 1.23734\n",
      "Epoch 869/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.2379 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00869: loss did not improve from 1.23734\n",
      "Epoch 870/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.2410 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00870: loss did not improve from 1.23734\n",
      "Epoch 871/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.2413 - accuracy: 0.4625\n",
      "\n",
      "Epoch 00871: loss did not improve from 1.23734\n",
      "Epoch 872/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 1.2405 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00872: loss did not improve from 1.23734\n",
      "Epoch 873/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.2382 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00873: loss did not improve from 1.23734\n",
      "Epoch 874/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.2376 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00874: loss did not improve from 1.23734\n",
      "Epoch 875/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.2375 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00875: loss did not improve from 1.23734\n",
      "Epoch 876/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 1.2375 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00876: loss did not improve from 1.23734\n",
      "Epoch 877/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.2376 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00877: loss did not improve from 1.23734\n",
      "Epoch 878/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2376 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00878: loss did not improve from 1.23734\n",
      "Epoch 879/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2376 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00879: loss did not improve from 1.23734\n",
      "Epoch 880/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2373 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00880: loss improved from 1.23734 to 1.23732, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 881/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 1.2382 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00881: loss did not improve from 1.23732\n",
      "Epoch 882/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.2394 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00882: loss did not improve from 1.23732\n",
      "Epoch 883/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.2430 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00883: loss did not improve from 1.23732\n",
      "Epoch 884/2000\n",
      "240/240 [==============================] - 0s 672us/step - loss: 1.2404 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00884: loss did not improve from 1.23732\n",
      "Epoch 885/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.2388 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00885: loss did not improve from 1.23732\n",
      "Epoch 886/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.2389 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00886: loss did not improve from 1.23732\n",
      "Epoch 887/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.2380 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00887: loss did not improve from 1.23732\n",
      "Epoch 888/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.2378 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00888: loss did not improve from 1.23732\n",
      "Epoch 889/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2379 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00889: loss did not improve from 1.23732\n",
      "Epoch 890/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 1.2375 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00890: loss did not improve from 1.23732\n",
      "Epoch 891/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.2513 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00891: loss did not improve from 1.23732\n",
      "Epoch 892/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.2410 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00892: loss did not improve from 1.23732\n",
      "Epoch 893/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 1.2392 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00893: loss did not improve from 1.23732\n",
      "Epoch 894/2000\n",
      "240/240 [==============================] - 0s 653us/step - loss: 1.2383 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00894: loss did not improve from 1.23732\n",
      "Epoch 895/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.2378 - accuracy: 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00895: loss did not improve from 1.23732\n",
      "Epoch 896/2000\n",
      "240/240 [==============================] - 0s 619us/step - loss: 1.2378 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00896: loss did not improve from 1.23732\n",
      "Epoch 897/2000\n",
      "240/240 [==============================] - 0s 779us/step - loss: 1.2376 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00897: loss did not improve from 1.23732\n",
      "Epoch 898/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 1.2444 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00898: loss did not improve from 1.23732\n",
      "Epoch 899/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2391 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00899: loss did not improve from 1.23732\n",
      "Epoch 900/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.2386 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00900: loss did not improve from 1.23732\n",
      "Epoch 901/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.2385 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00901: loss did not improve from 1.23732\n",
      "Epoch 902/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 1.2381 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00902: loss did not improve from 1.23732\n",
      "Epoch 903/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.2378 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00903: loss did not improve from 1.23732\n",
      "Epoch 904/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.2378 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00904: loss did not improve from 1.23732\n",
      "Epoch 905/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.2376 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00905: loss did not improve from 1.23732\n",
      "Epoch 906/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2380 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00906: loss did not improve from 1.23732\n",
      "Epoch 907/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 1.2376 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00907: loss did not improve from 1.23732\n",
      "Epoch 908/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.2375 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00908: loss did not improve from 1.23732\n",
      "Epoch 909/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.2375 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00909: loss did not improve from 1.23732\n",
      "Epoch 910/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.2417 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00910: loss did not improve from 1.23732\n",
      "Epoch 911/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 1.2384 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00911: loss did not improve from 1.23732\n",
      "Epoch 912/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 1.2688 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00912: loss did not improve from 1.23732\n",
      "Epoch 913/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 1.2454 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00913: loss did not improve from 1.23732\n",
      "Epoch 914/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.2396 - accuracy: 0.4708\n",
      "\n",
      "Epoch 00914: loss did not improve from 1.23732\n",
      "Epoch 915/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 1.2386 - accuracy: 0.4625\n",
      "\n",
      "Epoch 00915: loss did not improve from 1.23732\n",
      "Epoch 916/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2471 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00916: loss did not improve from 1.23732\n",
      "Epoch 917/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 1.2423 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00917: loss did not improve from 1.23732\n",
      "Epoch 918/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 1.2383 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00918: loss did not improve from 1.23732\n",
      "Epoch 919/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.2380 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00919: loss did not improve from 1.23732\n",
      "Epoch 920/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.2380 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00920: loss did not improve from 1.23732\n",
      "Epoch 921/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.2377 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00921: loss did not improve from 1.23732\n",
      "Epoch 922/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.2375 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00922: loss did not improve from 1.23732\n",
      "Epoch 923/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.2375 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00923: loss did not improve from 1.23732\n",
      "Epoch 924/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 1.2372 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00924: loss improved from 1.23732 to 1.23721, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 925/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.2387 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00925: loss did not improve from 1.23721\n",
      "Epoch 926/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.2372 - accuracy: 0.4083\n",
      "\n",
      "Epoch 00926: loss improved from 1.23721 to 1.23720, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 927/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.2377 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00927: loss did not improve from 1.23720\n",
      "Epoch 928/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2372 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00928: loss did not improve from 1.23720\n",
      "Epoch 929/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.2373 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00929: loss did not improve from 1.23720\n",
      "Epoch 930/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.2372 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00930: loss improved from 1.23720 to 1.23718, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 931/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.2373 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00931: loss did not improve from 1.23718\n",
      "Epoch 932/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 1.2374 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00932: loss did not improve from 1.23718\n",
      "Epoch 933/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.2382 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00933: loss did not improve from 1.23718\n",
      "Epoch 934/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.2379 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00934: loss did not improve from 1.23718\n",
      "Epoch 935/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 1.2384 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00935: loss did not improve from 1.23718\n",
      "Epoch 936/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.2380 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00936: loss did not improve from 1.23718\n",
      "Epoch 937/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 1.2383 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00937: loss did not improve from 1.23718\n",
      "Epoch 938/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.2382 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00938: loss did not improve from 1.23718\n",
      "Epoch 939/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 1.2391 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00939: loss did not improve from 1.23718\n",
      "Epoch 940/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.2392 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00940: loss did not improve from 1.23718\n",
      "Epoch 941/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.2385 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00941: loss did not improve from 1.23718\n",
      "Epoch 942/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.2377 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00942: loss did not improve from 1.23718\n",
      "Epoch 943/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.2376 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00943: loss did not improve from 1.23718\n",
      "Epoch 944/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.2382 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00944: loss did not improve from 1.23718\n",
      "Epoch 945/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 1.2372 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00945: loss did not improve from 1.23718\n",
      "Epoch 946/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 1.2371 - accuracy: 0.4042\n",
      "\n",
      "Epoch 00946: loss improved from 1.23718 to 1.23715, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 947/2000\n",
      "240/240 [==============================] - 0s 791us/step - loss: 1.2376 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00947: loss did not improve from 1.23715\n",
      "Epoch 948/2000\n",
      "240/240 [==============================] - 0s 822us/step - loss: 1.2378 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00948: loss did not improve from 1.23715\n",
      "Epoch 949/2000\n",
      "240/240 [==============================] - 0s 813us/step - loss: 1.2400 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00949: loss did not improve from 1.23715\n",
      "Epoch 950/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.2437 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00950: loss did not improve from 1.23715\n",
      "Epoch 951/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.2397 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00951: loss did not improve from 1.23715\n",
      "Epoch 952/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.2380 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00952: loss did not improve from 1.23715\n",
      "Epoch 953/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 1.2374 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00953: loss did not improve from 1.23715\n",
      "Epoch 954/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.2374 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00954: loss did not improve from 1.23715\n",
      "Epoch 955/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2372 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00955: loss did not improve from 1.23715\n",
      "Epoch 956/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2372 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00956: loss did not improve from 1.23715\n",
      "Epoch 957/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 1.2372 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00957: loss did not improve from 1.23715\n",
      "Epoch 958/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.2370 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00958: loss improved from 1.23715 to 1.23705, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 959/2000\n",
      "240/240 [==============================] - 0s 744us/step - loss: 1.2371 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00959: loss did not improve from 1.23705\n",
      "Epoch 960/2000\n",
      "240/240 [==============================] - 0s 740us/step - loss: 1.2368 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00960: loss improved from 1.23705 to 1.23678, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 961/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.2373 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00961: loss did not improve from 1.23678\n",
      "Epoch 962/2000\n",
      "240/240 [==============================] - 0s 739us/step - loss: 1.2373 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00962: loss did not improve from 1.23678\n",
      "Epoch 963/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 1.2372 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00963: loss did not improve from 1.23678\n",
      "Epoch 964/2000\n",
      "240/240 [==============================] - 0s 732us/step - loss: 1.2374 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00964: loss did not improve from 1.23678\n",
      "Epoch 965/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 1.2373 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00965: loss did not improve from 1.23678\n",
      "Epoch 966/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 1.2369 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00966: loss did not improve from 1.23678\n",
      "Epoch 967/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 1.2371 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00967: loss did not improve from 1.23678\n",
      "Epoch 968/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.2374 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00968: loss did not improve from 1.23678\n",
      "Epoch 969/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.2371 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00969: loss did not improve from 1.23678\n",
      "Epoch 970/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 1.2386 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00970: loss did not improve from 1.23678\n",
      "Epoch 971/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.2383 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00971: loss did not improve from 1.23678\n",
      "Epoch 972/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 1.2388 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00972: loss did not improve from 1.23678\n",
      "Epoch 973/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.2415 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00973: loss did not improve from 1.23678\n",
      "Epoch 974/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.2389 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00974: loss did not improve from 1.23678\n",
      "Epoch 975/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 1.2375 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00975: loss did not improve from 1.23678\n",
      "Epoch 976/2000\n",
      "240/240 [==============================] - 0s 619us/step - loss: 1.2374 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00976: loss did not improve from 1.23678\n",
      "Epoch 977/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.2373 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00977: loss did not improve from 1.23678\n",
      "Epoch 978/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2372 - accuracy: 0.4542\n",
      "\n",
      "Epoch 00978: loss did not improve from 1.23678\n",
      "Epoch 979/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.2765 - accuracy: 0.4250\n",
      "\n",
      "Epoch 00979: loss did not improve from 1.23678\n",
      "Epoch 980/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.2460 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00980: loss did not improve from 1.23678\n",
      "Epoch 981/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 1.2588 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00981: loss did not improve from 1.23678\n",
      "Epoch 982/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.2407 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00982: loss did not improve from 1.23678\n",
      "Epoch 983/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.2385 - accuracy: 0.4125\n",
      "\n",
      "Epoch 00983: loss did not improve from 1.23678\n",
      "Epoch 984/2000\n",
      "240/240 [==============================] - 0s 898us/step - loss: 1.2382 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00984: loss did not improve from 1.23678\n",
      "Epoch 985/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 1.2378 - accuracy: 0.4583\n",
      "\n",
      "Epoch 00985: loss did not improve from 1.23678\n",
      "Epoch 986/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.2377 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00986: loss did not improve from 1.23678\n",
      "Epoch 987/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.2374 - accuracy: 0.4500\n",
      "\n",
      "Epoch 00987: loss did not improve from 1.23678\n",
      "Epoch 988/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.2376 - accuracy: 0.4417\n",
      "\n",
      "Epoch 00988: loss did not improve from 1.23678\n",
      "Epoch 989/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 1.2371 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00989: loss did not improve from 1.23678\n",
      "Epoch 990/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 1.2373 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00990: loss did not improve from 1.23678\n",
      "Epoch 991/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.2370 - accuracy: 0.4458\n",
      "\n",
      "Epoch 00991: loss did not improve from 1.23678\n",
      "Epoch 992/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.2371 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00992: loss did not improve from 1.23678\n",
      "Epoch 993/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2370 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00993: loss did not improve from 1.23678\n",
      "Epoch 994/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.2376 - accuracy: 0.4208\n",
      "\n",
      "Epoch 00994: loss did not improve from 1.23678\n",
      "Epoch 995/2000\n",
      "240/240 [==============================] - 0s 697us/step - loss: 1.2375 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00995: loss did not improve from 1.23678\n",
      "Epoch 996/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.2373 - accuracy: 0.4333\n",
      "\n",
      "Epoch 00996: loss did not improve from 1.23678\n",
      "Epoch 997/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.2374 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00997: loss did not improve from 1.23678\n",
      "Epoch 998/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.2378 - accuracy: 0.4375\n",
      "\n",
      "Epoch 00998: loss did not improve from 1.23678\n",
      "Epoch 999/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 710us/step - loss: 1.2372 - accuracy: 0.4292\n",
      "\n",
      "Epoch 00999: loss did not improve from 1.23678\n",
      "Epoch 1000/2000\n",
      "240/240 [==============================] - 0s 672us/step - loss: 1.2371 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01000: loss did not improve from 1.23678\n",
      "Epoch 1001/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.2371 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01001: loss did not improve from 1.23678\n",
      "Epoch 1002/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.2371 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01002: loss did not improve from 1.23678\n",
      "Epoch 1003/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 1.2373 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01003: loss did not improve from 1.23678\n",
      "Epoch 1004/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.2373 - accuracy: 0.4583\n",
      "\n",
      "Epoch 01004: loss did not improve from 1.23678\n",
      "Epoch 1005/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.2376 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01005: loss did not improve from 1.23678\n",
      "Epoch 1006/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.2372 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01006: loss did not improve from 1.23678\n",
      "Epoch 1007/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.2374 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01007: loss did not improve from 1.23678\n",
      "Epoch 1008/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 1.2371 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01008: loss did not improve from 1.23678\n",
      "Epoch 1009/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.2370 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01009: loss did not improve from 1.23678\n",
      "Epoch 1010/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 1.2369 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01010: loss did not improve from 1.23678\n",
      "Epoch 1011/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 1.2374 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01011: loss did not improve from 1.23678\n",
      "Epoch 1012/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 1.2378 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01012: loss did not improve from 1.23678\n",
      "Epoch 1013/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.3355 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01013: loss did not improve from 1.23678\n",
      "Epoch 1014/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 1.2840 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01014: loss did not improve from 1.23678\n",
      "Epoch 1015/2000\n",
      "240/240 [==============================] - 0s 635us/step - loss: 1.2426 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01015: loss did not improve from 1.23678\n",
      "Epoch 1016/2000\n",
      "240/240 [==============================] - 0s 739us/step - loss: 1.2386 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01016: loss did not improve from 1.23678\n",
      "Epoch 1017/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 1.2385 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01017: loss did not improve from 1.23678\n",
      "Epoch 1018/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2376 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01018: loss did not improve from 1.23678\n",
      "Epoch 1019/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2372 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01019: loss did not improve from 1.23678\n",
      "Epoch 1020/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2374 - accuracy: 0.4667\n",
      "\n",
      "Epoch 01020: loss did not improve from 1.23678\n",
      "Epoch 1021/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.2375 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01021: loss did not improve from 1.23678\n",
      "Epoch 1022/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 1.2374 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01022: loss did not improve from 1.23678\n",
      "Epoch 1023/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.2370 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01023: loss did not improve from 1.23678\n",
      "Epoch 1024/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.2378 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01024: loss did not improve from 1.23678\n",
      "Epoch 1025/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.2374 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01025: loss did not improve from 1.23678\n",
      "Epoch 1026/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.2372 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01026: loss did not improve from 1.23678\n",
      "Epoch 1027/2000\n",
      "240/240 [==============================] - 0s 627us/step - loss: 1.2371 - accuracy: 0.4667\n",
      "\n",
      "Epoch 01027: loss did not improve from 1.23678\n",
      "Epoch 1028/2000\n",
      "240/240 [==============================] - 0s 747us/step - loss: 1.2370 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01028: loss did not improve from 1.23678\n",
      "Epoch 1029/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.2370 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01029: loss did not improve from 1.23678\n",
      "Epoch 1030/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.2371 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01030: loss did not improve from 1.23678\n",
      "Epoch 1031/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.2369 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01031: loss did not improve from 1.23678\n",
      "Epoch 1032/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 1.2370 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01032: loss did not improve from 1.23678\n",
      "Epoch 1033/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 1.2368 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01033: loss improved from 1.23678 to 1.23675, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 1034/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.2369 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01034: loss did not improve from 1.23675\n",
      "Epoch 1035/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.4437 - accuracy: 0.4667\n",
      "\n",
      "Epoch 01035: loss did not improve from 1.23675\n",
      "Epoch 1036/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2432 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01036: loss did not improve from 1.23675\n",
      "Epoch 1037/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.2384 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01037: loss did not improve from 1.23675\n",
      "Epoch 1038/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 1.2374 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01038: loss did not improve from 1.23675\n",
      "Epoch 1039/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 1.2380 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01039: loss did not improve from 1.23675\n",
      "Epoch 1040/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.2372 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01040: loss did not improve from 1.23675\n",
      "Epoch 1041/2000\n",
      "240/240 [==============================] - 0s 844us/step - loss: 1.2373 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01041: loss did not improve from 1.23675\n",
      "Epoch 1042/2000\n",
      "240/240 [==============================] - 0s 807us/step - loss: 1.2381 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01042: loss did not improve from 1.23675\n",
      "Epoch 1043/2000\n",
      "240/240 [==============================] - 0s 812us/step - loss: 1.2375 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01043: loss did not improve from 1.23675\n",
      "Epoch 1044/2000\n",
      "240/240 [==============================] - 0s 766us/step - loss: 1.2374 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01044: loss did not improve from 1.23675\n",
      "Epoch 1045/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.2370 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01045: loss did not improve from 1.23675\n",
      "Epoch 1046/2000\n",
      "240/240 [==============================] - 0s 623us/step - loss: 1.2370 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01046: loss did not improve from 1.23675\n",
      "Epoch 1047/2000\n",
      "240/240 [==============================] - 0s 628us/step - loss: 1.2369 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01047: loss did not improve from 1.23675\n",
      "Epoch 1048/2000\n",
      "240/240 [==============================] - 0s 627us/step - loss: 1.2373 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01048: loss did not improve from 1.23675\n",
      "Epoch 1049/2000\n",
      "240/240 [==============================] - 0s 650us/step - loss: 1.2369 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01049: loss did not improve from 1.23675\n",
      "Epoch 1050/2000\n",
      "240/240 [==============================] - 0s 659us/step - loss: 1.2376 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01050: loss did not improve from 1.23675\n",
      "Epoch 1051/2000\n",
      "240/240 [==============================] - 0s 629us/step - loss: 1.2374 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01051: loss did not improve from 1.23675\n",
      "Epoch 1052/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 1.2374 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01052: loss did not improve from 1.23675\n",
      "Epoch 1053/2000\n",
      "240/240 [==============================] - 0s 661us/step - loss: 1.2368 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01053: loss did not improve from 1.23675\n",
      "Epoch 1054/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 1.2369 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01054: loss did not improve from 1.23675\n",
      "Epoch 1055/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.2369 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01055: loss did not improve from 1.23675\n",
      "Epoch 1056/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2368 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01056: loss did not improve from 1.23675\n",
      "Epoch 1057/2000\n",
      "240/240 [==============================] - 0s 613us/step - loss: 1.2370 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01057: loss did not improve from 1.23675\n",
      "Epoch 1058/2000\n",
      "240/240 [==============================] - 0s 619us/step - loss: 1.2368 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01058: loss did not improve from 1.23675\n",
      "Epoch 1059/2000\n",
      "240/240 [==============================] - 0s 624us/step - loss: 1.2368 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01059: loss did not improve from 1.23675\n",
      "Epoch 1060/2000\n",
      "240/240 [==============================] - 0s 620us/step - loss: 1.2367 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01060: loss improved from 1.23675 to 1.23671, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 1061/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 1.2365 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01061: loss improved from 1.23671 to 1.23655, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 1062/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.2367 - accuracy: 0.4625\n",
      "\n",
      "Epoch 01062: loss did not improve from 1.23655\n",
      "Epoch 1063/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 1.2370 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01063: loss did not improve from 1.23655\n",
      "Epoch 1064/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 1.2370 - accuracy: 0.4667\n",
      "\n",
      "Epoch 01064: loss did not improve from 1.23655\n",
      "Epoch 1065/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.2390 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01065: loss did not improve from 1.23655\n",
      "Epoch 1066/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2375 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01066: loss did not improve from 1.23655\n",
      "Epoch 1067/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 1.2371 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01067: loss did not improve from 1.23655\n",
      "Epoch 1068/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.2375 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01068: loss did not improve from 1.23655\n",
      "Epoch 1069/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.2367 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01069: loss did not improve from 1.23655\n",
      "Epoch 1070/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.2372 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01070: loss did not improve from 1.23655\n",
      "Epoch 1071/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 1.2373 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01071: loss did not improve from 1.23655\n",
      "Epoch 1072/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.2653 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01072: loss did not improve from 1.23655\n",
      "Epoch 1073/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2383 - accuracy: 0.4583\n",
      "\n",
      "Epoch 01073: loss did not improve from 1.23655\n",
      "Epoch 1074/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.2378 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01074: loss did not improve from 1.23655\n",
      "Epoch 1075/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2387 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01075: loss did not improve from 1.23655\n",
      "Epoch 1076/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2384 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01076: loss did not improve from 1.23655\n",
      "Epoch 1077/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 1.2386 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01077: loss did not improve from 1.23655\n",
      "Epoch 1078/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 1.2376 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01078: loss did not improve from 1.23655\n",
      "Epoch 1079/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 1.2369 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01079: loss did not improve from 1.23655\n",
      "Epoch 1080/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 1.2365 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01080: loss improved from 1.23655 to 1.23650, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 1081/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.2369 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01081: loss did not improve from 1.23650\n",
      "Epoch 1082/2000\n",
      "240/240 [==============================] - 0s 718us/step - loss: 1.2370 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01082: loss did not improve from 1.23650\n",
      "Epoch 1083/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.2381 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01083: loss did not improve from 1.23650\n",
      "Epoch 1084/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.2400 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01084: loss did not improve from 1.23650\n",
      "Epoch 1085/2000\n",
      "240/240 [==============================] - 0s 728us/step - loss: 1.2375 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01085: loss did not improve from 1.23650\n",
      "Epoch 1086/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.2402 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01086: loss did not improve from 1.23650\n",
      "Epoch 1087/2000\n",
      "240/240 [==============================] - 0s 750us/step - loss: 1.2373 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01087: loss did not improve from 1.23650\n",
      "Epoch 1088/2000\n",
      "240/240 [==============================] - 0s 776us/step - loss: 1.2369 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01088: loss did not improve from 1.23650\n",
      "Epoch 1089/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.2378 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01089: loss did not improve from 1.23650\n",
      "Epoch 1090/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.2369 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01090: loss did not improve from 1.23650\n",
      "Epoch 1091/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2369 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01091: loss did not improve from 1.23650\n",
      "Epoch 1092/2000\n",
      "240/240 [==============================] - 0s 672us/step - loss: 1.2367 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01092: loss did not improve from 1.23650\n",
      "Epoch 1093/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2369 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01093: loss did not improve from 1.23650\n",
      "Epoch 1094/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2374 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01094: loss did not improve from 1.23650\n",
      "Epoch 1095/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.2380 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01095: loss did not improve from 1.23650\n",
      "Epoch 1096/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.2423 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01096: loss did not improve from 1.23650\n",
      "Epoch 1097/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.2381 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01097: loss did not improve from 1.23650\n",
      "Epoch 1098/2000\n",
      "240/240 [==============================] - 0s 755us/step - loss: 1.2370 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01098: loss did not improve from 1.23650\n",
      "Epoch 1099/2000\n",
      "240/240 [==============================] - 0s 756us/step - loss: 1.2386 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01099: loss did not improve from 1.23650\n",
      "Epoch 1100/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.2369 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01100: loss did not improve from 1.23650\n",
      "Epoch 1101/2000\n",
      "240/240 [==============================] - 0s 740us/step - loss: 1.2370 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01101: loss did not improve from 1.23650\n",
      "Epoch 1102/2000\n",
      "240/240 [==============================] - 0s 766us/step - loss: 1.2372 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01102: loss did not improve from 1.23650\n",
      "Epoch 1103/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 697us/step - loss: 1.2366 - accuracy: 0.4583\n",
      "\n",
      "Epoch 01103: loss did not improve from 1.23650\n",
      "Epoch 1104/2000\n",
      "240/240 [==============================] - 0s 631us/step - loss: 1.2365 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01104: loss did not improve from 1.23650\n",
      "Epoch 1105/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.2369 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01105: loss did not improve from 1.23650\n",
      "Epoch 1106/2000\n",
      "240/240 [==============================] - 0s 752us/step - loss: 1.2370 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01106: loss did not improve from 1.23650\n",
      "Epoch 1107/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.2382 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01107: loss did not improve from 1.23650\n",
      "Epoch 1108/2000\n",
      "240/240 [==============================] - 0s 765us/step - loss: 1.2368 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01108: loss did not improve from 1.23650\n",
      "Epoch 1109/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.2368 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01109: loss did not improve from 1.23650\n",
      "Epoch 1110/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.2363 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01110: loss improved from 1.23650 to 1.23634, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 1111/2000\n",
      "240/240 [==============================] - 0s 664us/step - loss: 1.2366 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01111: loss did not improve from 1.23634\n",
      "Epoch 1112/2000\n",
      "240/240 [==============================] - 0s 667us/step - loss: 1.2369 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01112: loss did not improve from 1.23634\n",
      "Epoch 1113/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 1.2382 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01113: loss did not improve from 1.23634\n",
      "Epoch 1114/2000\n",
      "240/240 [==============================] - 0s 641us/step - loss: 1.2398 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01114: loss did not improve from 1.23634\n",
      "Epoch 1115/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 1.2378 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01115: loss did not improve from 1.23634\n",
      "Epoch 1116/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 1.2371 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01116: loss did not improve from 1.23634\n",
      "Epoch 1117/2000\n",
      "240/240 [==============================] - 0s 643us/step - loss: 1.2367 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01117: loss did not improve from 1.23634\n",
      "Epoch 1118/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 1.2365 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01118: loss did not improve from 1.23634\n",
      "Epoch 1119/2000\n",
      "240/240 [==============================] - 0s 658us/step - loss: 1.2369 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01119: loss did not improve from 1.23634\n",
      "Epoch 1120/2000\n",
      "240/240 [==============================] - 0s 651us/step - loss: 1.2374 - accuracy: 0.4708\n",
      "\n",
      "Epoch 01120: loss did not improve from 1.23634\n",
      "Epoch 1121/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 1.2373 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01121: loss did not improve from 1.23634\n",
      "Epoch 1122/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 1.2371 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01122: loss did not improve from 1.23634\n",
      "Epoch 1123/2000\n",
      "240/240 [==============================] - 0s 641us/step - loss: 1.2367 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01123: loss did not improve from 1.23634\n",
      "Epoch 1124/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 1.2569 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01124: loss did not improve from 1.23634\n",
      "Epoch 1125/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 1.2441 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01125: loss did not improve from 1.23634\n",
      "Epoch 1126/2000\n",
      "240/240 [==============================] - 0s 639us/step - loss: 1.2400 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01126: loss did not improve from 1.23634\n",
      "Epoch 1127/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.3310 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01127: loss did not improve from 1.23634\n",
      "Epoch 1128/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.2413 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01128: loss did not improve from 1.23634\n",
      "Epoch 1129/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2385 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01129: loss did not improve from 1.23634\n",
      "Epoch 1130/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 1.2373 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01130: loss did not improve from 1.23634\n",
      "Epoch 1131/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.2372 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01131: loss did not improve from 1.23634\n",
      "Epoch 1132/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.2367 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01132: loss did not improve from 1.23634\n",
      "Epoch 1133/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.2366 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01133: loss did not improve from 1.23634\n",
      "Epoch 1134/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.2371 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01134: loss did not improve from 1.23634\n",
      "Epoch 1135/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 1.2364 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01135: loss did not improve from 1.23634\n",
      "Epoch 1136/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 1.2365 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01136: loss did not improve from 1.23634\n",
      "Epoch 1137/2000\n",
      "240/240 [==============================] - 0s 758us/step - loss: 1.2365 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01137: loss did not improve from 1.23634\n",
      "Epoch 1138/2000\n",
      "240/240 [==============================] - 0s 802us/step - loss: 1.2364 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01138: loss did not improve from 1.23634\n",
      "Epoch 1139/2000\n",
      "240/240 [==============================] - 0s 805us/step - loss: 1.2365 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01139: loss did not improve from 1.23634\n",
      "Epoch 1140/2000\n",
      "240/240 [==============================] - 0s 838us/step - loss: 1.2366 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01140: loss did not improve from 1.23634\n",
      "Epoch 1141/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 1.2365 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01141: loss did not improve from 1.23634\n",
      "Epoch 1142/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2364 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01142: loss did not improve from 1.23634\n",
      "Epoch 1143/2000\n",
      "240/240 [==============================] - 0s 632us/step - loss: 1.2367 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01143: loss did not improve from 1.23634\n",
      "Epoch 1144/2000\n",
      "240/240 [==============================] - 0s 643us/step - loss: 1.2367 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01144: loss did not improve from 1.23634\n",
      "Epoch 1145/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.2365 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01145: loss did not improve from 1.23634\n",
      "Epoch 1146/2000\n",
      "240/240 [==============================] - 0s 658us/step - loss: 1.2366 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01146: loss did not improve from 1.23634\n",
      "Epoch 1147/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.2364 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01147: loss did not improve from 1.23634\n",
      "Epoch 1148/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 1.2364 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01148: loss did not improve from 1.23634\n",
      "Epoch 1149/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.2364 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01149: loss did not improve from 1.23634\n",
      "Epoch 1150/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 1.2371 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01150: loss did not improve from 1.23634\n",
      "Epoch 1151/2000\n",
      "240/240 [==============================] - 0s 646us/step - loss: 1.2369 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01151: loss did not improve from 1.23634\n",
      "Epoch 1152/2000\n",
      "240/240 [==============================] - 0s 662us/step - loss: 1.2366 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01152: loss did not improve from 1.23634\n",
      "Epoch 1153/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.2366 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01153: loss did not improve from 1.23634\n",
      "Epoch 1154/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2365 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01154: loss did not improve from 1.23634\n",
      "Epoch 1155/2000\n",
      "240/240 [==============================] - 0s 649us/step - loss: 1.2364 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01155: loss did not improve from 1.23634\n",
      "Epoch 1156/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.2370 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01156: loss did not improve from 1.23634\n",
      "Epoch 1157/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 1.2411 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01157: loss did not improve from 1.23634\n",
      "Epoch 1158/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 1.2405 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01158: loss did not improve from 1.23634\n",
      "Epoch 1159/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.2395 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01159: loss did not improve from 1.23634\n",
      "Epoch 1160/2000\n",
      "240/240 [==============================] - 0s 616us/step - loss: 1.2383 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01160: loss did not improve from 1.23634\n",
      "Epoch 1161/2000\n",
      "240/240 [==============================] - 0s 639us/step - loss: 1.2371 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01161: loss did not improve from 1.23634\n",
      "Epoch 1162/2000\n",
      "240/240 [==============================] - 0s 662us/step - loss: 1.2367 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01162: loss did not improve from 1.23634\n",
      "Epoch 1163/2000\n",
      "240/240 [==============================] - 0s 647us/step - loss: 1.2363 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01163: loss improved from 1.23634 to 1.23633, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 1164/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.2363 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01164: loss did not improve from 1.23633\n",
      "Epoch 1165/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.2365 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01165: loss did not improve from 1.23633\n",
      "Epoch 1166/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.2365 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01166: loss did not improve from 1.23633\n",
      "Epoch 1167/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 1.2369 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01167: loss did not improve from 1.23633\n",
      "Epoch 1168/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.2364 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01168: loss did not improve from 1.23633\n",
      "Epoch 1169/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.2364 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01169: loss did not improve from 1.23633\n",
      "Epoch 1170/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.2365 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01170: loss did not improve from 1.23633\n",
      "Epoch 1171/2000\n",
      "240/240 [==============================] - 0s 741us/step - loss: 1.2366 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01171: loss did not improve from 1.23633\n",
      "Epoch 1172/2000\n",
      "240/240 [==============================] - ETA: 0s - loss: 1.3146 - accuracy: 0.41 - 0s 748us/step - loss: 1.2366 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01172: loss did not improve from 1.23633\n",
      "Epoch 1173/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.2540 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01173: loss did not improve from 1.23633\n",
      "Epoch 1174/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.2387 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01174: loss did not improve from 1.23633\n",
      "Epoch 1175/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.2377 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01175: loss did not improve from 1.23633\n",
      "Epoch 1176/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.2374 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01176: loss did not improve from 1.23633\n",
      "Epoch 1177/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 1.2378 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01177: loss did not improve from 1.23633\n",
      "Epoch 1178/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.2372 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01178: loss did not improve from 1.23633\n",
      "Epoch 1179/2000\n",
      "240/240 [==============================] - 0s 722us/step - loss: 1.2371 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01179: loss did not improve from 1.23633\n",
      "Epoch 1180/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.2366 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01180: loss did not improve from 1.23633\n",
      "Epoch 1181/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 1.2369 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01181: loss did not improve from 1.23633\n",
      "Epoch 1182/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2374 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01182: loss did not improve from 1.23633\n",
      "Epoch 1183/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.2367 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01183: loss did not improve from 1.23633\n",
      "Epoch 1184/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 1.2366 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01184: loss did not improve from 1.23633\n",
      "Epoch 1185/2000\n",
      "240/240 [==============================] - 0s 737us/step - loss: 1.2364 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01185: loss did not improve from 1.23633\n",
      "Epoch 1186/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.2366 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01186: loss did not improve from 1.23633\n",
      "Epoch 1187/2000\n",
      "240/240 [==============================] - ETA: 0s - loss: 1.2368 - accuracy: 0.44 - 0s 674us/step - loss: 1.2368 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01187: loss did not improve from 1.23633\n",
      "Epoch 1188/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.2514 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01188: loss did not improve from 1.23633\n",
      "Epoch 1189/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.2413 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01189: loss did not improve from 1.23633\n",
      "Epoch 1190/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.2369 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01190: loss did not improve from 1.23633\n",
      "Epoch 1191/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.2369 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01191: loss did not improve from 1.23633\n",
      "Epoch 1192/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2372 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01192: loss did not improve from 1.23633\n",
      "Epoch 1193/2000\n",
      "240/240 [==============================] - 0s 754us/step - loss: 1.2366 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01193: loss did not improve from 1.23633\n",
      "Epoch 1194/2000\n",
      "240/240 [==============================] - 0s 738us/step - loss: 1.2365 - accuracy: 0.4708\n",
      "\n",
      "Epoch 01194: loss did not improve from 1.23633\n",
      "Epoch 1195/2000\n",
      "240/240 [==============================] - 0s 629us/step - loss: 1.2365 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01195: loss did not improve from 1.23633\n",
      "Epoch 1196/2000\n",
      "240/240 [==============================] - 0s 644us/step - loss: 1.2362 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01196: loss improved from 1.23633 to 1.23620, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 1197/2000\n",
      "240/240 [==============================] - 0s 693us/step - loss: 1.2364 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01197: loss did not improve from 1.23620\n",
      "Epoch 1198/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.2363 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01198: loss did not improve from 1.23620\n",
      "Epoch 1199/2000\n",
      "240/240 [==============================] - 0s 757us/step - loss: 1.2363 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01199: loss did not improve from 1.23620\n",
      "Epoch 1200/2000\n",
      "240/240 [==============================] - 0s 875us/step - loss: 1.2366 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01200: loss did not improve from 1.23620\n",
      "Epoch 1201/2000\n",
      "240/240 [==============================] - 0s 641us/step - loss: 1.2367 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01201: loss did not improve from 1.23620\n",
      "Epoch 1202/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2365 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01202: loss did not improve from 1.23620\n",
      "Epoch 1203/2000\n",
      "240/240 [==============================] - 0s 647us/step - loss: 1.2364 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01203: loss did not improve from 1.23620\n",
      "Epoch 1204/2000\n",
      "240/240 [==============================] - 0s 827us/step - loss: 1.2366 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01204: loss did not improve from 1.23620\n",
      "Epoch 1205/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.2364 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01205: loss did not improve from 1.23620\n",
      "Epoch 1206/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.2366 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01206: loss did not improve from 1.23620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1207/2000\n",
      "240/240 [==============================] - 0s 835us/step - loss: 1.2365 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01207: loss did not improve from 1.23620\n",
      "Epoch 1208/2000\n",
      "240/240 [==============================] - 0s 881us/step - loss: 1.2368 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01208: loss did not improve from 1.23620\n",
      "Epoch 1209/2000\n",
      "240/240 [==============================] - 0s 652us/step - loss: 1.2397 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01209: loss did not improve from 1.23620\n",
      "Epoch 1210/2000\n",
      "240/240 [==============================] - 0s 831us/step - loss: 1.2400 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01210: loss did not improve from 1.23620\n",
      "Epoch 1211/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.2381 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01211: loss did not improve from 1.23620\n",
      "Epoch 1212/2000\n",
      "240/240 [==============================] - 0s 632us/step - loss: 1.2371 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01212: loss did not improve from 1.23620\n",
      "Epoch 1213/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 1.2361 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01213: loss improved from 1.23620 to 1.23610, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 1214/2000\n",
      "240/240 [==============================] - 0s 737us/step - loss: 1.2362 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01214: loss did not improve from 1.23610\n",
      "Epoch 1215/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.2362 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01215: loss did not improve from 1.23610\n",
      "Epoch 1216/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 1.2364 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01216: loss did not improve from 1.23610\n",
      "Epoch 1217/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.2363 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01217: loss did not improve from 1.23610\n",
      "Epoch 1218/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.2363 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01218: loss did not improve from 1.23610\n",
      "Epoch 1219/2000\n",
      "240/240 [==============================] - 0s 769us/step - loss: 1.2364 - accuracy: 0.4583\n",
      "\n",
      "Epoch 01219: loss did not improve from 1.23610\n",
      "Epoch 1220/2000\n",
      "240/240 [==============================] - 0s 768us/step - loss: 1.2363 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01220: loss did not improve from 1.23610\n",
      "Epoch 1221/2000\n",
      "240/240 [==============================] - 0s 730us/step - loss: 1.2364 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01221: loss did not improve from 1.23610\n",
      "Epoch 1222/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 1.2362 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01222: loss did not improve from 1.23610\n",
      "Epoch 1223/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.2362 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01223: loss did not improve from 1.23610\n",
      "Epoch 1224/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.2365 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01224: loss did not improve from 1.23610\n",
      "Epoch 1225/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.2386 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01225: loss did not improve from 1.23610\n",
      "Epoch 1226/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.2368 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01226: loss did not improve from 1.23610\n",
      "Epoch 1227/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 1.2365 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01227: loss did not improve from 1.23610\n",
      "Epoch 1228/2000\n",
      "240/240 [==============================] - 0s 721us/step - loss: 1.2366 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01228: loss did not improve from 1.23610\n",
      "Epoch 1229/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.2366 - accuracy: 0.4583\n",
      "\n",
      "Epoch 01229: loss did not improve from 1.23610\n",
      "Epoch 1230/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.2370 - accuracy: 0.4708\n",
      "\n",
      "Epoch 01230: loss did not improve from 1.23610\n",
      "Epoch 1231/2000\n",
      "240/240 [==============================] - 0s 908us/step - loss: 1.5020 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01231: loss did not improve from 1.23610\n",
      "Epoch 1232/2000\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 1.2385 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01232: loss did not improve from 1.23610\n",
      "Epoch 1233/2000\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 1.2379 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01233: loss did not improve from 1.23610\n",
      "Epoch 1234/2000\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 1.2373 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01234: loss did not improve from 1.23610\n",
      "Epoch 1235/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.2367 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01235: loss did not improve from 1.23610\n",
      "Epoch 1236/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.2374 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01236: loss did not improve from 1.23610\n",
      "Epoch 1237/2000\n",
      "240/240 [==============================] - 0s 663us/step - loss: 1.2407 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01237: loss did not improve from 1.23610\n",
      "Epoch 1238/2000\n",
      "240/240 [==============================] - 0s 661us/step - loss: 1.2373 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01238: loss did not improve from 1.23610\n",
      "Epoch 1239/2000\n",
      "240/240 [==============================] - 0s 654us/step - loss: 1.2366 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01239: loss did not improve from 1.23610\n",
      "Epoch 1240/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.2368 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01240: loss did not improve from 1.23610\n",
      "Epoch 1241/2000\n",
      "240/240 [==============================] - 0s 663us/step - loss: 1.2366 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01241: loss did not improve from 1.23610\n",
      "Epoch 1242/2000\n",
      "240/240 [==============================] - 0s 664us/step - loss: 1.2366 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01242: loss did not improve from 1.23610\n",
      "Epoch 1243/2000\n",
      "240/240 [==============================] - 0s 659us/step - loss: 1.2360 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01243: loss improved from 1.23610 to 1.23599, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 1244/2000\n",
      "240/240 [==============================] - 0s 754us/step - loss: 1.2383 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01244: loss did not improve from 1.23599\n",
      "Epoch 1245/2000\n",
      "240/240 [==============================] - 0s 850us/step - loss: 1.2366 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01245: loss did not improve from 1.23599\n",
      "Epoch 1246/2000\n",
      "240/240 [==============================] - 0s 697us/step - loss: 1.2363 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01246: loss did not improve from 1.23599\n",
      "Epoch 1247/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.2373 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01247: loss did not improve from 1.23599\n",
      "Epoch 1248/2000\n",
      "240/240 [==============================] - 0s 652us/step - loss: 1.2371 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01248: loss did not improve from 1.23599\n",
      "Epoch 1249/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 1.2366 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01249: loss did not improve from 1.23599\n",
      "Epoch 1250/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2381 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01250: loss did not improve from 1.23599\n",
      "Epoch 1251/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.2370 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01251: loss did not improve from 1.23599\n",
      "Epoch 1252/2000\n",
      "240/240 [==============================] - 0s 749us/step - loss: 1.2369 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01252: loss did not improve from 1.23599\n",
      "Epoch 1253/2000\n",
      "240/240 [==============================] - 0s 697us/step - loss: 1.3103 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01253: loss did not improve from 1.23599\n",
      "Epoch 1254/2000\n",
      "240/240 [==============================] - ETA: 0s - loss: 1.1852 - accuracy: 0.45 - 0s 680us/step - loss: 1.2383 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01254: loss did not improve from 1.23599\n",
      "Epoch 1255/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.2378 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01255: loss did not improve from 1.23599\n",
      "Epoch 1256/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 1.2374 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01256: loss did not improve from 1.23599\n",
      "Epoch 1257/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.2368 - accuracy: 0.4583\n",
      "\n",
      "Epoch 01257: loss did not improve from 1.23599\n",
      "Epoch 1258/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.2365 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01258: loss did not improve from 1.23599\n",
      "Epoch 1259/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 692us/step - loss: 1.2365 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01259: loss did not improve from 1.23599\n",
      "Epoch 1260/2000\n",
      "240/240 [==============================] - 0s 658us/step - loss: 1.2364 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01260: loss did not improve from 1.23599\n",
      "Epoch 1261/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 1.2362 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01261: loss did not improve from 1.23599\n",
      "Epoch 1262/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.2363 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01262: loss did not improve from 1.23599\n",
      "Epoch 1263/2000\n",
      "240/240 [==============================] - 0s 756us/step - loss: 1.2362 - accuracy: 0.4250TA: 0s - loss: 1.0756 - accuracy: 0.52\n",
      "\n",
      "Epoch 01263: loss did not improve from 1.23599\n",
      "Epoch 1264/2000\n",
      "240/240 [==============================] - 0s 750us/step - loss: 1.2364 - accuracy: 0.4625\n",
      "\n",
      "Epoch 01264: loss did not improve from 1.23599\n",
      "Epoch 1265/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 1.2368 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01265: loss did not improve from 1.23599\n",
      "Epoch 1266/2000\n",
      "240/240 [==============================] - 0s 738us/step - loss: 1.2368 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01266: loss did not improve from 1.23599\n",
      "Epoch 1267/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 1.2361 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01267: loss did not improve from 1.23599\n",
      "Epoch 1268/2000\n",
      "240/240 [==============================] - 0s 781us/step - loss: 1.2363 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01268: loss did not improve from 1.23599\n",
      "Epoch 1269/2000\n",
      "240/240 [==============================] - 0s 759us/step - loss: 1.2363 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01269: loss did not improve from 1.23599\n",
      "Epoch 1270/2000\n",
      "240/240 [==============================] - 0s 751us/step - loss: 1.2423 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01270: loss did not improve from 1.23599\n",
      "Epoch 1271/2000\n",
      "240/240 [==============================] - 0s 742us/step - loss: 1.2657 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01271: loss did not improve from 1.23599\n",
      "Epoch 1272/2000\n",
      "240/240 [==============================] - 0s 769us/step - loss: 1.2606 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01272: loss did not improve from 1.23599\n",
      "Epoch 1273/2000\n",
      "240/240 [==============================] - 0s 753us/step - loss: 1.2402 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01273: loss did not improve from 1.23599\n",
      "Epoch 1274/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 1.2393 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01274: loss did not improve from 1.23599\n",
      "Epoch 1275/2000\n",
      "240/240 [==============================] - 0s 761us/step - loss: 1.3621 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01275: loss did not improve from 1.23599\n",
      "Epoch 1276/2000\n",
      "240/240 [==============================] - 0s 747us/step - loss: 1.2889 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01276: loss did not improve from 1.23599\n",
      "Epoch 1277/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.2373 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01277: loss did not improve from 1.23599\n",
      "Epoch 1278/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.2369 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01278: loss did not improve from 1.23599\n",
      "Epoch 1279/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.2366 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01279: loss did not improve from 1.23599\n",
      "Epoch 1280/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.2364 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01280: loss did not improve from 1.23599\n",
      "Epoch 1281/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 1.2365 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01281: loss did not improve from 1.23599\n",
      "Epoch 1282/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 1.2364 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01282: loss did not improve from 1.23599\n",
      "Epoch 1283/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.2363 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01283: loss did not improve from 1.23599\n",
      "Epoch 1284/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.2362 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01284: loss did not improve from 1.23599\n",
      "Epoch 1285/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.2364 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01285: loss did not improve from 1.23599\n",
      "Epoch 1286/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.2365 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01286: loss did not improve from 1.23599\n",
      "Epoch 1287/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.2364 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01287: loss did not improve from 1.23599\n",
      "Epoch 1288/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.2365 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01288: loss did not improve from 1.23599\n",
      "Epoch 1289/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.2365 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01289: loss did not improve from 1.23599\n",
      "Epoch 1290/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.2364 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01290: loss did not improve from 1.23599\n",
      "Epoch 1291/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.2361 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01291: loss did not improve from 1.23599\n",
      "Epoch 1292/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.2362 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01292: loss did not improve from 1.23599\n",
      "Epoch 1293/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 1.2372 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01293: loss did not improve from 1.23599\n",
      "Epoch 1294/2000\n",
      "240/240 [==============================] - 0s 749us/step - loss: 1.2390 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01294: loss did not improve from 1.23599\n",
      "Epoch 1295/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.2366 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01295: loss did not improve from 1.23599\n",
      "Epoch 1296/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.2361 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01296: loss did not improve from 1.23599\n",
      "Epoch 1297/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.2361 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01297: loss did not improve from 1.23599\n",
      "Epoch 1298/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.2361 - accuracy: 0.4625\n",
      "\n",
      "Epoch 01298: loss did not improve from 1.23599\n",
      "Epoch 1299/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.2362 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01299: loss did not improve from 1.23599\n",
      "Epoch 1300/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.2360 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01300: loss improved from 1.23599 to 1.23598, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 1301/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.2364 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01301: loss did not improve from 1.23598\n",
      "Epoch 1302/2000\n",
      "240/240 [==============================] - 0s 749us/step - loss: 1.2362 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01302: loss did not improve from 1.23598\n",
      "Epoch 1303/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.2363 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01303: loss did not improve from 1.23598\n",
      "Epoch 1304/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.2361 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01304: loss did not improve from 1.23598\n",
      "Epoch 1305/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.2359 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01305: loss improved from 1.23598 to 1.23587, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 1306/2000\n",
      "240/240 [==============================] - 0s 770us/step - loss: 1.2359 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01306: loss did not improve from 1.23587\n",
      "Epoch 1307/2000\n",
      "240/240 [==============================] - 0s 765us/step - loss: 1.2359 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01307: loss improved from 1.23587 to 1.23586, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 1308/2000\n",
      "240/240 [==============================] - 0s 755us/step - loss: 1.2362 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01308: loss did not improve from 1.23586\n",
      "Epoch 1309/2000\n",
      "240/240 [==============================] - 0s 765us/step - loss: 1.2385 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01309: loss did not improve from 1.23586\n",
      "Epoch 1310/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 1.2383 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01310: loss did not improve from 1.23586\n",
      "Epoch 1311/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.2371 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01311: loss did not improve from 1.23586\n",
      "Epoch 1312/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.2361 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01312: loss did not improve from 1.23586\n",
      "Epoch 1313/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.2361 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01313: loss did not improve from 1.23586\n",
      "Epoch 1314/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2361 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01314: loss did not improve from 1.23586\n",
      "Epoch 1315/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.2358 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01315: loss improved from 1.23586 to 1.23584, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 1316/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.2360 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01316: loss did not improve from 1.23584\n",
      "Epoch 1317/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.2368 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01317: loss did not improve from 1.23584\n",
      "Epoch 1318/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.2407 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01318: loss did not improve from 1.23584\n",
      "Epoch 1319/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.2400 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01319: loss did not improve from 1.23584\n",
      "Epoch 1320/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.2380 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01320: loss did not improve from 1.23584\n",
      "Epoch 1321/2000\n",
      "240/240 [==============================] - 0s 726us/step - loss: 1.2365 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01321: loss did not improve from 1.23584\n",
      "Epoch 1322/2000\n",
      "240/240 [==============================] - 0s 842us/step - loss: 1.2365 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01322: loss did not improve from 1.23584\n",
      "Epoch 1323/2000\n",
      "240/240 [==============================] - 0s 809us/step - loss: 1.2363 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01323: loss did not improve from 1.23584\n",
      "Epoch 1324/2000\n",
      "240/240 [==============================] - 0s 830us/step - loss: 1.2362 - accuracy: 0.4708\n",
      "\n",
      "Epoch 01324: loss did not improve from 1.23584\n",
      "Epoch 1325/2000\n",
      "240/240 [==============================] - 0s 751us/step - loss: 1.2360 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01325: loss did not improve from 1.23584\n",
      "Epoch 1326/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 1.2361 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01326: loss did not improve from 1.23584\n",
      "Epoch 1327/2000\n",
      "240/240 [==============================] - 0s 642us/step - loss: 1.2359 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01327: loss did not improve from 1.23584\n",
      "Epoch 1328/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 1.2360 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01328: loss did not improve from 1.23584\n",
      "Epoch 1329/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2360 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01329: loss did not improve from 1.23584\n",
      "Epoch 1330/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.2360 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01330: loss did not improve from 1.23584\n",
      "Epoch 1331/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2360 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01331: loss did not improve from 1.23584\n",
      "Epoch 1332/2000\n",
      "240/240 [==============================] - 0s 629us/step - loss: 1.2361 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01332: loss did not improve from 1.23584\n",
      "Epoch 1333/2000\n",
      "240/240 [==============================] - 0s 627us/step - loss: 1.2366 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01333: loss did not improve from 1.23584\n",
      "Epoch 1334/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2360 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01334: loss did not improve from 1.23584\n",
      "Epoch 1335/2000\n",
      "240/240 [==============================] - 0s 624us/step - loss: 1.2361 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01335: loss did not improve from 1.23584\n",
      "Epoch 1336/2000\n",
      "240/240 [==============================] - 0s 627us/step - loss: 1.2361 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01336: loss did not improve from 1.23584\n",
      "Epoch 1337/2000\n",
      "240/240 [==============================] - 0s 662us/step - loss: 1.2361 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01337: loss did not improve from 1.23584\n",
      "Epoch 1338/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2361 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01338: loss did not improve from 1.23584\n",
      "Epoch 1339/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2360 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01339: loss did not improve from 1.23584\n",
      "Epoch 1340/2000\n",
      "240/240 [==============================] - 0s 638us/step - loss: 1.2370 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01340: loss did not improve from 1.23584\n",
      "Epoch 1341/2000\n",
      "240/240 [==============================] - 0s 645us/step - loss: 1.2370 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01341: loss did not improve from 1.23584\n",
      "Epoch 1342/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.2365 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01342: loss did not improve from 1.23584\n",
      "Epoch 1343/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.2364 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01343: loss did not improve from 1.23584\n",
      "Epoch 1344/2000\n",
      "240/240 [==============================] - 0s 910us/step - loss: 1.2368 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01344: loss did not improve from 1.23584\n",
      "Epoch 1345/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.2373 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01345: loss did not improve from 1.23584\n",
      "Epoch 1346/2000\n",
      "240/240 [==============================] - 0s 667us/step - loss: 1.2390 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01346: loss did not improve from 1.23584\n",
      "Epoch 1347/2000\n",
      "240/240 [==============================] - 0s 852us/step - loss: 1.2476 - accuracy: 0.4667\n",
      "\n",
      "Epoch 01347: loss did not improve from 1.23584\n",
      "Epoch 1348/2000\n",
      "240/240 [==============================] - 0s 655us/step - loss: 1.2369 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01348: loss did not improve from 1.23584\n",
      "Epoch 1349/2000\n",
      "240/240 [==============================] - 0s 632us/step - loss: 1.2367 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01349: loss did not improve from 1.23584\n",
      "Epoch 1350/2000\n",
      "240/240 [==============================] - 0s 873us/step - loss: 1.2377 - accuracy: 0.4583\n",
      "\n",
      "Epoch 01350: loss did not improve from 1.23584\n",
      "Epoch 1351/2000\n",
      "240/240 [==============================] - 0s 766us/step - loss: 1.2369 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01351: loss did not improve from 1.23584\n",
      "Epoch 1352/2000\n",
      "240/240 [==============================] - 0s 819us/step - loss: 1.2362 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01352: loss did not improve from 1.23584\n",
      "Epoch 1353/2000\n",
      "240/240 [==============================] - 0s 786us/step - loss: 1.2362 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01353: loss did not improve from 1.23584\n",
      "Epoch 1354/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.2369 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01354: loss did not improve from 1.23584\n",
      "Epoch 1355/2000\n",
      "240/240 [==============================] - 0s 835us/step - loss: 1.2378 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01355: loss did not improve from 1.23584\n",
      "Epoch 1356/2000\n",
      "240/240 [==============================] - 0s 651us/step - loss: 1.2365 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01356: loss did not improve from 1.23584\n",
      "Epoch 1357/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 1.2363 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01357: loss did not improve from 1.23584\n",
      "Epoch 1358/2000\n",
      "240/240 [==============================] - 0s 641us/step - loss: 1.2371 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01358: loss did not improve from 1.23584\n",
      "Epoch 1359/2000\n",
      "240/240 [==============================] - 0s 627us/step - loss: 1.2363 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01359: loss did not improve from 1.23584\n",
      "Epoch 1360/2000\n",
      "240/240 [==============================] - 0s 749us/step - loss: 1.2363 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01360: loss did not improve from 1.23584\n",
      "Epoch 1361/2000\n",
      "240/240 [==============================] - 0s 756us/step - loss: 1.2362 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01361: loss did not improve from 1.23584\n",
      "Epoch 1362/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 727us/step - loss: 1.2361 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01362: loss did not improve from 1.23584\n",
      "Epoch 1363/2000\n",
      "240/240 [==============================] - 0s 777us/step - loss: 1.2360 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01363: loss did not improve from 1.23584\n",
      "Epoch 1364/2000\n",
      "240/240 [==============================] - 0s 750us/step - loss: 1.2362 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01364: loss did not improve from 1.23584\n",
      "Epoch 1365/2000\n",
      "240/240 [==============================] - 0s 733us/step - loss: 1.2362 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01365: loss did not improve from 1.23584\n",
      "Epoch 1366/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 1.2360 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01366: loss did not improve from 1.23584\n",
      "Epoch 1367/2000\n",
      "240/240 [==============================] - 0s 810us/step - loss: 1.2360 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01367: loss did not improve from 1.23584\n",
      "Epoch 1368/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.2374 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01368: loss did not improve from 1.23584\n",
      "Epoch 1369/2000\n",
      "240/240 [==============================] - 0s 618us/step - loss: 1.2361 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01369: loss did not improve from 1.23584\n",
      "Epoch 1370/2000\n",
      "240/240 [==============================] - 0s 628us/step - loss: 1.2383 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01370: loss did not improve from 1.23584\n",
      "Epoch 1371/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.2506 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01371: loss did not improve from 1.23584\n",
      "Epoch 1372/2000\n",
      "240/240 [==============================] - 0s 777us/step - loss: 1.2404 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01372: loss did not improve from 1.23584\n",
      "Epoch 1373/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.2388 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01373: loss did not improve from 1.23584\n",
      "Epoch 1374/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 1.2392 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01374: loss did not improve from 1.23584\n",
      "Epoch 1375/2000\n",
      "240/240 [==============================] - 0s 623us/step - loss: 1.2369 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01375: loss did not improve from 1.23584\n",
      "Epoch 1376/2000\n",
      "240/240 [==============================] - 0s 671us/step - loss: 1.2365 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01376: loss did not improve from 1.23584\n",
      "Epoch 1377/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.2364 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01377: loss did not improve from 1.23584\n",
      "Epoch 1378/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.2367 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01378: loss did not improve from 1.23584\n",
      "Epoch 1379/2000\n",
      "240/240 [==============================] - 0s 902us/step - loss: 1.2364 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01379: loss did not improve from 1.23584\n",
      "Epoch 1380/2000\n",
      "240/240 [==============================] - 0s 745us/step - loss: 1.2362 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01380: loss did not improve from 1.23584\n",
      "Epoch 1381/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.2364 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01381: loss did not improve from 1.23584\n",
      "Epoch 1382/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 1.2366 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01382: loss did not improve from 1.23584\n",
      "Epoch 1383/2000\n",
      "240/240 [==============================] - 0s 748us/step - loss: 1.2363 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01383: loss did not improve from 1.23584\n",
      "Epoch 1384/2000\n",
      "240/240 [==============================] - 0s 885us/step - loss: 1.2361 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01384: loss did not improve from 1.23584\n",
      "Epoch 1385/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.2362 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01385: loss did not improve from 1.23584\n",
      "Epoch 1386/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.2373 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01386: loss did not improve from 1.23584\n",
      "Epoch 1387/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.2375 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01387: loss did not improve from 1.23584\n",
      "Epoch 1388/2000\n",
      "240/240 [==============================] - 0s 877us/step - loss: 1.2381 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01388: loss did not improve from 1.23584\n",
      "Epoch 1389/2000\n",
      "240/240 [==============================] - 0s 776us/step - loss: 1.2382 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01389: loss did not improve from 1.23584\n",
      "Epoch 1390/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.2367 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01390: loss did not improve from 1.23584\n",
      "Epoch 1391/2000\n",
      "240/240 [==============================] - 0s 882us/step - loss: 1.2363 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01391: loss did not improve from 1.23584\n",
      "Epoch 1392/2000\n",
      "240/240 [==============================] - 0s 836us/step - loss: 1.2426 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01392: loss did not improve from 1.23584\n",
      "Epoch 1393/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.2374 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01393: loss did not improve from 1.23584\n",
      "Epoch 1394/2000\n",
      "240/240 [==============================] - 0s 935us/step - loss: 1.2368 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01394: loss did not improve from 1.23584\n",
      "Epoch 1395/2000\n",
      "240/240 [==============================] - 0s 781us/step - loss: 1.2369 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01395: loss did not improve from 1.23584\n",
      "Epoch 1396/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.2577 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01396: loss did not improve from 1.23584\n",
      "Epoch 1397/2000\n",
      "240/240 [==============================] - 0s 922us/step - loss: 1.2375 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01397: loss did not improve from 1.23584\n",
      "Epoch 1398/2000\n",
      "240/240 [==============================] - 0s 761us/step - loss: 1.2366 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01398: loss did not improve from 1.23584\n",
      "Epoch 1399/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.2360 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01399: loss did not improve from 1.23584\n",
      "Epoch 1400/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.2362 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01400: loss did not improve from 1.23584\n",
      "Epoch 1401/2000\n",
      "240/240 [==============================] - 0s 628us/step - loss: 1.2360 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01401: loss did not improve from 1.23584\n",
      "Epoch 1402/2000\n",
      "240/240 [==============================] - 0s 738us/step - loss: 1.2359 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01402: loss did not improve from 1.23584\n",
      "Epoch 1403/2000\n",
      "240/240 [==============================] - 0s 893us/step - loss: 1.2359 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01403: loss did not improve from 1.23584\n",
      "Epoch 1404/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.2358 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01404: loss improved from 1.23584 to 1.23575, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 1405/2000\n",
      "240/240 [==============================] - 0s 856us/step - loss: 1.2360 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01405: loss did not improve from 1.23575\n",
      "Epoch 1406/2000\n",
      "240/240 [==============================] - 0s 721us/step - loss: 1.2362 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01406: loss did not improve from 1.23575\n",
      "Epoch 1407/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.2372 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01407: loss did not improve from 1.23575\n",
      "Epoch 1408/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 1.2395 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01408: loss did not improve from 1.23575\n",
      "Epoch 1409/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.2385 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01409: loss did not improve from 1.23575\n",
      "Epoch 1410/2000\n",
      "240/240 [==============================] - 0s 745us/step - loss: 1.2367 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01410: loss did not improve from 1.23575\n",
      "Epoch 1411/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.2365 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01411: loss did not improve from 1.23575\n",
      "Epoch 1412/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.2363 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01412: loss did not improve from 1.23575\n",
      "Epoch 1413/2000\n",
      "240/240 [==============================] - 0s 760us/step - loss: 1.2362 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01413: loss did not improve from 1.23575\n",
      "Epoch 1414/2000\n",
      "240/240 [==============================] - 0s 1ms/step - loss: 1.2363 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01414: loss did not improve from 1.23575\n",
      "Epoch 1415/2000\n",
      "240/240 [==============================] - 0s 832us/step - loss: 1.2362 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01415: loss did not improve from 1.23575\n",
      "Epoch 1416/2000\n",
      "240/240 [==============================] - 0s 836us/step - loss: 1.2363 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01416: loss did not improve from 1.23575\n",
      "Epoch 1417/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.2367 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01417: loss did not improve from 1.23575\n",
      "Epoch 1418/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.2386 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01418: loss did not improve from 1.23575\n",
      "Epoch 1419/2000\n",
      "240/240 [==============================] - 0s 756us/step - loss: 1.2389 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01419: loss did not improve from 1.23575\n",
      "Epoch 1420/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 1.2371 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01420: loss did not improve from 1.23575\n",
      "Epoch 1421/2000\n",
      "240/240 [==============================] - 0s 644us/step - loss: 1.2634 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01421: loss did not improve from 1.23575\n",
      "Epoch 1422/2000\n",
      "240/240 [==============================] - 0s 806us/step - loss: 1.4259 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01422: loss did not improve from 1.23575\n",
      "Epoch 1423/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 1.2805 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01423: loss did not improve from 1.23575\n",
      "Epoch 1424/2000\n",
      "240/240 [==============================] - 0s 846us/step - loss: 1.2401 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01424: loss did not improve from 1.23575\n",
      "Epoch 1425/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 1.2379 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01425: loss did not improve from 1.23575\n",
      "Epoch 1426/2000\n",
      "240/240 [==============================] - 0s 852us/step - loss: 1.2373 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01426: loss did not improve from 1.23575\n",
      "Epoch 1427/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.2367 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01427: loss did not improve from 1.23575\n",
      "Epoch 1428/2000\n",
      "240/240 [==============================] - 0s 623us/step - loss: 1.2367 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01428: loss did not improve from 1.23575\n",
      "Epoch 1429/2000\n",
      "240/240 [==============================] - 0s 643us/step - loss: 1.2363 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01429: loss did not improve from 1.23575\n",
      "Epoch 1430/2000\n",
      "240/240 [==============================] - 0s 661us/step - loss: 1.2364 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01430: loss did not improve from 1.23575\n",
      "Epoch 1431/2000\n",
      "240/240 [==============================] - 0s 628us/step - loss: 1.2362 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01431: loss did not improve from 1.23575\n",
      "Epoch 1432/2000\n",
      "240/240 [==============================] - 0s 765us/step - loss: 1.2360 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01432: loss did not improve from 1.23575\n",
      "Epoch 1433/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.2363 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01433: loss did not improve from 1.23575\n",
      "Epoch 1434/2000\n",
      "240/240 [==============================] - 0s 644us/step - loss: 1.2361 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01434: loss did not improve from 1.23575\n",
      "Epoch 1435/2000\n",
      "240/240 [==============================] - 0s 652us/step - loss: 1.2362 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01435: loss did not improve from 1.23575\n",
      "Epoch 1436/2000\n",
      "240/240 [==============================] - 0s 906us/step - loss: 1.2361 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01436: loss did not improve from 1.23575\n",
      "Epoch 1437/2000\n",
      "240/240 [==============================] - 0s 731us/step - loss: 1.2360 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01437: loss did not improve from 1.23575\n",
      "Epoch 1438/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.2361 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01438: loss did not improve from 1.23575\n",
      "Epoch 1439/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.2361 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01439: loss did not improve from 1.23575\n",
      "Epoch 1440/2000\n",
      "240/240 [==============================] - 0s 661us/step - loss: 1.2361 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01440: loss did not improve from 1.23575\n",
      "Epoch 1441/2000\n",
      "240/240 [==============================] - 0s 615us/step - loss: 1.2361 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01441: loss did not improve from 1.23575\n",
      "Epoch 1442/2000\n",
      "240/240 [==============================] - 0s 625us/step - loss: 1.2361 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01442: loss did not improve from 1.23575\n",
      "Epoch 1443/2000\n",
      "240/240 [==============================] - 0s 667us/step - loss: 1.2360 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01443: loss did not improve from 1.23575\n",
      "Epoch 1444/2000\n",
      "240/240 [==============================] - 0s 629us/step - loss: 1.2359 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01444: loss did not improve from 1.23575\n",
      "Epoch 1445/2000\n",
      "240/240 [==============================] - 0s 637us/step - loss: 1.2360 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01445: loss did not improve from 1.23575\n",
      "Epoch 1446/2000\n",
      "240/240 [==============================] - 0s 629us/step - loss: 1.2367 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01446: loss did not improve from 1.23575\n",
      "Epoch 1447/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 1.2377 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01447: loss did not improve from 1.23575\n",
      "Epoch 1448/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.2372 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01448: loss did not improve from 1.23575\n",
      "Epoch 1449/2000\n",
      "240/240 [==============================] - 0s 655us/step - loss: 1.2365 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01449: loss did not improve from 1.23575\n",
      "Epoch 1450/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2363 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01450: loss did not improve from 1.23575\n",
      "Epoch 1451/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.2368 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01451: loss did not improve from 1.23575\n",
      "Epoch 1452/2000\n",
      "240/240 [==============================] - 0s 781us/step - loss: 1.2358 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01452: loss did not improve from 1.23575\n",
      "Epoch 1453/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 1.2362 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01453: loss did not improve from 1.23575\n",
      "Epoch 1454/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.2357 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01454: loss improved from 1.23575 to 1.23570, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 1455/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.2358 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01455: loss did not improve from 1.23570\n",
      "Epoch 1456/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.2361 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01456: loss did not improve from 1.23570\n",
      "Epoch 1457/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 1.2358 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01457: loss did not improve from 1.23570\n",
      "Epoch 1458/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 1.2358 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01458: loss did not improve from 1.23570\n",
      "Epoch 1459/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.2371 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01459: loss did not improve from 1.23570\n",
      "Epoch 1460/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2364 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01460: loss did not improve from 1.23570\n",
      "Epoch 1461/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2362 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01461: loss did not improve from 1.23570\n",
      "Epoch 1462/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.2361 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01462: loss did not improve from 1.23570\n",
      "Epoch 1463/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2359 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01463: loss did not improve from 1.23570\n",
      "Epoch 1464/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2357 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01464: loss did not improve from 1.23570\n",
      "Epoch 1465/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.2370 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01465: loss did not improve from 1.23570\n",
      "Epoch 1466/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 677us/step - loss: 1.2384 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01466: loss did not improve from 1.23570\n",
      "Epoch 1467/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2364 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01467: loss did not improve from 1.23570\n",
      "Epoch 1468/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.2363 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01468: loss did not improve from 1.23570\n",
      "Epoch 1469/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.2365 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01469: loss did not improve from 1.23570\n",
      "Epoch 1470/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2359 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01470: loss did not improve from 1.23570\n",
      "Epoch 1471/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.2358 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01471: loss did not improve from 1.23570\n",
      "Epoch 1472/2000\n",
      "240/240 [==============================] - 0s 672us/step - loss: 1.2360 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01472: loss did not improve from 1.23570\n",
      "Epoch 1473/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 1.2360 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01473: loss did not improve from 1.23570\n",
      "Epoch 1474/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 1.2359 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01474: loss did not improve from 1.23570\n",
      "Epoch 1475/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.2361 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01475: loss did not improve from 1.23570\n",
      "Epoch 1476/2000\n",
      "240/240 [==============================] - 0s 717us/step - loss: 1.2362 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01476: loss did not improve from 1.23570\n",
      "Epoch 1477/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.2361 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01477: loss did not improve from 1.23570\n",
      "Epoch 1478/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.2388 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01478: loss did not improve from 1.23570\n",
      "Epoch 1479/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.2386 - accuracy: 0.4583\n",
      "\n",
      "Epoch 01479: loss did not improve from 1.23570\n",
      "Epoch 1480/2000\n",
      "240/240 [==============================] - 0s 666us/step - loss: 1.2383 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01480: loss did not improve from 1.23570\n",
      "Epoch 1481/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.2386 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01481: loss did not improve from 1.23570\n",
      "Epoch 1482/2000\n",
      "240/240 [==============================] - 0s 727us/step - loss: 1.2367 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01482: loss did not improve from 1.23570\n",
      "Epoch 1483/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.2361 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01483: loss did not improve from 1.23570\n",
      "Epoch 1484/2000\n",
      "240/240 [==============================] - 0s 752us/step - loss: 1.2362 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01484: loss did not improve from 1.23570\n",
      "Epoch 1485/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.2357 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01485: loss did not improve from 1.23570\n",
      "Epoch 1486/2000\n",
      "240/240 [==============================] - 0s 773us/step - loss: 1.2358 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01486: loss did not improve from 1.23570\n",
      "Epoch 1487/2000\n",
      "240/240 [==============================] - 0s 814us/step - loss: 1.2355 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01487: loss improved from 1.23570 to 1.23554, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 1488/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.2358 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01488: loss did not improve from 1.23554\n",
      "Epoch 1489/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.2358 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01489: loss did not improve from 1.23554\n",
      "Epoch 1490/2000\n",
      "240/240 [==============================] - 0s 823us/step - loss: 1.2362 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01490: loss did not improve from 1.23554\n",
      "Epoch 1491/2000\n",
      "240/240 [==============================] - 0s 615us/step - loss: 1.2361 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01491: loss did not improve from 1.23554\n",
      "Epoch 1492/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 1.2362 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01492: loss did not improve from 1.23554\n",
      "Epoch 1493/2000\n",
      "240/240 [==============================] - 0s 629us/step - loss: 1.2358 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01493: loss did not improve from 1.23554\n",
      "Epoch 1494/2000\n",
      "240/240 [==============================] - 0s 637us/step - loss: 1.2358 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01494: loss did not improve from 1.23554\n",
      "Epoch 1495/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.2359 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01495: loss did not improve from 1.23554\n",
      "Epoch 1496/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 1.2358 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01496: loss did not improve from 1.23554\n",
      "Epoch 1497/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.2357 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01497: loss did not improve from 1.23554\n",
      "Epoch 1498/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 1.2359 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01498: loss did not improve from 1.23554\n",
      "Epoch 1499/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 1.2357 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01499: loss did not improve from 1.23554\n",
      "Epoch 1500/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.2365 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01500: loss did not improve from 1.23554\n",
      "Epoch 1501/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 1.3829 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01501: loss did not improve from 1.23554\n",
      "Epoch 1502/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.2522 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01502: loss did not improve from 1.23554\n",
      "Epoch 1503/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.2432 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01503: loss did not improve from 1.23554\n",
      "Epoch 1504/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 1.2387 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01504: loss did not improve from 1.23554\n",
      "Epoch 1505/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 1.2386 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01505: loss did not improve from 1.23554\n",
      "Epoch 1506/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.2373 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01506: loss did not improve from 1.23554\n",
      "Epoch 1507/2000\n",
      "240/240 [==============================] - 0s 633us/step - loss: 1.2369 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01507: loss did not improve from 1.23554\n",
      "Epoch 1508/2000\n",
      "240/240 [==============================] - 0s 641us/step - loss: 1.2365 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01508: loss did not improve from 1.23554\n",
      "Epoch 1509/2000\n",
      "240/240 [==============================] - 0s 773us/step - loss: 1.2361 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01509: loss did not improve from 1.23554\n",
      "Epoch 1510/2000\n",
      "240/240 [==============================] - 0s 815us/step - loss: 1.2360 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01510: loss did not improve from 1.23554\n",
      "Epoch 1511/2000\n",
      "240/240 [==============================] - 0s 830us/step - loss: 1.2358 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01511: loss did not improve from 1.23554\n",
      "Epoch 1512/2000\n",
      "240/240 [==============================] - 0s 772us/step - loss: 1.2359 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01512: loss did not improve from 1.23554\n",
      "Epoch 1513/2000\n",
      "240/240 [==============================] - 0s 660us/step - loss: 1.2357 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01513: loss did not improve from 1.23554\n",
      "Epoch 1514/2000\n",
      "240/240 [==============================] - 0s 629us/step - loss: 1.2358 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01514: loss did not improve from 1.23554\n",
      "Epoch 1515/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 1.2358 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01515: loss did not improve from 1.23554\n",
      "Epoch 1516/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.2356 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01516: loss did not improve from 1.23554\n",
      "Epoch 1517/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.2359 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01517: loss did not improve from 1.23554\n",
      "Epoch 1518/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 1.2361 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01518: loss did not improve from 1.23554\n",
      "Epoch 1519/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 1.2360 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01519: loss did not improve from 1.23554\n",
      "Epoch 1520/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.2359 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01520: loss did not improve from 1.23554\n",
      "Epoch 1521/2000\n",
      "240/240 [==============================] - 0s 638us/step - loss: 1.2357 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01521: loss did not improve from 1.23554\n",
      "Epoch 1522/2000\n",
      "240/240 [==============================] - 0s 659us/step - loss: 1.2385 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01522: loss did not improve from 1.23554\n",
      "Epoch 1523/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.2394 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01523: loss did not improve from 1.23554\n",
      "Epoch 1524/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.2374 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01524: loss did not improve from 1.23554\n",
      "Epoch 1525/2000\n",
      "240/240 [==============================] - 0s 647us/step - loss: 1.2372 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01525: loss did not improve from 1.23554\n",
      "Epoch 1526/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.2371 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01526: loss did not improve from 1.23554\n",
      "Epoch 1527/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 1.2368 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01527: loss did not improve from 1.23554\n",
      "Epoch 1528/2000\n",
      "240/240 [==============================] - 0s 672us/step - loss: 1.2367 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01528: loss did not improve from 1.23554\n",
      "Epoch 1529/2000\n",
      "240/240 [==============================] - 0s 624us/step - loss: 1.2368 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01529: loss did not improve from 1.23554\n",
      "Epoch 1530/2000\n",
      "240/240 [==============================] - 0s 647us/step - loss: 1.2366 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01530: loss did not improve from 1.23554\n",
      "Epoch 1531/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.2363 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01531: loss did not improve from 1.23554\n",
      "Epoch 1532/2000\n",
      "240/240 [==============================] - 0s 714us/step - loss: 1.2366 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01532: loss did not improve from 1.23554\n",
      "Epoch 1533/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.2380 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01533: loss did not improve from 1.23554\n",
      "Epoch 1534/2000\n",
      "240/240 [==============================] - 0s 686us/step - loss: 1.2369 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01534: loss did not improve from 1.23554\n",
      "Epoch 1535/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 1.2361 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01535: loss did not improve from 1.23554\n",
      "Epoch 1536/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.2360 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01536: loss did not improve from 1.23554\n",
      "Epoch 1537/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.2363 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01537: loss did not improve from 1.23554\n",
      "Epoch 1538/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.2363 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01538: loss did not improve from 1.23554\n",
      "Epoch 1539/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.2359 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01539: loss did not improve from 1.23554\n",
      "Epoch 1540/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 1.2361 - accuracy: 0.4583\n",
      "\n",
      "Epoch 01540: loss did not improve from 1.23554\n",
      "Epoch 1541/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.2359 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01541: loss did not improve from 1.23554\n",
      "Epoch 1542/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 1.2374 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01542: loss did not improve from 1.23554\n",
      "Epoch 1543/2000\n",
      "240/240 [==============================] - 0s 756us/step - loss: 1.2363 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01543: loss did not improve from 1.23554\n",
      "Epoch 1544/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.2364 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01544: loss did not improve from 1.23554\n",
      "Epoch 1545/2000\n",
      "240/240 [==============================] - 0s 687us/step - loss: 1.2363 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01545: loss did not improve from 1.23554\n",
      "Epoch 1546/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2360 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01546: loss did not improve from 1.23554\n",
      "Epoch 1547/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.2362 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01547: loss did not improve from 1.23554\n",
      "Epoch 1548/2000\n",
      "240/240 [==============================] - 0s 672us/step - loss: 1.2363 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01548: loss did not improve from 1.23554\n",
      "Epoch 1549/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.2364 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01549: loss did not improve from 1.23554\n",
      "Epoch 1550/2000\n",
      "240/240 [==============================] - 0s 709us/step - loss: 1.2362 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01550: loss did not improve from 1.23554\n",
      "Epoch 1551/2000\n",
      "240/240 [==============================] - 0s 935us/step - loss: 1.2359 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01551: loss did not improve from 1.23554\n",
      "Epoch 1552/2000\n",
      "240/240 [==============================] - 0s 910us/step - loss: 1.2360 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01552: loss did not improve from 1.23554\n",
      "Epoch 1553/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.2361 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01553: loss did not improve from 1.23554\n",
      "Epoch 1554/2000\n",
      "240/240 [==============================] - 0s 749us/step - loss: 1.2361 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01554: loss did not improve from 1.23554\n",
      "Epoch 1555/2000\n",
      "240/240 [==============================] - 0s 790us/step - loss: 1.2360 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01555: loss did not improve from 1.23554\n",
      "Epoch 1556/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2363 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01556: loss did not improve from 1.23554\n",
      "Epoch 1557/2000\n",
      "240/240 [==============================] - 0s 814us/step - loss: 1.2637 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01557: loss did not improve from 1.23554\n",
      "Epoch 1558/2000\n",
      "240/240 [==============================] - 0s 644us/step - loss: 1.2387 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01558: loss did not improve from 1.23554\n",
      "Epoch 1559/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 1.2376 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01559: loss did not improve from 1.23554\n",
      "Epoch 1560/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 1.2365 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01560: loss did not improve from 1.23554\n",
      "Epoch 1561/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.2366 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01561: loss did not improve from 1.23554\n",
      "Epoch 1562/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.2365 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01562: loss did not improve from 1.23554\n",
      "Epoch 1563/2000\n",
      "240/240 [==============================] - 0s 644us/step - loss: 1.3427 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01563: loss did not improve from 1.23554\n",
      "Epoch 1564/2000\n",
      "240/240 [==============================] - 0s 646us/step - loss: 1.3396 - accuracy: 0.4625\n",
      "\n",
      "Epoch 01564: loss did not improve from 1.23554\n",
      "Epoch 1565/2000\n",
      "240/240 [==============================] - 0s 632us/step - loss: 1.2382 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01565: loss did not improve from 1.23554\n",
      "Epoch 1566/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 1.2367 - accuracy: 0.4625\n",
      "\n",
      "Epoch 01566: loss did not improve from 1.23554\n",
      "Epoch 1567/2000\n",
      "240/240 [==============================] - 0s 638us/step - loss: 1.2366 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01567: loss did not improve from 1.23554\n",
      "Epoch 1568/2000\n",
      "240/240 [==============================] - 0s 643us/step - loss: 1.2365 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01568: loss did not improve from 1.23554\n",
      "Epoch 1569/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2361 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01569: loss did not improve from 1.23554\n",
      "Epoch 1570/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2396 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01570: loss did not improve from 1.23554\n",
      "Epoch 1571/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 630us/step - loss: 1.2369 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01571: loss did not improve from 1.23554\n",
      "Epoch 1572/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.2364 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01572: loss did not improve from 1.23554\n",
      "Epoch 1573/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 1.2363 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01573: loss did not improve from 1.23554\n",
      "Epoch 1574/2000\n",
      "240/240 [==============================] - 0s 655us/step - loss: 1.2360 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01574: loss did not improve from 1.23554\n",
      "Epoch 1575/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.2362 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01575: loss did not improve from 1.23554\n",
      "Epoch 1576/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2362 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01576: loss did not improve from 1.23554\n",
      "Epoch 1577/2000\n",
      "240/240 [==============================] - 0s 650us/step - loss: 1.2361 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01577: loss did not improve from 1.23554\n",
      "Epoch 1578/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2365 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01578: loss did not improve from 1.23554\n",
      "Epoch 1579/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 1.2367 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01579: loss did not improve from 1.23554\n",
      "Epoch 1580/2000\n",
      "240/240 [==============================] - 0s 647us/step - loss: 1.2378 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01580: loss did not improve from 1.23554\n",
      "Epoch 1581/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2381 - accuracy: 0.4667\n",
      "\n",
      "Epoch 01581: loss did not improve from 1.23554\n",
      "Epoch 1582/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.2374 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01582: loss did not improve from 1.23554\n",
      "Epoch 1583/2000\n",
      "240/240 [==============================] - 0s 650us/step - loss: 1.2366 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01583: loss did not improve from 1.23554\n",
      "Epoch 1584/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 1.2362 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01584: loss did not improve from 1.23554\n",
      "Epoch 1585/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.2360 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01585: loss did not improve from 1.23554\n",
      "Epoch 1586/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.2359 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01586: loss did not improve from 1.23554\n",
      "Epoch 1587/2000\n",
      "240/240 [==============================] - 0s 629us/step - loss: 1.2369 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01587: loss did not improve from 1.23554\n",
      "Epoch 1588/2000\n",
      "240/240 [==============================] - 0s 627us/step - loss: 1.2366 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01588: loss did not improve from 1.23554\n",
      "Epoch 1589/2000\n",
      "240/240 [==============================] - 0s 645us/step - loss: 1.2359 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01589: loss did not improve from 1.23554\n",
      "Epoch 1590/2000\n",
      "240/240 [==============================] - 0s 658us/step - loss: 1.2363 - accuracy: 0.4625\n",
      "\n",
      "Epoch 01590: loss did not improve from 1.23554\n",
      "Epoch 1591/2000\n",
      "240/240 [==============================] - 0s 639us/step - loss: 1.2360 - accuracy: 0.4583\n",
      "\n",
      "Epoch 01591: loss did not improve from 1.23554\n",
      "Epoch 1592/2000\n",
      "240/240 [==============================] - 0s 653us/step - loss: 1.2359 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01592: loss did not improve from 1.23554\n",
      "Epoch 1593/2000\n",
      "240/240 [==============================] - 0s 627us/step - loss: 1.2363 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01593: loss did not improve from 1.23554\n",
      "Epoch 1594/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.2358 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01594: loss did not improve from 1.23554\n",
      "Epoch 1595/2000\n",
      "240/240 [==============================] - 0s 645us/step - loss: 1.2357 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01595: loss did not improve from 1.23554\n",
      "Epoch 1596/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.2365 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01596: loss did not improve from 1.23554\n",
      "Epoch 1597/2000\n",
      "240/240 [==============================] - 0s 666us/step - loss: 1.2380 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01597: loss did not improve from 1.23554\n",
      "Epoch 1598/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.2363 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01598: loss did not improve from 1.23554\n",
      "Epoch 1599/2000\n",
      "240/240 [==============================] - 0s 655us/step - loss: 1.2357 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01599: loss did not improve from 1.23554\n",
      "Epoch 1600/2000\n",
      "240/240 [==============================] - 0s 662us/step - loss: 1.2357 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01600: loss did not improve from 1.23554\n",
      "Epoch 1601/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 1.2364 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01601: loss did not improve from 1.23554\n",
      "Epoch 1602/2000\n",
      "240/240 [==============================] - 0s 635us/step - loss: 1.2360 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01602: loss did not improve from 1.23554\n",
      "Epoch 1603/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2359 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01603: loss did not improve from 1.23554\n",
      "Epoch 1604/2000\n",
      "240/240 [==============================] - 0s 662us/step - loss: 1.2366 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01604: loss did not improve from 1.23554\n",
      "Epoch 1605/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.2359 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01605: loss did not improve from 1.23554\n",
      "Epoch 1606/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.2671 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01606: loss did not improve from 1.23554\n",
      "Epoch 1607/2000\n",
      "240/240 [==============================] - 0s 666us/step - loss: 1.2382 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01607: loss did not improve from 1.23554\n",
      "Epoch 1608/2000\n",
      "240/240 [==============================] - 0s 839us/step - loss: 1.2369 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01608: loss did not improve from 1.23554\n",
      "Epoch 1609/2000\n",
      "240/240 [==============================] - 0s 835us/step - loss: 1.2372 - accuracy: 0.4583\n",
      "\n",
      "Epoch 01609: loss did not improve from 1.23554\n",
      "Epoch 1610/2000\n",
      "240/240 [==============================] - 0s 809us/step - loss: 1.2360 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01610: loss did not improve from 1.23554\n",
      "Epoch 1611/2000\n",
      "240/240 [==============================] - 0s 705us/step - loss: 1.2366 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01611: loss did not improve from 1.23554\n",
      "Epoch 1612/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 1.2362 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01612: loss did not improve from 1.23554\n",
      "Epoch 1613/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.2360 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01613: loss did not improve from 1.23554\n",
      "Epoch 1614/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2359 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01614: loss did not improve from 1.23554\n",
      "Epoch 1615/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2358 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01615: loss did not improve from 1.23554\n",
      "Epoch 1616/2000\n",
      "240/240 [==============================] - 0s 661us/step - loss: 1.2357 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01616: loss did not improve from 1.23554\n",
      "Epoch 1617/2000\n",
      "240/240 [==============================] - 0s 663us/step - loss: 1.2357 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01617: loss did not improve from 1.23554\n",
      "Epoch 1618/2000\n",
      "240/240 [==============================] - 0s 678us/step - loss: 1.2360 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01618: loss did not improve from 1.23554\n",
      "Epoch 1619/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 1.2357 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01619: loss did not improve from 1.23554\n",
      "Epoch 1620/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.2357 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01620: loss did not improve from 1.23554\n",
      "Epoch 1621/2000\n",
      "240/240 [==============================] - 0s 650us/step - loss: 1.2357 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01621: loss did not improve from 1.23554\n",
      "Epoch 1622/2000\n",
      "240/240 [==============================] - 0s 646us/step - loss: 1.2355 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01622: loss improved from 1.23554 to 1.23552, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 1623/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.2361 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01623: loss did not improve from 1.23552\n",
      "Epoch 1624/2000\n",
      "240/240 [==============================] - 0s 647us/step - loss: 1.2537 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01624: loss did not improve from 1.23552\n",
      "Epoch 1625/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.2382 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01625: loss did not improve from 1.23552\n",
      "Epoch 1626/2000\n",
      "240/240 [==============================] - 0s 648us/step - loss: 1.2374 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01626: loss did not improve from 1.23552\n",
      "Epoch 1627/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.2365 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01627: loss did not improve from 1.23552\n",
      "Epoch 1628/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 1.2362 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01628: loss did not improve from 1.23552\n",
      "Epoch 1629/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 1.2361 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01629: loss did not improve from 1.23552\n",
      "Epoch 1630/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.2358 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01630: loss did not improve from 1.23552\n",
      "Epoch 1631/2000\n",
      "240/240 [==============================] - 0s 744us/step - loss: 1.2358 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01631: loss did not improve from 1.23552\n",
      "Epoch 1632/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.2357 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01632: loss did not improve from 1.23552\n",
      "Epoch 1633/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.2358 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01633: loss did not improve from 1.23552\n",
      "Epoch 1634/2000\n",
      "240/240 [==============================] - 0s 703us/step - loss: 1.2357 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01634: loss did not improve from 1.23552\n",
      "Epoch 1635/2000\n",
      "240/240 [==============================] - 0s 723us/step - loss: 1.2361 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01635: loss did not improve from 1.23552\n",
      "Epoch 1636/2000\n",
      "240/240 [==============================] - 0s 720us/step - loss: 1.2368 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01636: loss did not improve from 1.23552\n",
      "Epoch 1637/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.2377 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01637: loss did not improve from 1.23552\n",
      "Epoch 1638/2000\n",
      "240/240 [==============================] - 0s 707us/step - loss: 1.2367 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01638: loss did not improve from 1.23552\n",
      "Epoch 1639/2000\n",
      "240/240 [==============================] - 0s 716us/step - loss: 1.2364 - accuracy: 0.4583\n",
      "\n",
      "Epoch 01639: loss did not improve from 1.23552\n",
      "Epoch 1640/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.2357 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01640: loss did not improve from 1.23552\n",
      "Epoch 1641/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.2356 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01641: loss did not improve from 1.23552\n",
      "Epoch 1642/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.2358 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01642: loss did not improve from 1.23552\n",
      "Epoch 1643/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.2358 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01643: loss did not improve from 1.23552\n",
      "Epoch 1644/2000\n",
      "240/240 [==============================] - 0s 725us/step - loss: 1.2359 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01644: loss did not improve from 1.23552\n",
      "Epoch 1645/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.2364 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01645: loss did not improve from 1.23552\n",
      "Epoch 1646/2000\n",
      "240/240 [==============================] - 0s 706us/step - loss: 1.2359 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01646: loss did not improve from 1.23552\n",
      "Epoch 1647/2000\n",
      "240/240 [==============================] - 0s 873us/step - loss: 1.2364 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01647: loss did not improve from 1.23552\n",
      "Epoch 1648/2000\n",
      "240/240 [==============================] - 0s 794us/step - loss: 1.2380 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01648: loss did not improve from 1.23552\n",
      "Epoch 1649/2000\n",
      "240/240 [==============================] - 0s 689us/step - loss: 1.2376 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01649: loss did not improve from 1.23552\n",
      "Epoch 1650/2000\n",
      "240/240 [==============================] - 0s 696us/step - loss: 1.2370 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01650: loss did not improve from 1.23552\n",
      "Epoch 1651/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.2365 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01651: loss did not improve from 1.23552\n",
      "Epoch 1652/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 1.2359 - accuracy: 0.4583\n",
      "\n",
      "Epoch 01652: loss did not improve from 1.23552\n",
      "Epoch 1653/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.2358 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01653: loss did not improve from 1.23552\n",
      "Epoch 1654/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 1.2359 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01654: loss did not improve from 1.23552\n",
      "Epoch 1655/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.2356 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01655: loss did not improve from 1.23552\n",
      "Epoch 1656/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.2355 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01656: loss improved from 1.23552 to 1.23549, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 1657/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.2354 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01657: loss improved from 1.23549 to 1.23543, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 1658/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.2357 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01658: loss did not improve from 1.23543\n",
      "Epoch 1659/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.2356 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01659: loss did not improve from 1.23543\n",
      "Epoch 1660/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2358 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01660: loss did not improve from 1.23543\n",
      "Epoch 1661/2000\n",
      "240/240 [==============================] - 0s 741us/step - loss: 1.2365 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01661: loss did not improve from 1.23543\n",
      "Epoch 1662/2000\n",
      "240/240 [==============================] - 0s 684us/step - loss: 1.2360 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01662: loss did not improve from 1.23543\n",
      "Epoch 1663/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.2357 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01663: loss did not improve from 1.23543\n",
      "Epoch 1664/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.2361 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01664: loss did not improve from 1.23543\n",
      "Epoch 1665/2000\n",
      "240/240 [==============================] - 0s 675us/step - loss: 1.2369 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01665: loss did not improve from 1.23543\n",
      "Epoch 1666/2000\n",
      "240/240 [==============================] - 0s 672us/step - loss: 1.2373 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01666: loss did not improve from 1.23543\n",
      "Epoch 1667/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.2362 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01667: loss did not improve from 1.23543\n",
      "Epoch 1668/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.2362 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01668: loss did not improve from 1.23543\n",
      "Epoch 1669/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.2364 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01669: loss did not improve from 1.23543\n",
      "Epoch 1670/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.2359 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01670: loss did not improve from 1.23543\n",
      "Epoch 1671/2000\n",
      "240/240 [==============================] - 0s 681us/step - loss: 1.2358 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01671: loss did not improve from 1.23543\n",
      "Epoch 1672/2000\n",
      "240/240 [==============================] - 0s 637us/step - loss: 1.2361 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01672: loss did not improve from 1.23543\n",
      "Epoch 1673/2000\n",
      "240/240 [==============================] - 0s 755us/step - loss: 1.2489 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01673: loss did not improve from 1.23543\n",
      "Epoch 1674/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.2421 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01674: loss did not improve from 1.23543\n",
      "Epoch 1675/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 678us/step - loss: 1.2385 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01675: loss did not improve from 1.23543\n",
      "Epoch 1676/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2369 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01676: loss did not improve from 1.23543\n",
      "Epoch 1677/2000\n",
      "240/240 [==============================] - 0s 679us/step - loss: 1.2364 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01677: loss did not improve from 1.23543\n",
      "Epoch 1678/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 1.2364 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01678: loss did not improve from 1.23543\n",
      "Epoch 1679/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 1.2363 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01679: loss did not improve from 1.23543\n",
      "Epoch 1680/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.2360 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01680: loss did not improve from 1.23543\n",
      "Epoch 1681/2000\n",
      "240/240 [==============================] - 0s 767us/step - loss: 1.2364 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01681: loss did not improve from 1.23543\n",
      "Epoch 1682/2000\n",
      "240/240 [==============================] - 0s 710us/step - loss: 1.2359 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01682: loss did not improve from 1.23543\n",
      "Epoch 1683/2000\n",
      "240/240 [==============================] - 0s 744us/step - loss: 1.2360 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01683: loss did not improve from 1.23543\n",
      "Epoch 1684/2000\n",
      "240/240 [==============================] - 0s 964us/step - loss: 1.2369 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01684: loss did not improve from 1.23543\n",
      "Epoch 1685/2000\n",
      "240/240 [==============================] - 0s 765us/step - loss: 1.2366 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01685: loss did not improve from 1.23543\n",
      "Epoch 1686/2000\n",
      "240/240 [==============================] - 0s 700us/step - loss: 1.2358 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01686: loss did not improve from 1.23543\n",
      "Epoch 1687/2000\n",
      "240/240 [==============================] - 0s 711us/step - loss: 1.2357 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01687: loss did not improve from 1.23543\n",
      "Epoch 1688/2000\n",
      "240/240 [==============================] - 0s 701us/step - loss: 1.2358 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01688: loss did not improve from 1.23543\n",
      "Epoch 1689/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.2368 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01689: loss did not improve from 1.23543\n",
      "Epoch 1690/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.2375 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01690: loss did not improve from 1.23543\n",
      "Epoch 1691/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.2364 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01691: loss did not improve from 1.23543\n",
      "Epoch 1692/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.2359 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01692: loss did not improve from 1.23543\n",
      "Epoch 1693/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.2360 - accuracy: 0.4583\n",
      "\n",
      "Epoch 01693: loss did not improve from 1.23543\n",
      "Epoch 1694/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.2360 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01694: loss did not improve from 1.23543\n",
      "Epoch 1695/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.2362 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01695: loss did not improve from 1.23543\n",
      "Epoch 1696/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.2363 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01696: loss did not improve from 1.23543\n",
      "Epoch 1697/2000\n",
      "240/240 [==============================] - 0s 735us/step - loss: 1.2369 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01697: loss did not improve from 1.23543\n",
      "Epoch 1698/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.2360 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01698: loss did not improve from 1.23543\n",
      "Epoch 1699/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.2362 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01699: loss did not improve from 1.23543\n",
      "Epoch 1700/2000\n",
      "240/240 [==============================] - 0s 712us/step - loss: 1.2359 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01700: loss did not improve from 1.23543\n",
      "Epoch 1701/2000\n",
      "240/240 [==============================] - 0s 724us/step - loss: 1.2358 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01701: loss did not improve from 1.23543\n",
      "Epoch 1702/2000\n",
      "240/240 [==============================] - 0s 820us/step - loss: 1.2360 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01702: loss did not improve from 1.23543\n",
      "Epoch 1703/2000\n",
      "240/240 [==============================] - 0s 826us/step - loss: 1.2355 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01703: loss did not improve from 1.23543\n",
      "Epoch 1704/2000\n",
      "240/240 [==============================] - 0s 815us/step - loss: 1.2353 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01704: loss improved from 1.23543 to 1.23527, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 1705/2000\n",
      "240/240 [==============================] - 0s 708us/step - loss: 1.2811 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01705: loss did not improve from 1.23527\n",
      "Epoch 1706/2000\n",
      "240/240 [==============================] - 0s 635us/step - loss: 1.2388 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01706: loss did not improve from 1.23527\n",
      "Epoch 1707/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.2374 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01707: loss did not improve from 1.23527\n",
      "Epoch 1708/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2371 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01708: loss did not improve from 1.23527\n",
      "Epoch 1709/2000\n",
      "240/240 [==============================] - 0s 670us/step - loss: 1.2368 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01709: loss did not improve from 1.23527\n",
      "Epoch 1710/2000\n",
      "240/240 [==============================] - 0s 627us/step - loss: 1.2360 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01710: loss did not improve from 1.23527\n",
      "Epoch 1711/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 1.2363 - accuracy: 0.4583\n",
      "\n",
      "Epoch 01711: loss did not improve from 1.23527\n",
      "Epoch 1712/2000\n",
      "240/240 [==============================] - 0s 627us/step - loss: 1.2360 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01712: loss did not improve from 1.23527\n",
      "Epoch 1713/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.2360 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01713: loss did not improve from 1.23527\n",
      "Epoch 1714/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 1.2359 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01714: loss did not improve from 1.23527\n",
      "Epoch 1715/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.2361 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01715: loss did not improve from 1.23527\n",
      "Epoch 1716/2000\n",
      "240/240 [==============================] - 0s 633us/step - loss: 1.2358 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01716: loss did not improve from 1.23527\n",
      "Epoch 1717/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.2359 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01717: loss did not improve from 1.23527\n",
      "Epoch 1718/2000\n",
      "240/240 [==============================] - 0s 674us/step - loss: 1.2358 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01718: loss did not improve from 1.23527\n",
      "Epoch 1719/2000\n",
      "240/240 [==============================] - 0s 661us/step - loss: 1.2360 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01719: loss did not improve from 1.23527\n",
      "Epoch 1720/2000\n",
      "240/240 [==============================] - 0s 661us/step - loss: 1.2360 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01720: loss did not improve from 1.23527\n",
      "Epoch 1721/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 1.2374 - accuracy: 0.4583\n",
      "\n",
      "Epoch 01721: loss did not improve from 1.23527\n",
      "Epoch 1722/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 1.2371 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01722: loss did not improve from 1.23527\n",
      "Epoch 1723/2000\n",
      "240/240 [==============================] - 0s 625us/step - loss: 1.2365 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01723: loss did not improve from 1.23527\n",
      "Epoch 1724/2000\n",
      "240/240 [==============================] - 0s 638us/step - loss: 1.2358 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01724: loss did not improve from 1.23527\n",
      "Epoch 1725/2000\n",
      "240/240 [==============================] - 0s 627us/step - loss: 1.2356 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01725: loss did not improve from 1.23527\n",
      "Epoch 1726/2000\n",
      "240/240 [==============================] - 0s 624us/step - loss: 1.2357 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01726: loss did not improve from 1.23527\n",
      "Epoch 1727/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.2356 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01727: loss did not improve from 1.23527\n",
      "Epoch 1728/2000\n",
      "240/240 [==============================] - 0s 653us/step - loss: 1.2356 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01728: loss did not improve from 1.23527\n",
      "Epoch 1729/2000\n",
      "240/240 [==============================] - 0s 661us/step - loss: 1.2357 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01729: loss did not improve from 1.23527\n",
      "Epoch 1730/2000\n",
      "240/240 [==============================] - 0s 627us/step - loss: 1.2354 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01730: loss did not improve from 1.23527\n",
      "Epoch 1731/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2356 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01731: loss did not improve from 1.23527\n",
      "Epoch 1732/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 1.2355 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01732: loss did not improve from 1.23527\n",
      "Epoch 1733/2000\n",
      "240/240 [==============================] - 0s 650us/step - loss: 1.2355 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01733: loss did not improve from 1.23527\n",
      "Epoch 1734/2000\n",
      "240/240 [==============================] - 0s 656us/step - loss: 1.2354 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01734: loss did not improve from 1.23527\n",
      "Epoch 1735/2000\n",
      "240/240 [==============================] - 0s 633us/step - loss: 1.2355 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01735: loss did not improve from 1.23527\n",
      "Epoch 1736/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2355 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01736: loss did not improve from 1.23527\n",
      "Epoch 1737/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2359 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01737: loss did not improve from 1.23527\n",
      "Epoch 1738/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2363 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01738: loss did not improve from 1.23527\n",
      "Epoch 1739/2000\n",
      "240/240 [==============================] - 0s 651us/step - loss: 1.2367 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01739: loss did not improve from 1.23527\n",
      "Epoch 1740/2000\n",
      "240/240 [==============================] - 0s 658us/step - loss: 1.2579 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01740: loss did not improve from 1.23527\n",
      "Epoch 1741/2000\n",
      "240/240 [==============================] - 0s 628us/step - loss: 1.2367 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01741: loss did not improve from 1.23527\n",
      "Epoch 1742/2000\n",
      "240/240 [==============================] - 0s 651us/step - loss: 1.2364 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01742: loss did not improve from 1.23527\n",
      "Epoch 1743/2000\n",
      "240/240 [==============================] - 0s 651us/step - loss: 1.2360 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01743: loss did not improve from 1.23527\n",
      "Epoch 1744/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.2360 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01744: loss did not improve from 1.23527\n",
      "Epoch 1745/2000\n",
      "240/240 [==============================] - 0s 637us/step - loss: 1.2364 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01745: loss did not improve from 1.23527\n",
      "Epoch 1746/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 1.2374 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01746: loss did not improve from 1.23527\n",
      "Epoch 1747/2000\n",
      "240/240 [==============================] - 0s 656us/step - loss: 1.2360 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01747: loss did not improve from 1.23527\n",
      "Epoch 1748/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 1.2359 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01748: loss did not improve from 1.23527\n",
      "Epoch 1749/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 1.2358 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01749: loss did not improve from 1.23527\n",
      "Epoch 1750/2000\n",
      "240/240 [==============================] - 0s 746us/step - loss: 1.2356 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01750: loss did not improve from 1.23527\n",
      "Epoch 1751/2000\n",
      "240/240 [==============================] - 0s 719us/step - loss: 1.2356 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01751: loss did not improve from 1.23527\n",
      "Epoch 1752/2000\n",
      "240/240 [==============================] - 0s 640us/step - loss: 1.2355 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01752: loss did not improve from 1.23527\n",
      "Epoch 1753/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.2356 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01753: loss did not improve from 1.23527\n",
      "Epoch 1754/2000\n",
      "240/240 [==============================] - 0s 642us/step - loss: 1.2355 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01754: loss did not improve from 1.23527\n",
      "Epoch 1755/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.2359 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01755: loss did not improve from 1.23527\n",
      "Epoch 1756/2000\n",
      "240/240 [==============================] - 0s 775us/step - loss: 1.2655 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01756: loss did not improve from 1.23527\n",
      "Epoch 1757/2000\n",
      "240/240 [==============================] - 0s 715us/step - loss: 1.2368 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01757: loss did not improve from 1.23527\n",
      "Epoch 1758/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 1.2364 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01758: loss did not improve from 1.23527\n",
      "Epoch 1759/2000\n",
      "240/240 [==============================] - 0s 641us/step - loss: 1.2364 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01759: loss did not improve from 1.23527\n",
      "Epoch 1760/2000\n",
      "240/240 [==============================] - 0s 641us/step - loss: 1.2385 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01760: loss did not improve from 1.23527\n",
      "Epoch 1761/2000\n",
      "240/240 [==============================] - 0s 672us/step - loss: 1.2366 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01761: loss did not improve from 1.23527\n",
      "Epoch 1762/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.2359 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01762: loss did not improve from 1.23527\n",
      "Epoch 1763/2000\n",
      "240/240 [==============================] - 0s 658us/step - loss: 1.2360 - accuracy: 0.4583\n",
      "\n",
      "Epoch 01763: loss did not improve from 1.23527\n",
      "Epoch 1764/2000\n",
      "240/240 [==============================] - 0s 609us/step - loss: 1.2359 - accuracy: 0.4625\n",
      "\n",
      "Epoch 01764: loss did not improve from 1.23527\n",
      "Epoch 1765/2000\n",
      "240/240 [==============================] - 0s 713us/step - loss: 1.2356 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01765: loss did not improve from 1.23527\n",
      "Epoch 1766/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.2359 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01766: loss did not improve from 1.23527\n",
      "Epoch 1767/2000\n",
      "240/240 [==============================] - 0s 685us/step - loss: 1.2357 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01767: loss did not improve from 1.23527\n",
      "Epoch 1768/2000\n",
      "240/240 [==============================] - 0s 628us/step - loss: 1.2361 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01768: loss did not improve from 1.23527\n",
      "Epoch 1769/2000\n",
      "240/240 [==============================] - 0s 663us/step - loss: 1.2359 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01769: loss did not improve from 1.23527\n",
      "Epoch 1770/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.2357 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01770: loss did not improve from 1.23527\n",
      "Epoch 1771/2000\n",
      "240/240 [==============================] - 0s 662us/step - loss: 1.2356 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01771: loss did not improve from 1.23527\n",
      "Epoch 1772/2000\n",
      "240/240 [==============================] - 0s 631us/step - loss: 1.2361 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01772: loss did not improve from 1.23527\n",
      "Epoch 1773/2000\n",
      "240/240 [==============================] - 0s 653us/step - loss: 1.2356 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01773: loss did not improve from 1.23527\n",
      "Epoch 1774/2000\n",
      "240/240 [==============================] - 0s 647us/step - loss: 1.2357 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01774: loss did not improve from 1.23527\n",
      "Epoch 1775/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2355 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01775: loss did not improve from 1.23527\n",
      "Epoch 1776/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2356 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01776: loss did not improve from 1.23527\n",
      "Epoch 1777/2000\n",
      "240/240 [==============================] - 0s 632us/step - loss: 1.2353 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01777: loss did not improve from 1.23527\n",
      "Epoch 1778/2000\n",
      "240/240 [==============================] - 0s 664us/step - loss: 1.2365 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01778: loss did not improve from 1.23527\n",
      "Epoch 1779/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.2371 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01779: loss did not improve from 1.23527\n",
      "Epoch 1780/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 634us/step - loss: 1.2371 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01780: loss did not improve from 1.23527\n",
      "Epoch 1781/2000\n",
      "240/240 [==============================] - 0s 648us/step - loss: 1.2374 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01781: loss did not improve from 1.23527\n",
      "Epoch 1782/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.2370 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01782: loss did not improve from 1.23527\n",
      "Epoch 1783/2000\n",
      "240/240 [==============================] - 0s 655us/step - loss: 1.2710 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01783: loss did not improve from 1.23527\n",
      "Epoch 1784/2000\n",
      "240/240 [==============================] - 0s 695us/step - loss: 1.2367 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01784: loss did not improve from 1.23527\n",
      "Epoch 1785/2000\n",
      "240/240 [==============================] - 0s 704us/step - loss: 1.2361 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01785: loss did not improve from 1.23527\n",
      "Epoch 1786/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.2365 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01786: loss did not improve from 1.23527\n",
      "Epoch 1787/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.2362 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01787: loss did not improve from 1.23527\n",
      "Epoch 1788/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.2459 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01788: loss did not improve from 1.23527\n",
      "Epoch 1789/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2364 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01789: loss did not improve from 1.23527\n",
      "Epoch 1790/2000\n",
      "240/240 [==============================] - 0s 643us/step - loss: 1.2360 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01790: loss did not improve from 1.23527\n",
      "Epoch 1791/2000\n",
      "240/240 [==============================] - 0s 653us/step - loss: 1.2356 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01791: loss did not improve from 1.23527\n",
      "Epoch 1792/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2356 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01792: loss did not improve from 1.23527\n",
      "Epoch 1793/2000\n",
      "240/240 [==============================] - 0s 664us/step - loss: 1.2354 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01793: loss did not improve from 1.23527\n",
      "Epoch 1794/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 1.2357 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01794: loss did not improve from 1.23527\n",
      "Epoch 1795/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2356 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01795: loss did not improve from 1.23527\n",
      "Epoch 1796/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2370 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01796: loss did not improve from 1.23527\n",
      "Epoch 1797/2000\n",
      "240/240 [==============================] - 0s 666us/step - loss: 1.2360 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01797: loss did not improve from 1.23527\n",
      "Epoch 1798/2000\n",
      "240/240 [==============================] - 0s 667us/step - loss: 1.2367 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01798: loss did not improve from 1.23527\n",
      "Epoch 1799/2000\n",
      "240/240 [==============================] - 0s 638us/step - loss: 1.2356 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01799: loss did not improve from 1.23527\n",
      "Epoch 1800/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.3072 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01800: loss did not improve from 1.23527\n",
      "Epoch 1801/2000\n",
      "240/240 [==============================] - 0s 642us/step - loss: 1.2368 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01801: loss did not improve from 1.23527\n",
      "Epoch 1802/2000\n",
      "240/240 [==============================] - 0s 702us/step - loss: 1.2363 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01802: loss did not improve from 1.23527\n",
      "Epoch 1803/2000\n",
      "240/240 [==============================] - 0s 798us/step - loss: 1.2358 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01803: loss did not improve from 1.23527\n",
      "Epoch 1804/2000\n",
      "240/240 [==============================] - 0s 823us/step - loss: 1.2354 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01804: loss did not improve from 1.23527\n",
      "Epoch 1805/2000\n",
      "240/240 [==============================] - 0s 809us/step - loss: 1.2357 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01805: loss did not improve from 1.23527\n",
      "Epoch 1806/2000\n",
      "240/240 [==============================] - 0s 769us/step - loss: 1.2369 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01806: loss did not improve from 1.23527\n",
      "Epoch 1807/2000\n",
      "240/240 [==============================] - 0s 650us/step - loss: 1.2361 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01807: loss did not improve from 1.23527\n",
      "Epoch 1808/2000\n",
      "240/240 [==============================] - 0s 647us/step - loss: 1.2355 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01808: loss did not improve from 1.23527\n",
      "Epoch 1809/2000\n",
      "240/240 [==============================] - 0s 629us/step - loss: 1.2355 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01809: loss did not improve from 1.23527\n",
      "Epoch 1810/2000\n",
      "240/240 [==============================] - 0s 643us/step - loss: 1.2357 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01810: loss did not improve from 1.23527\n",
      "Epoch 1811/2000\n",
      "240/240 [==============================] - 0s 625us/step - loss: 1.2357 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01811: loss did not improve from 1.23527\n",
      "Epoch 1812/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 1.2355 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01812: loss did not improve from 1.23527\n",
      "Epoch 1813/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.2355 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01813: loss did not improve from 1.23527\n",
      "Epoch 1814/2000\n",
      "240/240 [==============================] - 0s 640us/step - loss: 1.2356 - accuracy: 0.4583\n",
      "\n",
      "Epoch 01814: loss did not improve from 1.23527\n",
      "Epoch 1815/2000\n",
      "240/240 [==============================] - 0s 633us/step - loss: 1.2353 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01815: loss did not improve from 1.23527\n",
      "Epoch 1816/2000\n",
      "240/240 [==============================] - 0s 635us/step - loss: 1.2363 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01816: loss did not improve from 1.23527\n",
      "Epoch 1817/2000\n",
      "240/240 [==============================] - 0s 651us/step - loss: 1.2354 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01817: loss did not improve from 1.23527\n",
      "Epoch 1818/2000\n",
      "240/240 [==============================] - 0s 624us/step - loss: 1.2356 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01818: loss did not improve from 1.23527\n",
      "Epoch 1819/2000\n",
      "240/240 [==============================] - 0s 624us/step - loss: 1.2382 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01819: loss did not improve from 1.23527\n",
      "Epoch 1820/2000\n",
      "240/240 [==============================] - 0s 619us/step - loss: 1.2367 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01820: loss did not improve from 1.23527\n",
      "Epoch 1821/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.2676 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01821: loss did not improve from 1.23527\n",
      "Epoch 1822/2000\n",
      "240/240 [==============================] - 0s 633us/step - loss: 1.2369 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01822: loss did not improve from 1.23527\n",
      "Epoch 1823/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2362 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01823: loss did not improve from 1.23527\n",
      "Epoch 1824/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.2355 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01824: loss did not improve from 1.23527\n",
      "Epoch 1825/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2354 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01825: loss did not improve from 1.23527\n",
      "Epoch 1826/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2356 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01826: loss did not improve from 1.23527\n",
      "Epoch 1827/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2368 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01827: loss did not improve from 1.23527\n",
      "Epoch 1828/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2359 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01828: loss did not improve from 1.23527\n",
      "Epoch 1829/2000\n",
      "240/240 [==============================] - 0s 646us/step - loss: 1.2358 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01829: loss did not improve from 1.23527\n",
      "Epoch 1830/2000\n",
      "240/240 [==============================] - 0s 647us/step - loss: 1.2355 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01830: loss did not improve from 1.23527\n",
      "Epoch 1831/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 1.2354 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01831: loss did not improve from 1.23527\n",
      "Epoch 1832/2000\n",
      "240/240 [==============================] - 0s 644us/step - loss: 1.2355 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01832: loss did not improve from 1.23527\n",
      "Epoch 1833/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 645us/step - loss: 1.2354 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01833: loss did not improve from 1.23527\n",
      "Epoch 1834/2000\n",
      "240/240 [==============================] - 0s 639us/step - loss: 1.2355 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01834: loss did not improve from 1.23527\n",
      "Epoch 1835/2000\n",
      "240/240 [==============================] - 0s 691us/step - loss: 1.2356 - accuracy: 0.4583\n",
      "\n",
      "Epoch 01835: loss did not improve from 1.23527\n",
      "Epoch 1836/2000\n",
      "240/240 [==============================] - 0s 644us/step - loss: 1.2354 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01836: loss did not improve from 1.23527\n",
      "Epoch 1837/2000\n",
      "240/240 [==============================] - 0s 663us/step - loss: 1.2355 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01837: loss did not improve from 1.23527\n",
      "Epoch 1838/2000\n",
      "240/240 [==============================] - 0s 629us/step - loss: 1.2357 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01838: loss did not improve from 1.23527\n",
      "Epoch 1839/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 1.2399 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01839: loss did not improve from 1.23527\n",
      "Epoch 1840/2000\n",
      "240/240 [==============================] - 0s 623us/step - loss: 1.2482 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01840: loss did not improve from 1.23527\n",
      "Epoch 1841/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.2381 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01841: loss did not improve from 1.23527\n",
      "Epoch 1842/2000\n",
      "240/240 [==============================] - 0s 633us/step - loss: 1.2364 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01842: loss did not improve from 1.23527\n",
      "Epoch 1843/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.2358 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01843: loss did not improve from 1.23527\n",
      "Epoch 1844/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.2357 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01844: loss did not improve from 1.23527\n",
      "Epoch 1845/2000\n",
      "240/240 [==============================] - 0s 642us/step - loss: 1.2359 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01845: loss did not improve from 1.23527\n",
      "Epoch 1846/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.2357 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01846: loss did not improve from 1.23527\n",
      "Epoch 1847/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2359 - accuracy: 0.4625\n",
      "\n",
      "Epoch 01847: loss did not improve from 1.23527\n",
      "Epoch 1848/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.2357 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01848: loss did not improve from 1.23527\n",
      "Epoch 1849/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.2357 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01849: loss did not improve from 1.23527\n",
      "Epoch 1850/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.2356 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01850: loss did not improve from 1.23527\n",
      "Epoch 1851/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.2358 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01851: loss did not improve from 1.23527\n",
      "Epoch 1852/2000\n",
      "240/240 [==============================] - 0s 645us/step - loss: 1.2359 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01852: loss did not improve from 1.23527\n",
      "Epoch 1853/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.2386 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01853: loss did not improve from 1.23527\n",
      "Epoch 1854/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2356 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01854: loss did not improve from 1.23527\n",
      "Epoch 1855/2000\n",
      "240/240 [==============================] - 0s 641us/step - loss: 1.2354 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01855: loss did not improve from 1.23527\n",
      "Epoch 1856/2000\n",
      "240/240 [==============================] - 0s 639us/step - loss: 1.2355 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01856: loss did not improve from 1.23527\n",
      "Epoch 1857/2000\n",
      "240/240 [==============================] - 0s 699us/step - loss: 1.2368 - accuracy: 0.4708\n",
      "\n",
      "Epoch 01857: loss did not improve from 1.23527\n",
      "Epoch 1858/2000\n",
      "240/240 [==============================] - 0s 648us/step - loss: 1.2391 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01858: loss did not improve from 1.23527\n",
      "Epoch 1859/2000\n",
      "240/240 [==============================] - 0s 640us/step - loss: 1.2368 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01859: loss did not improve from 1.23527\n",
      "Epoch 1860/2000\n",
      "240/240 [==============================] - 0s 639us/step - loss: 1.2357 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01860: loss did not improve from 1.23527\n",
      "Epoch 1861/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.2353 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01861: loss did not improve from 1.23527\n",
      "Epoch 1862/2000\n",
      "240/240 [==============================] - 0s 646us/step - loss: 1.2354 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01862: loss did not improve from 1.23527\n",
      "Epoch 1863/2000\n",
      "240/240 [==============================] - 0s 639us/step - loss: 1.2356 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01863: loss did not improve from 1.23527\n",
      "Epoch 1864/2000\n",
      "240/240 [==============================] - 0s 683us/step - loss: 1.2354 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01864: loss did not improve from 1.23527\n",
      "Epoch 1865/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.2526 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01865: loss did not improve from 1.23527\n",
      "Epoch 1866/2000\n",
      "240/240 [==============================] - 0s 647us/step - loss: 1.2358 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01866: loss did not improve from 1.23527\n",
      "Epoch 1867/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 1.2355 - accuracy: 0.4625\n",
      "\n",
      "Epoch 01867: loss did not improve from 1.23527\n",
      "Epoch 1868/2000\n",
      "240/240 [==============================] - 0s 661us/step - loss: 1.2356 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01868: loss did not improve from 1.23527\n",
      "Epoch 1869/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.2383 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01869: loss did not improve from 1.23527\n",
      "Epoch 1870/2000\n",
      "240/240 [==============================] - 0s 660us/step - loss: 1.2360 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01870: loss did not improve from 1.23527\n",
      "Epoch 1871/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 1.2357 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01871: loss did not improve from 1.23527\n",
      "Epoch 1872/2000\n",
      "240/240 [==============================] - 0s 631us/step - loss: 1.2373 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01872: loss did not improve from 1.23527\n",
      "Epoch 1873/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.2365 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01873: loss did not improve from 1.23527\n",
      "Epoch 1874/2000\n",
      "240/240 [==============================] - 0s 633us/step - loss: 1.2358 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01874: loss did not improve from 1.23527\n",
      "Epoch 1875/2000\n",
      "240/240 [==============================] - 0s 653us/step - loss: 1.2358 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01875: loss did not improve from 1.23527\n",
      "Epoch 1876/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 1.2354 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01876: loss did not improve from 1.23527\n",
      "Epoch 1877/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.2354 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01877: loss did not improve from 1.23527\n",
      "Epoch 1878/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.2352 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01878: loss improved from 1.23527 to 1.23515, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 1879/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 1.2354 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01879: loss did not improve from 1.23515\n",
      "Epoch 1880/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.2356 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01880: loss did not improve from 1.23515\n",
      "Epoch 1881/2000\n",
      "240/240 [==============================] - 0s 673us/step - loss: 1.2355 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01881: loss did not improve from 1.23515\n",
      "Epoch 1882/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.2355 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01882: loss did not improve from 1.23515\n",
      "Epoch 1883/2000\n",
      "240/240 [==============================] - 0s 642us/step - loss: 1.2357 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01883: loss did not improve from 1.23515\n",
      "Epoch 1884/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.2386 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01884: loss did not improve from 1.23515\n",
      "Epoch 1885/2000\n",
      "240/240 [==============================] - 0s 659us/step - loss: 1.2373 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01885: loss did not improve from 1.23515\n",
      "Epoch 1886/2000\n",
      "240/240 [==============================] - 0s 645us/step - loss: 1.2363 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01886: loss did not improve from 1.23515\n",
      "Epoch 1887/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2357 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01887: loss did not improve from 1.23515\n",
      "Epoch 1888/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2359 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01888: loss did not improve from 1.23515\n",
      "Epoch 1889/2000\n",
      "240/240 [==============================] - 0s 604us/step - loss: 1.2354 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01889: loss did not improve from 1.23515\n",
      "Epoch 1890/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 1.2356 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01890: loss did not improve from 1.23515\n",
      "Epoch 1891/2000\n",
      "240/240 [==============================] - 0s 629us/step - loss: 1.2355 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01891: loss did not improve from 1.23515\n",
      "Epoch 1892/2000\n",
      "240/240 [==============================] - 0s 642us/step - loss: 1.2354 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01892: loss did not improve from 1.23515\n",
      "Epoch 1893/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.2355 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01893: loss did not improve from 1.23515\n",
      "Epoch 1894/2000\n",
      "240/240 [==============================] - 0s 638us/step - loss: 1.2361 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01894: loss did not improve from 1.23515\n",
      "Epoch 1895/2000\n",
      "240/240 [==============================] - 0s 627us/step - loss: 1.2357 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01895: loss did not improve from 1.23515\n",
      "Epoch 1896/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 1.2362 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01896: loss did not improve from 1.23515\n",
      "Epoch 1897/2000\n",
      "240/240 [==============================] - 0s 632us/step - loss: 1.2385 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01897: loss did not improve from 1.23515\n",
      "Epoch 1898/2000\n",
      "240/240 [==============================] - 0s 628us/step - loss: 1.2378 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01898: loss did not improve from 1.23515\n",
      "Epoch 1899/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.2364 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01899: loss did not improve from 1.23515\n",
      "Epoch 1900/2000\n",
      "240/240 [==============================] - 0s 661us/step - loss: 1.2359 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01900: loss did not improve from 1.23515\n",
      "Epoch 1901/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.2354 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01901: loss did not improve from 1.23515\n",
      "Epoch 1902/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2353 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01902: loss did not improve from 1.23515\n",
      "Epoch 1903/2000\n",
      "240/240 [==============================] - 0s 649us/step - loss: 1.2354 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01903: loss did not improve from 1.23515\n",
      "Epoch 1904/2000\n",
      "240/240 [==============================] - 0s 697us/step - loss: 1.2356 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01904: loss did not improve from 1.23515\n",
      "Epoch 1905/2000\n",
      "240/240 [==============================] - 0s 812us/step - loss: 1.2358 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01905: loss did not improve from 1.23515\n",
      "Epoch 1906/2000\n",
      "240/240 [==============================] - 0s 834us/step - loss: 1.2355 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01906: loss did not improve from 1.23515\n",
      "Epoch 1907/2000\n",
      "240/240 [==============================] - 0s 817us/step - loss: 1.2354 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01907: loss did not improve from 1.23515\n",
      "Epoch 1908/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.2360 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01908: loss did not improve from 1.23515\n",
      "Epoch 1909/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 1.2360 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01909: loss did not improve from 1.23515\n",
      "Epoch 1910/2000\n",
      "240/240 [==============================] - 0s 682us/step - loss: 1.2356 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01910: loss did not improve from 1.23515\n",
      "Epoch 1911/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 1.2354 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01911: loss did not improve from 1.23515\n",
      "Epoch 1912/2000\n",
      "240/240 [==============================] - 0s 690us/step - loss: 1.2351 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01912: loss improved from 1.23515 to 1.23505, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 1913/2000\n",
      "240/240 [==============================] - 0s 665us/step - loss: 1.2353 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01913: loss did not improve from 1.23505\n",
      "Epoch 1914/2000\n",
      "240/240 [==============================] - 0s 637us/step - loss: 1.2354 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01914: loss did not improve from 1.23505\n",
      "Epoch 1915/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.2354 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01915: loss did not improve from 1.23505\n",
      "Epoch 1916/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 1.2354 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01916: loss did not improve from 1.23505\n",
      "Epoch 1917/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 1.2354 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01917: loss did not improve from 1.23505\n",
      "Epoch 1918/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2366 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01918: loss did not improve from 1.23505\n",
      "Epoch 1919/2000\n",
      "240/240 [==============================] - 0s 642us/step - loss: 1.2364 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01919: loss did not improve from 1.23505\n",
      "Epoch 1920/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2356 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01920: loss did not improve from 1.23505\n",
      "Epoch 1921/2000\n",
      "240/240 [==============================] - 0s 628us/step - loss: 1.2430 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01921: loss did not improve from 1.23505\n",
      "Epoch 1922/2000\n",
      "240/240 [==============================] - 0s 632us/step - loss: 1.2360 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01922: loss did not improve from 1.23505\n",
      "Epoch 1923/2000\n",
      "240/240 [==============================] - 0s 637us/step - loss: 1.2360 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01923: loss did not improve from 1.23505\n",
      "Epoch 1924/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2365 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01924: loss did not improve from 1.23505\n",
      "Epoch 1925/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2357 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01925: loss did not improve from 1.23505\n",
      "Epoch 1926/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.2355 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01926: loss did not improve from 1.23505\n",
      "Epoch 1927/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2354 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01927: loss did not improve from 1.23505\n",
      "Epoch 1928/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.2353 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01928: loss did not improve from 1.23505\n",
      "Epoch 1929/2000\n",
      "240/240 [==============================] - 0s 642us/step - loss: 1.2362 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01929: loss did not improve from 1.23505\n",
      "Epoch 1930/2000\n",
      "240/240 [==============================] - 0s 666us/step - loss: 1.2353 - accuracy: 0.4583\n",
      "\n",
      "Epoch 01930: loss did not improve from 1.23505\n",
      "Epoch 1931/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.2355 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01931: loss did not improve from 1.23505\n",
      "Epoch 1932/2000\n",
      "240/240 [==============================] - 0s 644us/step - loss: 1.2356 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01932: loss did not improve from 1.23505\n",
      "Epoch 1933/2000\n",
      "240/240 [==============================] - 0s 652us/step - loss: 1.2356 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01933: loss did not improve from 1.23505\n",
      "Epoch 1934/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2359 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01934: loss did not improve from 1.23505\n",
      "Epoch 1935/2000\n",
      "240/240 [==============================] - 0s 625us/step - loss: 1.2358 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01935: loss did not improve from 1.23505\n",
      "Epoch 1936/2000\n",
      "240/240 [==============================] - 0s 635us/step - loss: 1.2354 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01936: loss did not improve from 1.23505\n",
      "Epoch 1937/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 638us/step - loss: 1.2354 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01937: loss did not improve from 1.23505\n",
      "Epoch 1938/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2354 - accuracy: 0.4083\n",
      "\n",
      "Epoch 01938: loss did not improve from 1.23505\n",
      "Epoch 1939/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.2353 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01939: loss did not improve from 1.23505\n",
      "Epoch 1940/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.2352 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01940: loss did not improve from 1.23505\n",
      "Epoch 1941/2000\n",
      "240/240 [==============================] - 0s 625us/step - loss: 1.2354 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01941: loss did not improve from 1.23505\n",
      "Epoch 1942/2000\n",
      "240/240 [==============================] - 0s 619us/step - loss: 1.2353 - accuracy: 0.4625\n",
      "\n",
      "Epoch 01942: loss did not improve from 1.23505\n",
      "Epoch 1943/2000\n",
      "240/240 [==============================] - 0s 631us/step - loss: 1.2354 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01943: loss did not improve from 1.23505\n",
      "Epoch 1944/2000\n",
      "240/240 [==============================] - 0s 633us/step - loss: 1.2357 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01944: loss did not improve from 1.23505\n",
      "Epoch 1945/2000\n",
      "240/240 [==============================] - 0s 623us/step - loss: 1.2352 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01945: loss did not improve from 1.23505\n",
      "Epoch 1946/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.2361 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01946: loss did not improve from 1.23505\n",
      "Epoch 1947/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 1.2353 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01947: loss did not improve from 1.23505\n",
      "Epoch 1948/2000\n",
      "240/240 [==============================] - 0s 629us/step - loss: 1.2351 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01948: loss did not improve from 1.23505\n",
      "Epoch 1949/2000\n",
      "240/240 [==============================] - 0s 623us/step - loss: 1.2353 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01949: loss did not improve from 1.23505\n",
      "Epoch 1950/2000\n",
      "240/240 [==============================] - 0s 647us/step - loss: 1.2363 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01950: loss did not improve from 1.23505\n",
      "Epoch 1951/2000\n",
      "240/240 [==============================] - 0s 624us/step - loss: 1.2353 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01951: loss did not improve from 1.23505\n",
      "Epoch 1952/2000\n",
      "240/240 [==============================] - 0s 633us/step - loss: 1.2353 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01952: loss did not improve from 1.23505\n",
      "Epoch 1953/2000\n",
      "240/240 [==============================] - 0s 627us/step - loss: 1.2353 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01953: loss did not improve from 1.23505\n",
      "Epoch 1954/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2365 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01954: loss did not improve from 1.23505\n",
      "Epoch 1955/2000\n",
      "240/240 [==============================] - 0s 630us/step - loss: 1.2374 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01955: loss did not improve from 1.23505\n",
      "Epoch 1956/2000\n",
      "240/240 [==============================] - 0s 622us/step - loss: 1.2368 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01956: loss did not improve from 1.23505\n",
      "Epoch 1957/2000\n",
      "240/240 [==============================] - 0s 631us/step - loss: 1.2371 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01957: loss did not improve from 1.23505\n",
      "Epoch 1958/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 1.2356 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01958: loss did not improve from 1.23505\n",
      "Epoch 1959/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2354 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01959: loss did not improve from 1.23505\n",
      "Epoch 1960/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2352 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01960: loss did not improve from 1.23505\n",
      "Epoch 1961/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2352 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01961: loss did not improve from 1.23505\n",
      "Epoch 1962/2000\n",
      "240/240 [==============================] - 0s 621us/step - loss: 1.2351 - accuracy: 0.4583\n",
      "\n",
      "Epoch 01962: loss did not improve from 1.23505\n",
      "Epoch 1963/2000\n",
      "240/240 [==============================] - 0s 650us/step - loss: 1.2358 - accuracy: 0.4333\n",
      "\n",
      "Epoch 01963: loss did not improve from 1.23505\n",
      "Epoch 1964/2000\n",
      "240/240 [==============================] - 0s 677us/step - loss: 1.2354 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01964: loss did not improve from 1.23505\n",
      "Epoch 1965/2000\n",
      "240/240 [==============================] - 0s 669us/step - loss: 1.2369 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01965: loss did not improve from 1.23505\n",
      "Epoch 1966/2000\n",
      "240/240 [==============================] - 0s 629us/step - loss: 1.2356 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01966: loss did not improve from 1.23505\n",
      "Epoch 1967/2000\n",
      "240/240 [==============================] - 0s 633us/step - loss: 1.2352 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01967: loss did not improve from 1.23505\n",
      "Epoch 1968/2000\n",
      "240/240 [==============================] - 0s 634us/step - loss: 1.2355 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01968: loss did not improve from 1.23505\n",
      "Epoch 1969/2000\n",
      "240/240 [==============================] - 0s 654us/step - loss: 1.2354 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01969: loss did not improve from 1.23505\n",
      "Epoch 1970/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.3951 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01970: loss did not improve from 1.23505\n",
      "Epoch 1971/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.4955 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01971: loss did not improve from 1.23505\n",
      "Epoch 1972/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.2835 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01972: loss did not improve from 1.23505\n",
      "Epoch 1973/2000\n",
      "240/240 [==============================] - 0s 752us/step - loss: 1.2387 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01973: loss did not improve from 1.23505\n",
      "Epoch 1974/2000\n",
      "240/240 [==============================] - 0s 777us/step - loss: 1.2360 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01974: loss did not improve from 1.23505\n",
      "Epoch 1975/2000\n",
      "240/240 [==============================] - 0s 736us/step - loss: 1.2362 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01975: loss did not improve from 1.23505\n",
      "Epoch 1976/2000\n",
      "240/240 [==============================] - 0s 680us/step - loss: 1.2357 - accuracy: 0.4250\n",
      "\n",
      "Epoch 01976: loss did not improve from 1.23505\n",
      "Epoch 1977/2000\n",
      "240/240 [==============================] - 0s 639us/step - loss: 1.2356 - accuracy: 0.4042\n",
      "\n",
      "Epoch 01977: loss did not improve from 1.23505\n",
      "Epoch 1978/2000\n",
      "240/240 [==============================] - 0s 915us/step - loss: 1.2356 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01978: loss did not improve from 1.23505\n",
      "Epoch 1979/2000\n",
      "240/240 [==============================] - 0s 668us/step - loss: 1.2355 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01979: loss did not improve from 1.23505\n",
      "Epoch 1980/2000\n",
      "240/240 [==============================] - 0s 627us/step - loss: 1.2354 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01980: loss did not improve from 1.23505\n",
      "Epoch 1981/2000\n",
      "240/240 [==============================] - 0s 619us/step - loss: 1.2353 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01981: loss did not improve from 1.23505\n",
      "Epoch 1982/2000\n",
      "240/240 [==============================] - 0s 626us/step - loss: 1.2352 - accuracy: 0.4125\n",
      "\n",
      "Epoch 01982: loss did not improve from 1.23505\n",
      "Epoch 1983/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 1.2351 - accuracy: 0.4292\n",
      "\n",
      "Epoch 01983: loss did not improve from 1.23505\n",
      "Epoch 1984/2000\n",
      "240/240 [==============================] - 0s 746us/step - loss: 1.2349 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01984: loss improved from 1.23505 to 1.23489, saving model to ./model/1000001000100010004.hdf5\n",
      "Epoch 1985/2000\n",
      "240/240 [==============================] - 0s 798us/step - loss: 1.2352 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01985: loss did not improve from 1.23489\n",
      "Epoch 1986/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.2352 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01986: loss did not improve from 1.23489\n",
      "Epoch 1987/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.2350 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01987: loss did not improve from 1.23489\n",
      "Epoch 1988/2000\n",
      "240/240 [==============================] - 0s 698us/step - loss: 1.2349 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01988: loss did not improve from 1.23489\n",
      "Epoch 1989/2000\n",
      "240/240 [==============================] - 0s 694us/step - loss: 1.2352 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01989: loss did not improve from 1.23489\n",
      "Epoch 1990/2000\n",
      "240/240 [==============================] - 0s 692us/step - loss: 1.2351 - accuracy: 0.4500\n",
      "\n",
      "Epoch 01990: loss did not improve from 1.23489\n",
      "Epoch 1991/2000\n",
      "240/240 [==============================] - 0s 835us/step - loss: 1.2352 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01991: loss did not improve from 1.23489\n",
      "Epoch 1992/2000\n",
      "240/240 [==============================] - 0s 761us/step - loss: 1.2351 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01992: loss did not improve from 1.23489\n",
      "Epoch 1993/2000\n",
      "240/240 [==============================] - 0s 769us/step - loss: 1.2352 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01993: loss did not improve from 1.23489\n",
      "Epoch 1994/2000\n",
      "240/240 [==============================] - 0s 688us/step - loss: 1.2352 - accuracy: 0.4458\n",
      "\n",
      "Epoch 01994: loss did not improve from 1.23489\n",
      "Epoch 1995/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.2358 - accuracy: 0.4542\n",
      "\n",
      "Epoch 01995: loss did not improve from 1.23489\n",
      "Epoch 1996/2000\n",
      "240/240 [==============================] - 0s 657us/step - loss: 1.2353 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01996: loss did not improve from 1.23489\n",
      "Epoch 1997/2000\n",
      "240/240 [==============================] - 0s 662us/step - loss: 1.2352 - accuracy: 0.4208\n",
      "\n",
      "Epoch 01997: loss did not improve from 1.23489\n",
      "Epoch 1998/2000\n",
      "240/240 [==============================] - 0s 617us/step - loss: 1.2350 - accuracy: 0.4417\n",
      "\n",
      "Epoch 01998: loss did not improve from 1.23489\n",
      "Epoch 1999/2000\n",
      "240/240 [==============================] - 0s 676us/step - loss: 1.2351 - accuracy: 0.4375\n",
      "\n",
      "Epoch 01999: loss did not improve from 1.23489\n",
      "Epoch 2000/2000\n",
      "240/240 [==============================] - 0s 636us/step - loss: 1.3765 - accuracy: 0.4333\n",
      "\n",
      "Epoch 02000: loss did not improve from 1.23489\n"
     ]
    }
   ],
   "source": [
    "readdata_and_savemodel(\"1000001000100010004.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 4.8216 - accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00001: loss improved from inf to 4.82161, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 2/2000\n",
      "108/108 [==============================] - 0s 923us/step - loss: 4.5535 - accuracy: 0.0556\n",
      "\n",
      "Epoch 00002: loss improved from 4.82161 to 4.55354, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 3/2000\n",
      "108/108 [==============================] - 0s 803us/step - loss: 4.1951 - accuracy: 0.0833\n",
      "\n",
      "Epoch 00003: loss improved from 4.55354 to 4.19508, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 4/2000\n",
      "108/108 [==============================] - 0s 877us/step - loss: 3.6757 - accuracy: 0.1204\n",
      "\n",
      "Epoch 00004: loss improved from 4.19508 to 3.67569, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 5/2000\n",
      "108/108 [==============================] - 0s 803us/step - loss: 3.1438 - accuracy: 0.2500\n",
      "\n",
      "Epoch 00005: loss improved from 3.67569 to 3.14384, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 6/2000\n",
      "108/108 [==============================] - 0s 837us/step - loss: 2.7062 - accuracy: 0.3519\n",
      "\n",
      "Epoch 00006: loss improved from 3.14384 to 2.70620, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 7/2000\n",
      "108/108 [==============================] - 0s 803us/step - loss: 2.3909 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00007: loss improved from 2.70620 to 2.39092, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 8/2000\n",
      "108/108 [==============================] - 0s 738us/step - loss: 2.1879 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00008: loss improved from 2.39092 to 2.18794, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 9/2000\n",
      "108/108 [==============================] - 0s 659us/step - loss: 2.0557 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00009: loss improved from 2.18794 to 2.05569, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 10/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.9839 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00010: loss improved from 2.05569 to 1.98395, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 11/2000\n",
      "108/108 [==============================] - 0s 535us/step - loss: 1.9295 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00011: loss improved from 1.98395 to 1.92951, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 12/2000\n",
      "108/108 [==============================] - 0s 671us/step - loss: 1.8850 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00012: loss improved from 1.92951 to 1.88498, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 13/2000\n",
      "108/108 [==============================] - 0s 733us/step - loss: 1.8515 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00013: loss improved from 1.88498 to 1.85154, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 14/2000\n",
      "108/108 [==============================] - 0s 711us/step - loss: 1.8382 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00014: loss improved from 1.85154 to 1.83823, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 15/2000\n",
      "108/108 [==============================] - 0s 685us/step - loss: 1.8274 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00015: loss improved from 1.83823 to 1.82740, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 16/2000\n",
      "108/108 [==============================] - 0s 764us/step - loss: 1.8031 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00016: loss improved from 1.82740 to 1.80305, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 17/2000\n",
      "108/108 [==============================] - 0s 705us/step - loss: 1.7912 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00017: loss improved from 1.80305 to 1.79124, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 18/2000\n",
      "108/108 [==============================] - 0s 751us/step - loss: 1.7789 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00018: loss improved from 1.79124 to 1.77893, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 19/2000\n",
      "108/108 [==============================] - 0s 724us/step - loss: 1.7752 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00019: loss improved from 1.77893 to 1.77522, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 20/2000\n",
      "108/108 [==============================] - 0s 742us/step - loss: 1.7601 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00020: loss improved from 1.77522 to 1.76009, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 21/2000\n",
      "108/108 [==============================] - 0s 729us/step - loss: 1.7662 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00021: loss did not improve from 1.76009\n",
      "Epoch 22/2000\n",
      "108/108 [==============================] - 0s 730us/step - loss: 1.7399 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00022: loss improved from 1.76009 to 1.73987, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 23/2000\n",
      "108/108 [==============================] - 0s 675us/step - loss: 1.7427 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00023: loss did not improve from 1.73987\n",
      "Epoch 24/2000\n",
      "108/108 [==============================] - 0s 722us/step - loss: 1.7433 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00024: loss did not improve from 1.73987\n",
      "Epoch 25/2000\n",
      "108/108 [==============================] - 0s 702us/step - loss: 1.7260 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00025: loss improved from 1.73987 to 1.72599, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 26/2000\n",
      "108/108 [==============================] - 0s 721us/step - loss: 1.7323 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00026: loss did not improve from 1.72599\n",
      "Epoch 27/2000\n",
      "108/108 [==============================] - 0s 698us/step - loss: 1.7250 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00027: loss improved from 1.72599 to 1.72496, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 28/2000\n",
      "108/108 [==============================] - 0s 761us/step - loss: 1.7211 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00028: loss improved from 1.72496 to 1.72114, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 29/2000\n",
      "108/108 [==============================] - 0s 690us/step - loss: 1.7220 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00029: loss did not improve from 1.72114\n",
      "Epoch 30/2000\n",
      "108/108 [==============================] - 0s 656us/step - loss: 1.7097 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00030: loss improved from 1.72114 to 1.70973, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 31/2000\n",
      "108/108 [==============================] - 0s 677us/step - loss: 1.7099 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00031: loss did not improve from 1.70973\n",
      "Epoch 32/2000\n",
      "108/108 [==============================] - 0s 671us/step - loss: 1.7014 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00032: loss improved from 1.70973 to 1.70145, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 33/2000\n",
      "108/108 [==============================] - 0s 680us/step - loss: 1.7046 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00033: loss did not improve from 1.70145\n",
      "Epoch 34/2000\n",
      "108/108 [==============================] - 0s 673us/step - loss: 1.6971 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00034: loss improved from 1.70145 to 1.69712, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 35/2000\n",
      "108/108 [==============================] - 0s 688us/step - loss: 1.6951 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00035: loss improved from 1.69712 to 1.69505, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 36/2000\n",
      "108/108 [==============================] - 0s 695us/step - loss: 1.6968 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00036: loss did not improve from 1.69505\n",
      "Epoch 37/2000\n",
      "108/108 [==============================] - 0s 665us/step - loss: 1.6920 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00037: loss improved from 1.69505 to 1.69196, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 38/2000\n",
      "108/108 [==============================] - 0s 729us/step - loss: 1.6835 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00038: loss improved from 1.69196 to 1.68352, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 39/2000\n",
      "108/108 [==============================] - 0s 669us/step - loss: 1.6845 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00039: loss did not improve from 1.68352\n",
      "Epoch 40/2000\n",
      "108/108 [==============================] - 0s 679us/step - loss: 1.6903 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00040: loss did not improve from 1.68352\n",
      "Epoch 41/2000\n",
      "108/108 [==============================] - 0s 676us/step - loss: 1.6787 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00041: loss improved from 1.68352 to 1.67867, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 42/2000\n",
      "108/108 [==============================] - 0s 758us/step - loss: 1.6838 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00042: loss did not improve from 1.67867\n",
      "Epoch 43/2000\n",
      "108/108 [==============================] - 0s 680us/step - loss: 1.6752 - accuracy: 0.4167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00043: loss improved from 1.67867 to 1.67515, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 44/2000\n",
      "108/108 [==============================] - 0s 712us/step - loss: 1.6760 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00044: loss did not improve from 1.67515\n",
      "Epoch 45/2000\n",
      "108/108 [==============================] - 0s 680us/step - loss: 1.6758 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00045: loss did not improve from 1.67515\n",
      "Epoch 46/2000\n",
      "108/108 [==============================] - 0s 689us/step - loss: 1.6655 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00046: loss improved from 1.67515 to 1.66551, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 47/2000\n",
      "108/108 [==============================] - 0s 726us/step - loss: 1.6656 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00047: loss did not improve from 1.66551\n",
      "Epoch 48/2000\n",
      "108/108 [==============================] - 0s 729us/step - loss: 1.6628 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00048: loss improved from 1.66551 to 1.66282, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 49/2000\n",
      "108/108 [==============================] - 0s 751us/step - loss: 1.6581 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00049: loss improved from 1.66282 to 1.65806, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 50/2000\n",
      "108/108 [==============================] - 0s 727us/step - loss: 1.6655 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00050: loss did not improve from 1.65806\n",
      "Epoch 51/2000\n",
      "108/108 [==============================] - 0s 716us/step - loss: 1.6631 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00051: loss did not improve from 1.65806\n",
      "Epoch 52/2000\n",
      "108/108 [==============================] - 0s 744us/step - loss: 1.6551 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00052: loss improved from 1.65806 to 1.65511, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 53/2000\n",
      "108/108 [==============================] - 0s 757us/step - loss: 1.6607 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00053: loss did not improve from 1.65511\n",
      "Epoch 54/2000\n",
      "108/108 [==============================] - 0s 751us/step - loss: 1.6539 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00054: loss improved from 1.65511 to 1.65390, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 55/2000\n",
      "108/108 [==============================] - 0s 757us/step - loss: 1.6551 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00055: loss did not improve from 1.65390\n",
      "Epoch 56/2000\n",
      "108/108 [==============================] - 0s 742us/step - loss: 1.6519 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00056: loss improved from 1.65390 to 1.65191, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 57/2000\n",
      "108/108 [==============================] - 0s 731us/step - loss: 1.6532 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00057: loss did not improve from 1.65191\n",
      "Epoch 58/2000\n",
      "108/108 [==============================] - 0s 741us/step - loss: 1.6549 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00058: loss did not improve from 1.65191\n",
      "Epoch 59/2000\n",
      "108/108 [==============================] - 0s 745us/step - loss: 1.6485 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00059: loss improved from 1.65191 to 1.64850, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 60/2000\n",
      "108/108 [==============================] - 0s 723us/step - loss: 1.6498 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00060: loss did not improve from 1.64850\n",
      "Epoch 61/2000\n",
      "108/108 [==============================] - 0s 726us/step - loss: 1.6462 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00061: loss improved from 1.64850 to 1.64620, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 62/2000\n",
      "108/108 [==============================] - 0s 737us/step - loss: 1.6465 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00062: loss did not improve from 1.64620\n",
      "Epoch 63/2000\n",
      "108/108 [==============================] - 0s 744us/step - loss: 1.6413 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00063: loss improved from 1.64620 to 1.64133, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 64/2000\n",
      "108/108 [==============================] - 0s 750us/step - loss: 1.6434 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00064: loss did not improve from 1.64133\n",
      "Epoch 65/2000\n",
      "108/108 [==============================] - 0s 734us/step - loss: 1.6408 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00065: loss improved from 1.64133 to 1.64082, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 66/2000\n",
      "108/108 [==============================] - 0s 739us/step - loss: 1.6380 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00066: loss improved from 1.64082 to 1.63796, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 67/2000\n",
      "108/108 [==============================] - 0s 716us/step - loss: 1.6405 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00067: loss did not improve from 1.63796\n",
      "Epoch 68/2000\n",
      "108/108 [==============================] - 0s 727us/step - loss: 1.6344 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00068: loss improved from 1.63796 to 1.63438, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 69/2000\n",
      "108/108 [==============================] - 0s 756us/step - loss: 1.6371 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00069: loss did not improve from 1.63438\n",
      "Epoch 70/2000\n",
      "108/108 [==============================] - 0s 735us/step - loss: 1.6369 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00070: loss did not improve from 1.63438\n",
      "Epoch 71/2000\n",
      "108/108 [==============================] - 0s 745us/step - loss: 1.6329 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00071: loss improved from 1.63438 to 1.63286, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 72/2000\n",
      "108/108 [==============================] - 0s 757us/step - loss: 1.6318 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00072: loss improved from 1.63286 to 1.63182, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 73/2000\n",
      "108/108 [==============================] - 0s 725us/step - loss: 1.6306 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00073: loss improved from 1.63182 to 1.63056, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 74/2000\n",
      "108/108 [==============================] - 0s 742us/step - loss: 1.6295 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00074: loss improved from 1.63056 to 1.62945, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 75/2000\n",
      "108/108 [==============================] - 0s 717us/step - loss: 1.6321 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00075: loss did not improve from 1.62945\n",
      "Epoch 76/2000\n",
      "108/108 [==============================] - 0s 744us/step - loss: 1.6212 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00076: loss improved from 1.62945 to 1.62122, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 77/2000\n",
      "108/108 [==============================] - 0s 720us/step - loss: 1.6249 - accuracy: 0.4352\n",
      "\n",
      "Epoch 00077: loss did not improve from 1.62122\n",
      "Epoch 78/2000\n",
      "108/108 [==============================] - 0s 733us/step - loss: 1.6231 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00078: loss did not improve from 1.62122\n",
      "Epoch 79/2000\n",
      "108/108 [==============================] - 0s 765us/step - loss: 1.6250 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00079: loss did not improve from 1.62122\n",
      "Epoch 80/2000\n",
      "108/108 [==============================] - 0s 761us/step - loss: 1.6240 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00080: loss did not improve from 1.62122\n",
      "Epoch 81/2000\n",
      "108/108 [==============================] - 0s 731us/step - loss: 1.6235 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00081: loss did not improve from 1.62122\n",
      "Epoch 82/2000\n",
      "108/108 [==============================] - 0s 723us/step - loss: 1.6185 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00082: loss improved from 1.62122 to 1.61850, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 83/2000\n",
      "108/108 [==============================] - 0s 728us/step - loss: 1.6179 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00083: loss improved from 1.61850 to 1.61791, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 84/2000\n",
      "108/108 [==============================] - 0s 736us/step - loss: 1.6164 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00084: loss improved from 1.61791 to 1.61639, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 85/2000\n",
      "108/108 [==============================] - 0s 708us/step - loss: 1.6218 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00085: loss did not improve from 1.61639\n",
      "Epoch 86/2000\n",
      "108/108 [==============================] - 0s 744us/step - loss: 1.6233 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00086: loss did not improve from 1.61639\n",
      "Epoch 87/2000\n",
      "108/108 [==============================] - 0s 737us/step - loss: 1.6179 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00087: loss did not improve from 1.61639\n",
      "Epoch 88/2000\n",
      "108/108 [==============================] - 0s 736us/step - loss: 1.6159 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00088: loss improved from 1.61639 to 1.61590, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 89/2000\n",
      "108/108 [==============================] - 0s 720us/step - loss: 1.6163 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00089: loss did not improve from 1.61590\n",
      "Epoch 90/2000\n",
      "108/108 [==============================] - 0s 734us/step - loss: 1.6142 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00090: loss improved from 1.61590 to 1.61422, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 91/2000\n",
      "108/108 [==============================] - 0s 719us/step - loss: 1.6118 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00091: loss improved from 1.61422 to 1.61180, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 92/2000\n",
      "108/108 [==============================] - 0s 769us/step - loss: 1.6107 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00092: loss improved from 1.61180 to 1.61073, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 93/2000\n",
      "108/108 [==============================] - 0s 724us/step - loss: 1.6129 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00093: loss did not improve from 1.61073\n",
      "Epoch 94/2000\n",
      "108/108 [==============================] - 0s 740us/step - loss: 1.6124 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00094: loss did not improve from 1.61073\n",
      "Epoch 95/2000\n",
      "108/108 [==============================] - 0s 728us/step - loss: 1.6117 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00095: loss did not improve from 1.61073\n",
      "Epoch 96/2000\n",
      "108/108 [==============================] - 0s 747us/step - loss: 1.6134 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00096: loss did not improve from 1.61073\n",
      "Epoch 97/2000\n",
      "108/108 [==============================] - 0s 728us/step - loss: 1.6131 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00097: loss did not improve from 1.61073\n",
      "Epoch 98/2000\n",
      "108/108 [==============================] - 0s 745us/step - loss: 1.6060 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00098: loss improved from 1.61073 to 1.60601, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 99/2000\n",
      "108/108 [==============================] - 0s 752us/step - loss: 1.6094 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00099: loss did not improve from 1.60601\n",
      "Epoch 100/2000\n",
      "108/108 [==============================] - 0s 739us/step - loss: 1.6087 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00100: loss did not improve from 1.60601\n",
      "Epoch 101/2000\n",
      "108/108 [==============================] - 0s 723us/step - loss: 1.6073 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00101: loss did not improve from 1.60601\n",
      "Epoch 102/2000\n",
      "108/108 [==============================] - 0s 747us/step - loss: 1.6061 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00102: loss did not improve from 1.60601\n",
      "Epoch 103/2000\n",
      "108/108 [==============================] - 0s 725us/step - loss: 1.6025 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00103: loss improved from 1.60601 to 1.60255, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 104/2000\n",
      "108/108 [==============================] - 0s 709us/step - loss: 1.6049 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00104: loss did not improve from 1.60255\n",
      "Epoch 105/2000\n",
      "108/108 [==============================] - 0s 721us/step - loss: 1.6059 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00105: loss did not improve from 1.60255\n",
      "Epoch 106/2000\n",
      "108/108 [==============================] - 0s 746us/step - loss: 1.6047 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00106: loss did not improve from 1.60255\n",
      "Epoch 107/2000\n",
      "108/108 [==============================] - 0s 738us/step - loss: 1.6029 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00107: loss did not improve from 1.60255\n",
      "Epoch 108/2000\n",
      "108/108 [==============================] - 0s 743us/step - loss: 1.6055 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00108: loss did not improve from 1.60255\n",
      "Epoch 109/2000\n",
      "108/108 [==============================] - 0s 736us/step - loss: 1.6027 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00109: loss did not improve from 1.60255\n",
      "Epoch 110/2000\n",
      "108/108 [==============================] - 0s 731us/step - loss: 1.6004 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00110: loss improved from 1.60255 to 1.60040, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 111/2000\n",
      "108/108 [==============================] - 0s 748us/step - loss: 1.6019 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00111: loss did not improve from 1.60040\n",
      "Epoch 112/2000\n",
      "108/108 [==============================] - 0s 725us/step - loss: 1.6026 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00112: loss did not improve from 1.60040\n",
      "Epoch 113/2000\n",
      "108/108 [==============================] - 0s 745us/step - loss: 1.5996 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00113: loss improved from 1.60040 to 1.59963, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 114/2000\n",
      "108/108 [==============================] - 0s 730us/step - loss: 1.6091 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00114: loss did not improve from 1.59963\n",
      "Epoch 115/2000\n",
      "108/108 [==============================] - 0s 735us/step - loss: 1.8643 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00115: loss did not improve from 1.59963\n",
      "Epoch 116/2000\n",
      "108/108 [==============================] - 0s 757us/step - loss: 1.5967 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00116: loss improved from 1.59963 to 1.59669, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 117/2000\n",
      "108/108 [==============================] - 0s 779us/step - loss: 1.5989 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00117: loss did not improve from 1.59669\n",
      "Epoch 118/2000\n",
      "108/108 [==============================] - 0s 709us/step - loss: 1.5971 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00118: loss did not improve from 1.59669\n",
      "Epoch 119/2000\n",
      "108/108 [==============================] - 0s 738us/step - loss: 1.5968 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00119: loss did not improve from 1.59669\n",
      "Epoch 120/2000\n",
      "108/108 [==============================] - 0s 743us/step - loss: 1.5958 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00120: loss improved from 1.59669 to 1.59579, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 121/2000\n",
      "108/108 [==============================] - 0s 789us/step - loss: 1.5958 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00121: loss did not improve from 1.59579\n",
      "Epoch 122/2000\n",
      "108/108 [==============================] - 0s 735us/step - loss: 1.5963 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00122: loss did not improve from 1.59579\n",
      "Epoch 123/2000\n",
      "108/108 [==============================] - 0s 731us/step - loss: 1.5937 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00123: loss improved from 1.59579 to 1.59366, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 124/2000\n",
      "108/108 [==============================] - 0s 714us/step - loss: 1.5978 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00124: loss did not improve from 1.59366\n",
      "Epoch 125/2000\n",
      "108/108 [==============================] - 0s 735us/step - loss: 1.5958 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00125: loss did not improve from 1.59366\n",
      "Epoch 126/2000\n",
      "108/108 [==============================] - 0s 718us/step - loss: 1.5943 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00126: loss did not improve from 1.59366\n",
      "Epoch 127/2000\n",
      "108/108 [==============================] - 0s 743us/step - loss: 1.5936 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00127: loss improved from 1.59366 to 1.59363, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 128/2000\n",
      "108/108 [==============================] - 0s 759us/step - loss: 1.5961 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00128: loss did not improve from 1.59363\n",
      "Epoch 129/2000\n",
      "108/108 [==============================] - 0s 760us/step - loss: 1.5941 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00129: loss did not improve from 1.59363\n",
      "Epoch 130/2000\n",
      "108/108 [==============================] - 0s 732us/step - loss: 1.5937 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00130: loss did not improve from 1.59363\n",
      "Epoch 131/2000\n",
      "108/108 [==============================] - 0s 738us/step - loss: 1.5936 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00131: loss improved from 1.59363 to 1.59358, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 132/2000\n",
      "108/108 [==============================] - 0s 747us/step - loss: 1.5948 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00132: loss did not improve from 1.59358\n",
      "Epoch 133/2000\n",
      "108/108 [==============================] - 0s 740us/step - loss: 1.5936 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00133: loss did not improve from 1.59358\n",
      "Epoch 134/2000\n",
      "108/108 [==============================] - 0s 726us/step - loss: 1.5925 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00134: loss improved from 1.59358 to 1.59248, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 135/2000\n",
      "108/108 [==============================] - 0s 776us/step - loss: 1.5923 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00135: loss improved from 1.59248 to 1.59233, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 136/2000\n",
      "108/108 [==============================] - 0s 721us/step - loss: 1.5907 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00136: loss improved from 1.59233 to 1.59072, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 137/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 725us/step - loss: 1.5909 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00137: loss did not improve from 1.59072\n",
      "Epoch 138/2000\n",
      "108/108 [==============================] - 0s 735us/step - loss: 1.5920 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00138: loss did not improve from 1.59072\n",
      "Epoch 139/2000\n",
      "108/108 [==============================] - 0s 731us/step - loss: 1.5925 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00139: loss did not improve from 1.59072\n",
      "Epoch 140/2000\n",
      "108/108 [==============================] - 0s 725us/step - loss: 1.5923 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00140: loss did not improve from 1.59072\n",
      "Epoch 141/2000\n",
      "108/108 [==============================] - 0s 735us/step - loss: 1.5919 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00141: loss did not improve from 1.59072\n",
      "Epoch 142/2000\n",
      "108/108 [==============================] - 0s 738us/step - loss: 1.5919 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00142: loss did not improve from 1.59072\n",
      "Epoch 143/2000\n",
      "108/108 [==============================] - 0s 754us/step - loss: 1.5967 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00143: loss did not improve from 1.59072\n",
      "Epoch 144/2000\n",
      "108/108 [==============================] - 0s 739us/step - loss: 1.5938 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00144: loss did not improve from 1.59072\n",
      "Epoch 145/2000\n",
      "108/108 [==============================] - 0s 723us/step - loss: 1.5898 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00145: loss improved from 1.59072 to 1.58982, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 146/2000\n",
      "108/108 [==============================] - 0s 731us/step - loss: 1.5896 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00146: loss improved from 1.58982 to 1.58962, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 147/2000\n",
      "108/108 [==============================] - 0s 746us/step - loss: 1.5895 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00147: loss improved from 1.58962 to 1.58952, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 148/2000\n",
      "108/108 [==============================] - 0s 744us/step - loss: 1.5875 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00148: loss improved from 1.58952 to 1.58749, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 149/2000\n",
      "108/108 [==============================] - 0s 714us/step - loss: 1.5888 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00149: loss did not improve from 1.58749\n",
      "Epoch 150/2000\n",
      "108/108 [==============================] - 0s 740us/step - loss: 1.5905 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00150: loss did not improve from 1.58749\n",
      "Epoch 151/2000\n",
      "108/108 [==============================] - 0s 729us/step - loss: 1.5889 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00151: loss did not improve from 1.58749\n",
      "Epoch 152/2000\n",
      "108/108 [==============================] - 0s 736us/step - loss: 1.5882 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00152: loss did not improve from 1.58749\n",
      "Epoch 153/2000\n",
      "108/108 [==============================] - 0s 730us/step - loss: 1.5899 - accuracy: 0.4352\n",
      "\n",
      "Epoch 00153: loss did not improve from 1.58749\n",
      "Epoch 154/2000\n",
      "108/108 [==============================] - 0s 747us/step - loss: 1.5893 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00154: loss did not improve from 1.58749\n",
      "Epoch 155/2000\n",
      "108/108 [==============================] - 0s 742us/step - loss: 1.5887 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00155: loss did not improve from 1.58749\n",
      "Epoch 156/2000\n",
      "108/108 [==============================] - 0s 738us/step - loss: 1.5886 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00156: loss did not improve from 1.58749\n",
      "Epoch 157/2000\n",
      "108/108 [==============================] - 0s 742us/step - loss: 1.5871 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00157: loss improved from 1.58749 to 1.58711, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 158/2000\n",
      "108/108 [==============================] - 0s 710us/step - loss: 1.5867 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00158: loss improved from 1.58711 to 1.58673, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 159/2000\n",
      "108/108 [==============================] - 0s 760us/step - loss: 1.5863 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00159: loss improved from 1.58673 to 1.58627, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 160/2000\n",
      "108/108 [==============================] - 0s 741us/step - loss: 1.5875 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00160: loss did not improve from 1.58627\n",
      "Epoch 161/2000\n",
      "108/108 [==============================] - 0s 726us/step - loss: 1.5867 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00161: loss did not improve from 1.58627\n",
      "Epoch 162/2000\n",
      "108/108 [==============================] - 0s 739us/step - loss: 1.5879 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00162: loss did not improve from 1.58627\n",
      "Epoch 163/2000\n",
      "108/108 [==============================] - 0s 740us/step - loss: 1.5872 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00163: loss did not improve from 1.58627\n",
      "Epoch 164/2000\n",
      "108/108 [==============================] - 0s 738us/step - loss: 1.5853 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00164: loss improved from 1.58627 to 1.58530, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 165/2000\n",
      "108/108 [==============================] - 0s 752us/step - loss: 1.5856 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00165: loss did not improve from 1.58530\n",
      "Epoch 166/2000\n",
      "108/108 [==============================] - 0s 750us/step - loss: 1.5889 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00166: loss did not improve from 1.58530\n",
      "Epoch 167/2000\n",
      "108/108 [==============================] - 0s 733us/step - loss: 1.5872 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00167: loss did not improve from 1.58530\n",
      "Epoch 168/2000\n",
      "108/108 [==============================] - 0s 723us/step - loss: 1.5863 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00168: loss did not improve from 1.58530\n",
      "Epoch 169/2000\n",
      "108/108 [==============================] - 0s 743us/step - loss: 1.5870 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00169: loss did not improve from 1.58530\n",
      "Epoch 170/2000\n",
      "108/108 [==============================] - 0s 721us/step - loss: 1.5850 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00170: loss improved from 1.58530 to 1.58505, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 171/2000\n",
      "108/108 [==============================] - 0s 754us/step - loss: 1.5851 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00171: loss did not improve from 1.58505\n",
      "Epoch 172/2000\n",
      "108/108 [==============================] - 0s 736us/step - loss: 1.5845 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00172: loss improved from 1.58505 to 1.58447, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 173/2000\n",
      "108/108 [==============================] - 0s 762us/step - loss: 1.5864 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00173: loss did not improve from 1.58447\n",
      "Epoch 174/2000\n",
      "108/108 [==============================] - 0s 746us/step - loss: 1.5893 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00174: loss did not improve from 1.58447\n",
      "Epoch 175/2000\n",
      "108/108 [==============================] - 0s 749us/step - loss: 1.5898 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00175: loss did not improve from 1.58447\n",
      "Epoch 176/2000\n",
      "108/108 [==============================] - 0s 741us/step - loss: 1.5881 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00176: loss did not improve from 1.58447\n",
      "Epoch 177/2000\n",
      "108/108 [==============================] - 0s 730us/step - loss: 1.5841 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00177: loss improved from 1.58447 to 1.58415, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 178/2000\n",
      "108/108 [==============================] - 0s 765us/step - loss: 1.5836 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00178: loss improved from 1.58415 to 1.58359, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 179/2000\n",
      "108/108 [==============================] - 0s 722us/step - loss: 1.5842 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00179: loss did not improve from 1.58359\n",
      "Epoch 180/2000\n",
      "108/108 [==============================] - 0s 729us/step - loss: 1.5836 - accuracy: 0.3704\n",
      "\n",
      "Epoch 00180: loss did not improve from 1.58359\n",
      "Epoch 181/2000\n",
      "108/108 [==============================] - 0s 739us/step - loss: 1.5842 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00181: loss did not improve from 1.58359\n",
      "Epoch 182/2000\n",
      "108/108 [==============================] - 0s 744us/step - loss: 1.5825 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00182: loss improved from 1.58359 to 1.58249, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 183/2000\n",
      "108/108 [==============================] - 0s 904us/step - loss: 1.5824 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00183: loss improved from 1.58249 to 1.58241, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 184/2000\n",
      "108/108 [==============================] - 0s 812us/step - loss: 1.5828 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00184: loss did not improve from 1.58241\n",
      "Epoch 185/2000\n",
      "108/108 [==============================] - 0s 791us/step - loss: 1.5804 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00185: loss improved from 1.58241 to 1.58041, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 186/2000\n",
      "108/108 [==============================] - 0s 766us/step - loss: 1.5820 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00186: loss did not improve from 1.58041\n",
      "Epoch 187/2000\n",
      "108/108 [==============================] - 0s 757us/step - loss: 1.5816 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00187: loss did not improve from 1.58041\n",
      "Epoch 188/2000\n",
      "108/108 [==============================] - 0s 742us/step - loss: 1.5841 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00188: loss did not improve from 1.58041\n",
      "Epoch 189/2000\n",
      "108/108 [==============================] - 0s 748us/step - loss: 1.5872 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00189: loss did not improve from 1.58041\n",
      "Epoch 190/2000\n",
      "108/108 [==============================] - 0s 726us/step - loss: 1.5846 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00190: loss did not improve from 1.58041\n",
      "Epoch 191/2000\n",
      "108/108 [==============================] - 0s 624us/step - loss: 1.5818 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00191: loss did not improve from 1.58041\n",
      "Epoch 192/2000\n",
      "108/108 [==============================] - 0s 618us/step - loss: 1.5825 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00192: loss did not improve from 1.58041\n",
      "Epoch 193/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5817 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00193: loss did not improve from 1.58041\n",
      "Epoch 194/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5878 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00194: loss did not improve from 1.58041\n",
      "Epoch 195/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5833 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00195: loss did not improve from 1.58041\n",
      "Epoch 196/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5809 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00196: loss did not improve from 1.58041\n",
      "Epoch 197/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5803 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00197: loss improved from 1.58041 to 1.58028, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 198/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5796 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00198: loss improved from 1.58028 to 1.57961, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 199/2000\n",
      "108/108 [==============================] - 0s 712us/step - loss: 1.5802 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00199: loss did not improve from 1.57961\n",
      "Epoch 200/2000\n",
      "108/108 [==============================] - 0s 704us/step - loss: 1.5809 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00200: loss did not improve from 1.57961\n",
      "Epoch 201/2000\n",
      "108/108 [==============================] - 0s 714us/step - loss: 1.5798 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00201: loss did not improve from 1.57961\n",
      "Epoch 202/2000\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.5297 - accuracy: 0.4400   - 0s 690us/step - loss: 1.5803 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00202: loss did not improve from 1.57961\n",
      "Epoch 203/2000\n",
      "108/108 [==============================] - 0s 701us/step - loss: 1.5800 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00203: loss did not improve from 1.57961\n",
      "Epoch 204/2000\n",
      "108/108 [==============================] - 0s 707us/step - loss: 1.5792 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00204: loss improved from 1.57961 to 1.57923, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 205/2000\n",
      "108/108 [==============================] - 0s 743us/step - loss: 1.5812 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00205: loss did not improve from 1.57923\n",
      "Epoch 206/2000\n",
      "108/108 [==============================] - 0s 742us/step - loss: 1.5796 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00206: loss did not improve from 1.57923\n",
      "Epoch 207/2000\n",
      "108/108 [==============================] - 0s 753us/step - loss: 1.5802 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00207: loss did not improve from 1.57923\n",
      "Epoch 208/2000\n",
      "108/108 [==============================] - 0s 740us/step - loss: 1.5785 - accuracy: 0.4352\n",
      "\n",
      "Epoch 00208: loss improved from 1.57923 to 1.57850, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 209/2000\n",
      "108/108 [==============================] - 0s 744us/step - loss: 1.5794 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00209: loss did not improve from 1.57850\n",
      "Epoch 210/2000\n",
      "108/108 [==============================] - 0s 740us/step - loss: 1.5783 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00210: loss improved from 1.57850 to 1.57833, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 211/2000\n",
      "108/108 [==============================] - 0s 760us/step - loss: 1.5776 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00211: loss improved from 1.57833 to 1.57761, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 212/2000\n",
      "108/108 [==============================] - 0s 738us/step - loss: 1.5790 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00212: loss did not improve from 1.57761\n",
      "Epoch 213/2000\n",
      "108/108 [==============================] - 0s 722us/step - loss: 1.5787 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00213: loss did not improve from 1.57761\n",
      "Epoch 214/2000\n",
      "108/108 [==============================] - 0s 745us/step - loss: 1.5789 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00214: loss did not improve from 1.57761\n",
      "Epoch 215/2000\n",
      "108/108 [==============================] - 0s 721us/step - loss: 1.5785 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00215: loss did not improve from 1.57761\n",
      "Epoch 216/2000\n",
      "108/108 [==============================] - 0s 747us/step - loss: 1.5781 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00216: loss did not improve from 1.57761\n",
      "Epoch 217/2000\n",
      "108/108 [==============================] - 0s 772us/step - loss: 1.5774 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00217: loss improved from 1.57761 to 1.57738, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 218/2000\n",
      "108/108 [==============================] - 0s 738us/step - loss: 1.5780 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00218: loss did not improve from 1.57738\n",
      "Epoch 219/2000\n",
      "108/108 [==============================] - 0s 741us/step - loss: 1.5776 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00219: loss did not improve from 1.57738\n",
      "Epoch 220/2000\n",
      "108/108 [==============================] - 0s 739us/step - loss: 1.5790 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00220: loss did not improve from 1.57738\n",
      "Epoch 221/2000\n",
      "108/108 [==============================] - 0s 735us/step - loss: 1.5783 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00221: loss did not improve from 1.57738\n",
      "Epoch 222/2000\n",
      "108/108 [==============================] - 0s 744us/step - loss: 1.5798 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00222: loss did not improve from 1.57738\n",
      "Epoch 223/2000\n",
      "108/108 [==============================] - 0s 746us/step - loss: 1.5900 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00223: loss did not improve from 1.57738\n",
      "Epoch 224/2000\n",
      "108/108 [==============================] - 0s 728us/step - loss: 1.5792 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00224: loss did not improve from 1.57738\n",
      "Epoch 225/2000\n",
      "108/108 [==============================] - 0s 741us/step - loss: 1.5775 - accuracy: 0.4352\n",
      "\n",
      "Epoch 00225: loss did not improve from 1.57738\n",
      "Epoch 226/2000\n",
      "108/108 [==============================] - 0s 725us/step - loss: 1.5766 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00226: loss improved from 1.57738 to 1.57663, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 227/2000\n",
      "108/108 [==============================] - 0s 736us/step - loss: 1.5777 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00227: loss did not improve from 1.57663\n",
      "Epoch 228/2000\n",
      "108/108 [==============================] - 0s 735us/step - loss: 1.5764 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00228: loss improved from 1.57663 to 1.57643, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 229/2000\n",
      "108/108 [==============================] - 0s 792us/step - loss: 1.5779 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00229: loss did not improve from 1.57643\n",
      "Epoch 230/2000\n",
      "108/108 [==============================] - 0s 717us/step - loss: 1.5785 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00230: loss did not improve from 1.57643\n",
      "Epoch 231/2000\n",
      "108/108 [==============================] - 0s 731us/step - loss: 1.5776 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00231: loss did not improve from 1.57643\n",
      "Epoch 232/2000\n",
      "108/108 [==============================] - 0s 745us/step - loss: 1.5764 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00232: loss improved from 1.57643 to 1.57638, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 233/2000\n",
      "108/108 [==============================] - 0s 728us/step - loss: 1.5776 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00233: loss did not improve from 1.57638\n",
      "Epoch 234/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 750us/step - loss: 1.5762 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00234: loss improved from 1.57638 to 1.57616, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 235/2000\n",
      "108/108 [==============================] - 0s 734us/step - loss: 1.5767 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00235: loss did not improve from 1.57616\n",
      "Epoch 236/2000\n",
      "108/108 [==============================] - 0s 734us/step - loss: 1.5773 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00236: loss did not improve from 1.57616\n",
      "Epoch 237/2000\n",
      "108/108 [==============================] - 0s 739us/step - loss: 1.5769 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00237: loss did not improve from 1.57616\n",
      "Epoch 238/2000\n",
      "108/108 [==============================] - 0s 744us/step - loss: 1.5773 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00238: loss did not improve from 1.57616\n",
      "Epoch 239/2000\n",
      "108/108 [==============================] - 0s 731us/step - loss: 1.5759 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00239: loss improved from 1.57616 to 1.57593, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 240/2000\n",
      "108/108 [==============================] - 0s 717us/step - loss: 1.5764 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00240: loss did not improve from 1.57593\n",
      "Epoch 241/2000\n",
      "108/108 [==============================] - 0s 738us/step - loss: 1.5761 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00241: loss did not improve from 1.57593\n",
      "Epoch 242/2000\n",
      "108/108 [==============================] - 0s 741us/step - loss: 1.5756 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00242: loss improved from 1.57593 to 1.57560, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 243/2000\n",
      "108/108 [==============================] - 0s 739us/step - loss: 1.5754 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00243: loss improved from 1.57560 to 1.57537, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 244/2000\n",
      "108/108 [==============================] - 0s 724us/step - loss: 1.5755 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00244: loss did not improve from 1.57537\n",
      "Epoch 245/2000\n",
      "108/108 [==============================] - 0s 735us/step - loss: 1.5754 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00245: loss improved from 1.57537 to 1.57535, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 246/2000\n",
      "108/108 [==============================] - 0s 724us/step - loss: 1.5770 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00246: loss did not improve from 1.57535\n",
      "Epoch 247/2000\n",
      "108/108 [==============================] - 0s 752us/step - loss: 1.5756 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00247: loss did not improve from 1.57535\n",
      "Epoch 248/2000\n",
      "108/108 [==============================] - 0s 723us/step - loss: 1.5768 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00248: loss did not improve from 1.57535\n",
      "Epoch 249/2000\n",
      "108/108 [==============================] - 0s 729us/step - loss: 1.5753 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00249: loss improved from 1.57535 to 1.57526, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 250/2000\n",
      "108/108 [==============================] - 0s 720us/step - loss: 1.5759 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00250: loss did not improve from 1.57526\n",
      "Epoch 251/2000\n",
      "108/108 [==============================] - 0s 726us/step - loss: 1.5755 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00251: loss did not improve from 1.57526\n",
      "Epoch 252/2000\n",
      "108/108 [==============================] - 0s 736us/step - loss: 1.5750 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00252: loss improved from 1.57526 to 1.57497, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 253/2000\n",
      "108/108 [==============================] - 0s 757us/step - loss: 1.5754 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00253: loss did not improve from 1.57497\n",
      "Epoch 254/2000\n",
      "108/108 [==============================] - 0s 731us/step - loss: 1.5745 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00254: loss improved from 1.57497 to 1.57449, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 255/2000\n",
      "108/108 [==============================] - 0s 743us/step - loss: 1.5744 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00255: loss improved from 1.57449 to 1.57442, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 256/2000\n",
      "108/108 [==============================] - 0s 715us/step - loss: 1.5737 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00256: loss improved from 1.57442 to 1.57366, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 257/2000\n",
      "108/108 [==============================] - 0s 723us/step - loss: 1.5745 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00257: loss did not improve from 1.57366\n",
      "Epoch 258/2000\n",
      "108/108 [==============================] - 0s 736us/step - loss: 1.5749 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00258: loss did not improve from 1.57366\n",
      "Epoch 259/2000\n",
      "108/108 [==============================] - 0s 708us/step - loss: 1.5741 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00259: loss did not improve from 1.57366\n",
      "Epoch 260/2000\n",
      "108/108 [==============================] - 0s 692us/step - loss: 1.5748 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00260: loss did not improve from 1.57366\n",
      "Epoch 261/2000\n",
      "108/108 [==============================] - 0s 702us/step - loss: 1.5745 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00261: loss did not improve from 1.57366\n",
      "Epoch 262/2000\n",
      "108/108 [==============================] - 0s 704us/step - loss: 1.5749 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00262: loss did not improve from 1.57366\n",
      "Epoch 263/2000\n",
      "108/108 [==============================] - 0s 700us/step - loss: 1.5754 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00263: loss did not improve from 1.57366\n",
      "Epoch 264/2000\n",
      "108/108 [==============================] - 0s 708us/step - loss: 1.5747 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00264: loss did not improve from 1.57366\n",
      "Epoch 265/2000\n",
      "108/108 [==============================] - 0s 675us/step - loss: 1.5758 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00265: loss did not improve from 1.57366\n",
      "Epoch 266/2000\n",
      "108/108 [==============================] - 0s 690us/step - loss: 1.5748 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00266: loss did not improve from 1.57366\n",
      "Epoch 267/2000\n",
      "108/108 [==============================] - 0s 702us/step - loss: 1.5736 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00267: loss improved from 1.57366 to 1.57362, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 268/2000\n",
      "108/108 [==============================] - 0s 700us/step - loss: 1.5745 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00268: loss did not improve from 1.57362\n",
      "Epoch 269/2000\n",
      "108/108 [==============================] - 0s 723us/step - loss: 1.5751 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00269: loss did not improve from 1.57362\n",
      "Epoch 270/2000\n",
      "108/108 [==============================] - 0s 688us/step - loss: 1.5727 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00270: loss improved from 1.57362 to 1.57267, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 271/2000\n",
      "108/108 [==============================] - 0s 687us/step - loss: 1.5855 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00271: loss did not improve from 1.57267\n",
      "Epoch 272/2000\n",
      "108/108 [==============================] - 0s 711us/step - loss: 1.5749 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00272: loss did not improve from 1.57267\n",
      "Epoch 273/2000\n",
      "108/108 [==============================] - 0s 718us/step - loss: 1.5741 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00273: loss did not improve from 1.57267\n",
      "Epoch 274/2000\n",
      "108/108 [==============================] - 0s 714us/step - loss: 1.5734 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00274: loss did not improve from 1.57267\n",
      "Epoch 275/2000\n",
      "108/108 [==============================] - 0s 712us/step - loss: 1.5725 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00275: loss improved from 1.57267 to 1.57251, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 276/2000\n",
      "108/108 [==============================] - 0s 700us/step - loss: 1.5789 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00276: loss did not improve from 1.57251\n",
      "Epoch 277/2000\n",
      "108/108 [==============================] - 0s 704us/step - loss: 1.5733 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00277: loss did not improve from 1.57251\n",
      "Epoch 278/2000\n",
      "108/108 [==============================] - 0s 716us/step - loss: 1.5723 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00278: loss improved from 1.57251 to 1.57231, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 279/2000\n",
      "108/108 [==============================] - 0s 688us/step - loss: 1.5732 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00279: loss did not improve from 1.57231\n",
      "Epoch 280/2000\n",
      "108/108 [==============================] - 0s 688us/step - loss: 1.5728 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00280: loss did not improve from 1.57231\n",
      "Epoch 281/2000\n",
      "108/108 [==============================] - 0s 709us/step - loss: 1.5727 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00281: loss did not improve from 1.57231\n",
      "Epoch 282/2000\n",
      "108/108 [==============================] - 0s 702us/step - loss: 1.5737 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00282: loss did not improve from 1.57231\n",
      "Epoch 283/2000\n",
      "108/108 [==============================] - 0s 698us/step - loss: 1.5735 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00283: loss did not improve from 1.57231\n",
      "Epoch 284/2000\n",
      "108/108 [==============================] - 0s 708us/step - loss: 1.5743 - accuracy: 0.4352\n",
      "\n",
      "Epoch 00284: loss did not improve from 1.57231\n",
      "Epoch 285/2000\n",
      "108/108 [==============================] - 0s 674us/step - loss: 1.5729 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00285: loss did not improve from 1.57231\n",
      "Epoch 286/2000\n",
      "108/108 [==============================] - 0s 738us/step - loss: 1.5725 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00286: loss did not improve from 1.57231\n",
      "Epoch 287/2000\n",
      "108/108 [==============================] - 0s 705us/step - loss: 1.5724 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00287: loss did not improve from 1.57231\n",
      "Epoch 288/2000\n",
      "108/108 [==============================] - 0s 704us/step - loss: 1.5727 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00288: loss did not improve from 1.57231\n",
      "Epoch 289/2000\n",
      "108/108 [==============================] - 0s 710us/step - loss: 1.5722 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00289: loss improved from 1.57231 to 1.57217, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 290/2000\n",
      "108/108 [==============================] - 0s 697us/step - loss: 1.5723 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00290: loss did not improve from 1.57217\n",
      "Epoch 291/2000\n",
      "108/108 [==============================] - 0s 699us/step - loss: 1.5729 - accuracy: 0.3704\n",
      "\n",
      "Epoch 00291: loss did not improve from 1.57217\n",
      "Epoch 292/2000\n",
      "108/108 [==============================] - 0s 701us/step - loss: 1.5716 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00292: loss improved from 1.57217 to 1.57158, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 293/2000\n",
      "108/108 [==============================] - 0s 739us/step - loss: 1.5739 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00293: loss did not improve from 1.57158\n",
      "Epoch 294/2000\n",
      "108/108 [==============================] - 0s 749us/step - loss: 1.5736 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00294: loss did not improve from 1.57158\n",
      "Epoch 295/2000\n",
      "108/108 [==============================] - 0s 736us/step - loss: 1.5731 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00295: loss did not improve from 1.57158\n",
      "Epoch 296/2000\n",
      "108/108 [==============================] - 0s 737us/step - loss: 1.5725 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00296: loss did not improve from 1.57158\n",
      "Epoch 297/2000\n",
      "108/108 [==============================] - 0s 736us/step - loss: 1.5722 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00297: loss did not improve from 1.57158\n",
      "Epoch 298/2000\n",
      "108/108 [==============================] - 0s 760us/step - loss: 1.5713 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00298: loss improved from 1.57158 to 1.57128, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 299/2000\n",
      "108/108 [==============================] - 0s 750us/step - loss: 1.5717 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00299: loss did not improve from 1.57128\n",
      "Epoch 300/2000\n",
      "108/108 [==============================] - 0s 735us/step - loss: 1.5721 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00300: loss did not improve from 1.57128\n",
      "Epoch 301/2000\n",
      "108/108 [==============================] - 0s 740us/step - loss: 1.5716 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00301: loss did not improve from 1.57128\n",
      "Epoch 302/2000\n",
      "108/108 [==============================] - 0s 726us/step - loss: 1.5723 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00302: loss did not improve from 1.57128\n",
      "Epoch 303/2000\n",
      "108/108 [==============================] - 0s 735us/step - loss: 1.5721 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00303: loss did not improve from 1.57128\n",
      "Epoch 304/2000\n",
      "108/108 [==============================] - 0s 744us/step - loss: 1.5737 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00304: loss did not improve from 1.57128\n",
      "Epoch 305/2000\n",
      "108/108 [==============================] - 0s 735us/step - loss: 1.5720 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00305: loss did not improve from 1.57128\n",
      "Epoch 306/2000\n",
      "108/108 [==============================] - 0s 736us/step - loss: 1.5717 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00306: loss did not improve from 1.57128\n",
      "Epoch 307/2000\n",
      "108/108 [==============================] - 0s 742us/step - loss: 1.5712 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00307: loss improved from 1.57128 to 1.57116, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 308/2000\n",
      "108/108 [==============================] - 0s 760us/step - loss: 1.5715 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00308: loss did not improve from 1.57116\n",
      "Epoch 309/2000\n",
      "108/108 [==============================] - 0s 737us/step - loss: 1.5711 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00309: loss improved from 1.57116 to 1.57106, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 310/2000\n",
      "108/108 [==============================] - 0s 749us/step - loss: 1.5713 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00310: loss did not improve from 1.57106\n",
      "Epoch 311/2000\n",
      "108/108 [==============================] - 0s 734us/step - loss: 1.5715 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00311: loss did not improve from 1.57106\n",
      "Epoch 312/2000\n",
      "108/108 [==============================] - 0s 737us/step - loss: 1.5714 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00312: loss did not improve from 1.57106\n",
      "Epoch 313/2000\n",
      "108/108 [==============================] - 0s 718us/step - loss: 1.5720 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00313: loss did not improve from 1.57106\n",
      "Epoch 314/2000\n",
      "108/108 [==============================] - 0s 735us/step - loss: 1.5708 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00314: loss improved from 1.57106 to 1.57080, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 315/2000\n",
      "108/108 [==============================] - 0s 718us/step - loss: 1.5713 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00315: loss did not improve from 1.57080\n",
      "Epoch 316/2000\n",
      "108/108 [==============================] - 0s 735us/step - loss: 1.5714 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00316: loss did not improve from 1.57080\n",
      "Epoch 317/2000\n",
      "108/108 [==============================] - 0s 710us/step - loss: 1.5711 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00317: loss did not improve from 1.57080\n",
      "Epoch 318/2000\n",
      "108/108 [==============================] - 0s 767us/step - loss: 1.5722 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00318: loss did not improve from 1.57080\n",
      "Epoch 319/2000\n",
      "108/108 [==============================] - 0s 729us/step - loss: 1.5716 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00319: loss did not improve from 1.57080\n",
      "Epoch 320/2000\n",
      "108/108 [==============================] - 0s 750us/step - loss: 1.5709 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00320: loss did not improve from 1.57080\n",
      "Epoch 321/2000\n",
      "108/108 [==============================] - 0s 741us/step - loss: 1.5772 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00321: loss did not improve from 1.57080\n",
      "Epoch 322/2000\n",
      "108/108 [==============================] - 0s 739us/step - loss: 1.5779 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00322: loss did not improve from 1.57080\n",
      "Epoch 323/2000\n",
      "108/108 [==============================] - 0s 741us/step - loss: 1.5729 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00323: loss did not improve from 1.57080\n",
      "Epoch 324/2000\n",
      "108/108 [==============================] - 0s 743us/step - loss: 1.5718 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00324: loss did not improve from 1.57080\n",
      "Epoch 325/2000\n",
      "108/108 [==============================] - 0s 724us/step - loss: 1.5708 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00325: loss improved from 1.57080 to 1.57080, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 326/2000\n",
      "108/108 [==============================] - 0s 755us/step - loss: 1.5791 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00326: loss did not improve from 1.57080\n",
      "Epoch 327/2000\n",
      "108/108 [==============================] - 0s 730us/step - loss: 1.5762 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00327: loss did not improve from 1.57080\n",
      "Epoch 328/2000\n",
      "108/108 [==============================] - 0s 731us/step - loss: 1.5723 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00328: loss did not improve from 1.57080\n",
      "Epoch 329/2000\n",
      "108/108 [==============================] - 0s 739us/step - loss: 1.5719 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00329: loss did not improve from 1.57080\n",
      "Epoch 330/2000\n",
      "108/108 [==============================] - 0s 735us/step - loss: 1.5719 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00330: loss did not improve from 1.57080\n",
      "Epoch 331/2000\n",
      "108/108 [==============================] - 0s 764us/step - loss: 1.5712 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00331: loss did not improve from 1.57080\n",
      "Epoch 332/2000\n",
      "108/108 [==============================] - 0s 729us/step - loss: 1.5715 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00332: loss did not improve from 1.57080\n",
      "Epoch 333/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 745us/step - loss: 1.5708 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00333: loss did not improve from 1.57080\n",
      "Epoch 334/2000\n",
      "108/108 [==============================] - 0s 739us/step - loss: 1.5728 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00334: loss did not improve from 1.57080\n",
      "Epoch 335/2000\n",
      "108/108 [==============================] - 0s 742us/step - loss: 1.5707 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00335: loss improved from 1.57080 to 1.57072, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 336/2000\n",
      "108/108 [==============================] - 0s 755us/step - loss: 1.5717 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00336: loss did not improve from 1.57072\n",
      "Epoch 337/2000\n",
      "108/108 [==============================] - 0s 722us/step - loss: 1.5702 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00337: loss improved from 1.57072 to 1.57023, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 338/2000\n",
      "108/108 [==============================] - 0s 730us/step - loss: 1.5703 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00338: loss did not improve from 1.57023\n",
      "Epoch 339/2000\n",
      "108/108 [==============================] - 0s 738us/step - loss: 1.5706 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00339: loss did not improve from 1.57023\n",
      "Epoch 340/2000\n",
      "108/108 [==============================] - 0s 742us/step - loss: 1.5709 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00340: loss did not improve from 1.57023\n",
      "Epoch 341/2000\n",
      "108/108 [==============================] - 0s 731us/step - loss: 1.5775 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00341: loss did not improve from 1.57023\n",
      "Epoch 342/2000\n",
      "108/108 [==============================] - 0s 714us/step - loss: 1.5710 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00342: loss did not improve from 1.57023\n",
      "Epoch 343/2000\n",
      "108/108 [==============================] - 0s 726us/step - loss: 1.5714 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00343: loss did not improve from 1.57023\n",
      "Epoch 344/2000\n",
      "108/108 [==============================] - 0s 740us/step - loss: 1.5728 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00344: loss did not improve from 1.57023\n",
      "Epoch 345/2000\n",
      "108/108 [==============================] - 0s 739us/step - loss: 1.5721 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00345: loss did not improve from 1.57023\n",
      "Epoch 346/2000\n",
      "108/108 [==============================] - 0s 750us/step - loss: 1.5706 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00346: loss did not improve from 1.57023\n",
      "Epoch 347/2000\n",
      "108/108 [==============================] - 0s 741us/step - loss: 1.5700 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00347: loss improved from 1.57023 to 1.57005, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 348/2000\n",
      "108/108 [==============================] - 0s 733us/step - loss: 1.5698 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00348: loss improved from 1.57005 to 1.56976, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 349/2000\n",
      "108/108 [==============================] - 0s 724us/step - loss: 1.5702 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00349: loss did not improve from 1.56976\n",
      "Epoch 350/2000\n",
      "108/108 [==============================] - 0s 759us/step - loss: 1.5705 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00350: loss did not improve from 1.56976\n",
      "Epoch 351/2000\n",
      "108/108 [==============================] - 0s 706us/step - loss: 1.5709 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00351: loss did not improve from 1.56976\n",
      "Epoch 352/2000\n",
      "108/108 [==============================] - 0s 732us/step - loss: 1.5703 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00352: loss did not improve from 1.56976\n",
      "Epoch 353/2000\n",
      "108/108 [==============================] - 0s 715us/step - loss: 1.5716 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00353: loss did not improve from 1.56976\n",
      "Epoch 354/2000\n",
      "108/108 [==============================] - 0s 740us/step - loss: 1.5700 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00354: loss did not improve from 1.56976\n",
      "Epoch 355/2000\n",
      "108/108 [==============================] - 0s 742us/step - loss: 1.5698 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00355: loss did not improve from 1.56976\n",
      "Epoch 356/2000\n",
      "108/108 [==============================] - 0s 748us/step - loss: 1.5698 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00356: loss improved from 1.56976 to 1.56975, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 357/2000\n",
      "108/108 [==============================] - 0s 759us/step - loss: 1.5695 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00357: loss improved from 1.56975 to 1.56945, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 358/2000\n",
      "108/108 [==============================] - 0s 746us/step - loss: 1.5692 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00358: loss improved from 1.56945 to 1.56922, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 359/2000\n",
      "108/108 [==============================] - 0s 724us/step - loss: 1.5713 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00359: loss did not improve from 1.56922\n",
      "Epoch 360/2000\n",
      "108/108 [==============================] - 0s 732us/step - loss: 1.5698 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00360: loss did not improve from 1.56922\n",
      "Epoch 361/2000\n",
      "108/108 [==============================] - 0s 749us/step - loss: 1.5694 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00361: loss did not improve from 1.56922\n",
      "Epoch 362/2000\n",
      "108/108 [==============================] - 0s 777us/step - loss: 1.6607 - accuracy: 0.3704\n",
      "\n",
      "Epoch 00362: loss did not improve from 1.56922\n",
      "Epoch 363/2000\n",
      "108/108 [==============================] - 0s 708us/step - loss: 2.0333 - accuracy: 0.3519\n",
      "\n",
      "Epoch 00363: loss did not improve from 1.56922\n",
      "Epoch 364/2000\n",
      "108/108 [==============================] - 0s 731us/step - loss: 1.5904 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00364: loss did not improve from 1.56922\n",
      "Epoch 365/2000\n",
      "108/108 [==============================] - 0s 724us/step - loss: 1.5745 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00365: loss did not improve from 1.56922\n",
      "Epoch 366/2000\n",
      "108/108 [==============================] - 0s 727us/step - loss: 1.5723 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00366: loss did not improve from 1.56922\n",
      "Epoch 367/2000\n",
      "108/108 [==============================] - 0s 745us/step - loss: 1.5706 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00367: loss did not improve from 1.56922\n",
      "Epoch 368/2000\n",
      "108/108 [==============================] - 0s 735us/step - loss: 1.5693 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00368: loss did not improve from 1.56922\n",
      "Epoch 369/2000\n",
      "108/108 [==============================] - 0s 763us/step - loss: 1.5697 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00369: loss did not improve from 1.56922\n",
      "Epoch 370/2000\n",
      "108/108 [==============================] - 0s 740us/step - loss: 1.5695 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00370: loss did not improve from 1.56922\n",
      "Epoch 371/2000\n",
      "108/108 [==============================] - 0s 747us/step - loss: 1.5692 - accuracy: 0.3704\n",
      "\n",
      "Epoch 00371: loss improved from 1.56922 to 1.56921, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 372/2000\n",
      "108/108 [==============================] - 0s 724us/step - loss: 1.5694 - accuracy: 0.3981TA: 0s - loss: 1.5831 - accuracy: 0.3977  \n",
      "\n",
      "Epoch 00372: loss did not improve from 1.56921\n",
      "Epoch 373/2000\n",
      "108/108 [==============================] - 0s 745us/step - loss: 1.5692 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00373: loss improved from 1.56921 to 1.56915, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 374/2000\n",
      "108/108 [==============================] - 0s 816us/step - loss: 1.5697 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00374: loss did not improve from 1.56915\n",
      "Epoch 375/2000\n",
      "108/108 [==============================] - 0s 836us/step - loss: 1.5692 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00375: loss did not improve from 1.56915\n",
      "Epoch 376/2000\n",
      "108/108 [==============================] - 0s 736us/step - loss: 1.5689 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00376: loss improved from 1.56915 to 1.56887, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 377/2000\n",
      "108/108 [==============================] - 0s 807us/step - loss: 1.5688 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00377: loss improved from 1.56887 to 1.56878, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 378/2000\n",
      "108/108 [==============================] - 0s 751us/step - loss: 1.5691 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00378: loss did not improve from 1.56878\n",
      "Epoch 379/2000\n",
      "108/108 [==============================] - 0s 781us/step - loss: 1.5690 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00379: loss did not improve from 1.56878\n",
      "Epoch 380/2000\n",
      "108/108 [==============================] - 0s 803us/step - loss: 1.5693 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00380: loss did not improve from 1.56878\n",
      "Epoch 381/2000\n",
      "108/108 [==============================] - 0s 758us/step - loss: 1.5689 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00381: loss did not improve from 1.56878\n",
      "Epoch 382/2000\n",
      "108/108 [==============================] - 0s 674us/step - loss: 1.5686 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00382: loss improved from 1.56878 to 1.56864, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 383/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5692 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00383: loss did not improve from 1.56864\n",
      "Epoch 384/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5689 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00384: loss did not improve from 1.56864\n",
      "Epoch 385/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5688 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00385: loss did not improve from 1.56864\n",
      "Epoch 386/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5686 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00386: loss improved from 1.56864 to 1.56863, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 387/2000\n",
      "108/108 [==============================] - 0s 662us/step - loss: 1.5691 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00387: loss did not improve from 1.56863\n",
      "Epoch 388/2000\n",
      "108/108 [==============================] - 0s 698us/step - loss: 1.5686 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00388: loss improved from 1.56863 to 1.56858, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 389/2000\n",
      "108/108 [==============================] - 0s 681us/step - loss: 1.5687 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00389: loss did not improve from 1.56858\n",
      "Epoch 390/2000\n",
      "108/108 [==============================] - 0s 734us/step - loss: 1.5687 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00390: loss did not improve from 1.56858\n",
      "Epoch 391/2000\n",
      "108/108 [==============================] - 0s 701us/step - loss: 1.5683 - accuracy: 0.4352\n",
      "\n",
      "Epoch 00391: loss improved from 1.56858 to 1.56825, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 392/2000\n",
      "108/108 [==============================] - 0s 716us/step - loss: 1.5687 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00392: loss did not improve from 1.56825\n",
      "Epoch 393/2000\n",
      "108/108 [==============================] - 0s 707us/step - loss: 1.5687 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00393: loss did not improve from 1.56825\n",
      "Epoch 394/2000\n",
      "108/108 [==============================] - 0s 726us/step - loss: 1.5687 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00394: loss did not improve from 1.56825\n",
      "Epoch 395/2000\n",
      "108/108 [==============================] - 0s 695us/step - loss: 1.5710 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00395: loss did not improve from 1.56825\n",
      "Epoch 396/2000\n",
      "108/108 [==============================] - 0s 697us/step - loss: 1.5703 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00396: loss did not improve from 1.56825\n",
      "Epoch 397/2000\n",
      "108/108 [==============================] - 0s 700us/step - loss: 1.5691 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00397: loss did not improve from 1.56825\n",
      "Epoch 398/2000\n",
      "108/108 [==============================] - 0s 705us/step - loss: 1.5682 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00398: loss improved from 1.56825 to 1.56824, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 399/2000\n",
      "108/108 [==============================] - 0s 737us/step - loss: 1.5689 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00399: loss did not improve from 1.56824\n",
      "Epoch 400/2000\n",
      "108/108 [==============================] - 0s 725us/step - loss: 1.5686 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00400: loss did not improve from 1.56824\n",
      "Epoch 401/2000\n",
      "108/108 [==============================] - 0s 710us/step - loss: 1.5685 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00401: loss did not improve from 1.56824\n",
      "Epoch 402/2000\n",
      "108/108 [==============================] - 0s 726us/step - loss: 1.5687 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00402: loss did not improve from 1.56824\n",
      "Epoch 403/2000\n",
      "108/108 [==============================] - 0s 708us/step - loss: 1.5689 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00403: loss did not improve from 1.56824\n",
      "Epoch 404/2000\n",
      "108/108 [==============================] - 0s 691us/step - loss: 1.5687 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00404: loss did not improve from 1.56824\n",
      "Epoch 405/2000\n",
      "108/108 [==============================] - 0s 708us/step - loss: 1.5684 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00405: loss did not improve from 1.56824\n",
      "Epoch 406/2000\n",
      "108/108 [==============================] - 0s 726us/step - loss: 1.5687 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00406: loss did not improve from 1.56824\n",
      "Epoch 407/2000\n",
      "108/108 [==============================] - 0s 701us/step - loss: 1.5682 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00407: loss improved from 1.56824 to 1.56824, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 408/2000\n",
      "108/108 [==============================] - 0s 705us/step - loss: 1.5679 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00408: loss improved from 1.56824 to 1.56792, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 409/2000\n",
      "108/108 [==============================] - 0s 728us/step - loss: 1.5684 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00409: loss did not improve from 1.56792\n",
      "Epoch 410/2000\n",
      "108/108 [==============================] - 0s 747us/step - loss: 1.5680 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00410: loss did not improve from 1.56792\n",
      "Epoch 411/2000\n",
      "108/108 [==============================] - 0s 721us/step - loss: 1.5680 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00411: loss did not improve from 1.56792\n",
      "Epoch 412/2000\n",
      "108/108 [==============================] - 0s 736us/step - loss: 1.5683 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00412: loss did not improve from 1.56792\n",
      "Epoch 413/2000\n",
      "108/108 [==============================] - 0s 739us/step - loss: 1.5701 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00413: loss did not improve from 1.56792\n",
      "Epoch 414/2000\n",
      "108/108 [==============================] - 0s 739us/step - loss: 1.5692 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00414: loss did not improve from 1.56792\n",
      "Epoch 415/2000\n",
      "108/108 [==============================] - 0s 734us/step - loss: 1.5683 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00415: loss did not improve from 1.56792\n",
      "Epoch 416/2000\n",
      "108/108 [==============================] - 0s 727us/step - loss: 1.5688 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00416: loss did not improve from 1.56792\n",
      "Epoch 417/2000\n",
      "108/108 [==============================] - 0s 751us/step - loss: 1.5689 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00417: loss did not improve from 1.56792\n",
      "Epoch 418/2000\n",
      "108/108 [==============================] - 0s 742us/step - loss: 1.5682 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00418: loss did not improve from 1.56792\n",
      "Epoch 419/2000\n",
      "108/108 [==============================] - 0s 734us/step - loss: 1.5683 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00419: loss did not improve from 1.56792\n",
      "Epoch 420/2000\n",
      "108/108 [==============================] - 0s 776us/step - loss: 1.5685 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00420: loss did not improve from 1.56792\n",
      "Epoch 421/2000\n",
      "108/108 [==============================] - 0s 699us/step - loss: 1.5679 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00421: loss improved from 1.56792 to 1.56787, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 422/2000\n",
      "108/108 [==============================] - 0s 727us/step - loss: 1.5684 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00422: loss did not improve from 1.56787\n",
      "Epoch 423/2000\n",
      "108/108 [==============================] - 0s 729us/step - loss: 1.5682 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00423: loss did not improve from 1.56787\n",
      "Epoch 424/2000\n",
      "108/108 [==============================] - 0s 755us/step - loss: 1.5695 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00424: loss did not improve from 1.56787\n",
      "Epoch 425/2000\n",
      "108/108 [==============================] - 0s 743us/step - loss: 1.5687 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00425: loss did not improve from 1.56787\n",
      "Epoch 426/2000\n",
      "108/108 [==============================] - 0s 730us/step - loss: 1.5684 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00426: loss did not improve from 1.56787\n",
      "Epoch 427/2000\n",
      "108/108 [==============================] - 0s 754us/step - loss: 1.5679 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00427: loss did not improve from 1.56787\n",
      "Epoch 428/2000\n",
      "108/108 [==============================] - 0s 740us/step - loss: 1.5677 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00428: loss improved from 1.56787 to 1.56774, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 429/2000\n",
      "108/108 [==============================] - 0s 731us/step - loss: 1.5676 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00429: loss improved from 1.56774 to 1.56760, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 430/2000\n",
      "108/108 [==============================] - 0s 729us/step - loss: 1.5674 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00430: loss improved from 1.56760 to 1.56745, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 431/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 739us/step - loss: 1.5681 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00431: loss did not improve from 1.56745\n",
      "Epoch 432/2000\n",
      "108/108 [==============================] - 0s 738us/step - loss: 1.5683 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00432: loss did not improve from 1.56745\n",
      "Epoch 433/2000\n",
      "108/108 [==============================] - 0s 735us/step - loss: 1.5682 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00433: loss did not improve from 1.56745\n",
      "Epoch 434/2000\n",
      "108/108 [==============================] - 0s 738us/step - loss: 1.5678 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00434: loss did not improve from 1.56745\n",
      "Epoch 435/2000\n",
      "108/108 [==============================] - 0s 722us/step - loss: 1.5677 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00435: loss did not improve from 1.56745\n",
      "Epoch 436/2000\n",
      "108/108 [==============================] - 0s 741us/step - loss: 1.5682 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00436: loss did not improve from 1.56745\n",
      "Epoch 437/2000\n",
      "108/108 [==============================] - 0s 740us/step - loss: 1.5677 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00437: loss did not improve from 1.56745\n",
      "Epoch 438/2000\n",
      "108/108 [==============================] - 0s 738us/step - loss: 1.5677 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00438: loss did not improve from 1.56745\n",
      "Epoch 439/2000\n",
      "108/108 [==============================] - 0s 744us/step - loss: 1.5673 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00439: loss improved from 1.56745 to 1.56734, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 440/2000\n",
      "108/108 [==============================] - 0s 715us/step - loss: 1.5679 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00440: loss did not improve from 1.56734\n",
      "Epoch 441/2000\n",
      "108/108 [==============================] - 0s 720us/step - loss: 1.5681 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00441: loss did not improve from 1.56734\n",
      "Epoch 442/2000\n",
      "108/108 [==============================] - 0s 733us/step - loss: 1.5678 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00442: loss did not improve from 1.56734\n",
      "Epoch 443/2000\n",
      "108/108 [==============================] - 0s 732us/step - loss: 1.5675 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00443: loss did not improve from 1.56734\n",
      "Epoch 444/2000\n",
      "108/108 [==============================] - 0s 736us/step - loss: 1.5674 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00444: loss did not improve from 1.56734\n",
      "Epoch 445/2000\n",
      "108/108 [==============================] - 0s 775us/step - loss: 1.5682 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00445: loss did not improve from 1.56734\n",
      "Epoch 446/2000\n",
      "108/108 [==============================] - 0s 716us/step - loss: 1.5678 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00446: loss did not improve from 1.56734\n",
      "Epoch 447/2000\n",
      "108/108 [==============================] - 0s 732us/step - loss: 1.5674 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00447: loss did not improve from 1.56734\n",
      "Epoch 448/2000\n",
      "108/108 [==============================] - 0s 744us/step - loss: 1.5679 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00448: loss did not improve from 1.56734\n",
      "Epoch 449/2000\n",
      "108/108 [==============================] - 0s 738us/step - loss: 1.5678 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00449: loss did not improve from 1.56734\n",
      "Epoch 450/2000\n",
      "108/108 [==============================] - 0s 743us/step - loss: 1.5675 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00450: loss did not improve from 1.56734\n",
      "Epoch 451/2000\n",
      "108/108 [==============================] - 0s 741us/step - loss: 1.5678 - accuracy: 0.4352\n",
      "\n",
      "Epoch 00451: loss did not improve from 1.56734\n",
      "Epoch 452/2000\n",
      "108/108 [==============================] - 0s 754us/step - loss: 1.5674 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00452: loss did not improve from 1.56734\n",
      "Epoch 453/2000\n",
      "108/108 [==============================] - 0s 729us/step - loss: 1.5673 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00453: loss improved from 1.56734 to 1.56730, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 454/2000\n",
      "108/108 [==============================] - 0s 734us/step - loss: 1.5674 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00454: loss did not improve from 1.56730\n",
      "Epoch 455/2000\n",
      "108/108 [==============================] - 0s 738us/step - loss: 1.5683 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00455: loss did not improve from 1.56730\n",
      "Epoch 456/2000\n",
      "108/108 [==============================] - 0s 739us/step - loss: 1.5679 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00456: loss did not improve from 1.56730\n",
      "Epoch 457/2000\n",
      "108/108 [==============================] - 0s 743us/step - loss: 1.5722 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00457: loss did not improve from 1.56730\n",
      "Epoch 458/2000\n",
      "108/108 [==============================] - 0s 787us/step - loss: 1.5728 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00458: loss did not improve from 1.56730\n",
      "Epoch 459/2000\n",
      "108/108 [==============================] - 0s 721us/step - loss: 1.5725 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00459: loss did not improve from 1.56730\n",
      "Epoch 460/2000\n",
      "108/108 [==============================] - 0s 731us/step - loss: 1.5692 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00460: loss did not improve from 1.56730\n",
      "Epoch 461/2000\n",
      "108/108 [==============================] - 0s 738us/step - loss: 1.5677 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00461: loss did not improve from 1.56730\n",
      "Epoch 462/2000\n",
      "108/108 [==============================] - 0s 743us/step - loss: 1.5670 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00462: loss improved from 1.56730 to 1.56703, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 463/2000\n",
      "108/108 [==============================] - 0s 728us/step - loss: 1.5673 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00463: loss did not improve from 1.56703\n",
      "Epoch 464/2000\n",
      "108/108 [==============================] - 0s 752us/step - loss: 1.5666 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00464: loss improved from 1.56703 to 1.56664, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 465/2000\n",
      "108/108 [==============================] - 0s 758us/step - loss: 1.5669 - accuracy: 0.4352\n",
      "\n",
      "Epoch 00465: loss did not improve from 1.56664\n",
      "Epoch 466/2000\n",
      "108/108 [==============================] - 0s 746us/step - loss: 1.5670 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00466: loss did not improve from 1.56664\n",
      "Epoch 467/2000\n",
      "108/108 [==============================] - 0s 724us/step - loss: 1.5669 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00467: loss did not improve from 1.56664\n",
      "Epoch 468/2000\n",
      "108/108 [==============================] - 0s 737us/step - loss: 1.5663 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00468: loss improved from 1.56664 to 1.56630, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 469/2000\n",
      "108/108 [==============================] - 0s 733us/step - loss: 1.5669 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00469: loss did not improve from 1.56630\n",
      "Epoch 470/2000\n",
      "108/108 [==============================] - 0s 739us/step - loss: 1.5674 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00470: loss did not improve from 1.56630\n",
      "Epoch 471/2000\n",
      "108/108 [==============================] - 0s 738us/step - loss: 1.5672 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00471: loss did not improve from 1.56630\n",
      "Epoch 472/2000\n",
      "108/108 [==============================] - 0s 735us/step - loss: 1.5668 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00472: loss did not improve from 1.56630\n",
      "Epoch 473/2000\n",
      "108/108 [==============================] - 0s 728us/step - loss: 1.5669 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00473: loss did not improve from 1.56630\n",
      "Epoch 474/2000\n",
      "108/108 [==============================] - 0s 754us/step - loss: 1.5671 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00474: loss did not improve from 1.56630\n",
      "Epoch 475/2000\n",
      "108/108 [==============================] - 0s 748us/step - loss: 1.5671 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00475: loss did not improve from 1.56630\n",
      "Epoch 476/2000\n",
      "108/108 [==============================] - 0s 742us/step - loss: 1.5666 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00476: loss did not improve from 1.56630\n",
      "Epoch 477/2000\n",
      "108/108 [==============================] - 0s 768us/step - loss: 1.5666 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00477: loss did not improve from 1.56630\n",
      "Epoch 478/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5670 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00478: loss did not improve from 1.56630\n",
      "Epoch 479/2000\n",
      "108/108 [==============================] - 0s 849us/step - loss: 1.5668 - accuracy: 0.4352\n",
      "\n",
      "Epoch 00479: loss did not improve from 1.56630\n",
      "Epoch 480/2000\n",
      "108/108 [==============================] - 0s 738us/step - loss: 1.5674 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00480: loss did not improve from 1.56630\n",
      "Epoch 481/2000\n",
      "108/108 [==============================] - 0s 746us/step - loss: 1.5669 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00481: loss did not improve from 1.56630\n",
      "Epoch 482/2000\n",
      "108/108 [==============================] - 0s 746us/step - loss: 1.5671 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00482: loss did not improve from 1.56630\n",
      "Epoch 483/2000\n",
      "108/108 [==============================] - 0s 735us/step - loss: 1.5662 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00483: loss improved from 1.56630 to 1.56618, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 484/2000\n",
      "108/108 [==============================] - 0s 757us/step - loss: 1.5671 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00484: loss did not improve from 1.56618\n",
      "Epoch 485/2000\n",
      "108/108 [==============================] - 0s 739us/step - loss: 1.5667 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00485: loss did not improve from 1.56618\n",
      "Epoch 486/2000\n",
      "108/108 [==============================] - 0s 747us/step - loss: 1.5663 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00486: loss did not improve from 1.56618\n",
      "Epoch 487/2000\n",
      "108/108 [==============================] - 0s 741us/step - loss: 1.5664 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00487: loss did not improve from 1.56618\n",
      "Epoch 488/2000\n",
      "108/108 [==============================] - 0s 721us/step - loss: 1.5666 - accuracy: 0.3704\n",
      "\n",
      "Epoch 00488: loss did not improve from 1.56618\n",
      "Epoch 489/2000\n",
      "108/108 [==============================] - 0s 731us/step - loss: 1.5668 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00489: loss did not improve from 1.56618\n",
      "Epoch 490/2000\n",
      "108/108 [==============================] - 0s 765us/step - loss: 1.5668 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00490: loss did not improve from 1.56618\n",
      "Epoch 491/2000\n",
      "108/108 [==============================] - 0s 702us/step - loss: 1.5666 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00491: loss did not improve from 1.56618\n",
      "Epoch 492/2000\n",
      "108/108 [==============================] - 0s 735us/step - loss: 1.5697 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00492: loss did not improve from 1.56618\n",
      "Epoch 493/2000\n",
      "108/108 [==============================] - 0s 725us/step - loss: 1.5698 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00493: loss did not improve from 1.56618\n",
      "Epoch 494/2000\n",
      "108/108 [==============================] - 0s 744us/step - loss: 1.5705 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00494: loss did not improve from 1.56618\n",
      "Epoch 495/2000\n",
      "108/108 [==============================] - 0s 736us/step - loss: 1.5707 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00495: loss did not improve from 1.56618\n",
      "Epoch 496/2000\n",
      "108/108 [==============================] - 0s 745us/step - loss: 1.5679 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00496: loss did not improve from 1.56618\n",
      "Epoch 497/2000\n",
      "108/108 [==============================] - 0s 744us/step - loss: 1.5673 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00497: loss did not improve from 1.56618\n",
      "Epoch 498/2000\n",
      "108/108 [==============================] - 0s 746us/step - loss: 1.5665 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00498: loss did not improve from 1.56618\n",
      "Epoch 499/2000\n",
      "108/108 [==============================] - 0s 725us/step - loss: 1.5711 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00499: loss did not improve from 1.56618\n",
      "Epoch 500/2000\n",
      "108/108 [==============================] - 0s 743us/step - loss: 1.5691 - accuracy: 0.3704\n",
      "\n",
      "Epoch 00500: loss did not improve from 1.56618\n",
      "Epoch 501/2000\n",
      "108/108 [==============================] - 0s 740us/step - loss: 1.5671 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00501: loss did not improve from 1.56618\n",
      "Epoch 502/2000\n",
      "108/108 [==============================] - 0s 725us/step - loss: 1.5659 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00502: loss improved from 1.56618 to 1.56592, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 503/2000\n",
      "108/108 [==============================] - 0s 740us/step - loss: 1.5665 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00503: loss did not improve from 1.56592\n",
      "Epoch 504/2000\n",
      "108/108 [==============================] - 0s 736us/step - loss: 1.5665 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00504: loss did not improve from 1.56592\n",
      "Epoch 505/2000\n",
      "108/108 [==============================] - 0s 721us/step - loss: 1.5665 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00505: loss did not improve from 1.56592\n",
      "Epoch 506/2000\n",
      "108/108 [==============================] - 0s 739us/step - loss: 1.5666 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00506: loss did not improve from 1.56592\n",
      "Epoch 507/2000\n",
      "108/108 [==============================] - 0s 722us/step - loss: 1.5665 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00507: loss did not improve from 1.56592\n",
      "Epoch 508/2000\n",
      "108/108 [==============================] - 0s 727us/step - loss: 1.5663 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00508: loss did not improve from 1.56592\n",
      "Epoch 509/2000\n",
      "108/108 [==============================] - 0s 740us/step - loss: 1.5662 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00509: loss did not improve from 1.56592\n",
      "Epoch 510/2000\n",
      "108/108 [==============================] - 0s 743us/step - loss: 1.5663 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00510: loss did not improve from 1.56592\n",
      "Epoch 511/2000\n",
      "108/108 [==============================] - 0s 753us/step - loss: 1.5665 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00511: loss did not improve from 1.56592\n",
      "Epoch 512/2000\n",
      "108/108 [==============================] - 0s 740us/step - loss: 1.5664 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00512: loss did not improve from 1.56592\n",
      "Epoch 513/2000\n",
      "108/108 [==============================] - 0s 750us/step - loss: 1.5663 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00513: loss did not improve from 1.56592\n",
      "Epoch 514/2000\n",
      "108/108 [==============================] - 0s 731us/step - loss: 1.5658 - accuracy: 0.3704\n",
      "\n",
      "Epoch 00514: loss improved from 1.56592 to 1.56584, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 515/2000\n",
      "108/108 [==============================] - 0s 721us/step - loss: 1.5662 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00515: loss did not improve from 1.56584\n",
      "Epoch 516/2000\n",
      "108/108 [==============================] - 0s 759us/step - loss: 1.5660 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00516: loss did not improve from 1.56584\n",
      "Epoch 517/2000\n",
      "108/108 [==============================] - 0s 746us/step - loss: 1.5660 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00517: loss did not improve from 1.56584\n",
      "Epoch 518/2000\n",
      "108/108 [==============================] - 0s 719us/step - loss: 1.5663 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00518: loss did not improve from 1.56584\n",
      "Epoch 519/2000\n",
      "108/108 [==============================] - 0s 736us/step - loss: 1.5660 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00519: loss did not improve from 1.56584\n",
      "Epoch 520/2000\n",
      "108/108 [==============================] - 0s 745us/step - loss: 1.5657 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00520: loss improved from 1.56584 to 1.56569, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 521/2000\n",
      "108/108 [==============================] - 0s 753us/step - loss: 1.5661 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00521: loss did not improve from 1.56569\n",
      "Epoch 522/2000\n",
      "108/108 [==============================] - 0s 752us/step - loss: 1.5657 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00522: loss did not improve from 1.56569\n",
      "Epoch 523/2000\n",
      "108/108 [==============================] - 0s 715us/step - loss: 1.5659 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00523: loss did not improve from 1.56569\n",
      "Epoch 524/2000\n",
      "108/108 [==============================] - 0s 722us/step - loss: 1.5659 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00524: loss did not improve from 1.56569\n",
      "Epoch 525/2000\n",
      "108/108 [==============================] - 0s 745us/step - loss: 1.5662 - accuracy: 0.4352\n",
      "\n",
      "Epoch 00525: loss did not improve from 1.56569\n",
      "Epoch 526/2000\n",
      "108/108 [==============================] - 0s 747us/step - loss: 1.5660 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00526: loss did not improve from 1.56569\n",
      "Epoch 527/2000\n",
      "108/108 [==============================] - 0s 747us/step - loss: 1.5665 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00527: loss did not improve from 1.56569\n",
      "Epoch 528/2000\n",
      "108/108 [==============================] - 0s 744us/step - loss: 1.5711 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00528: loss did not improve from 1.56569\n",
      "Epoch 529/2000\n",
      "108/108 [==============================] - 0s 736us/step - loss: 1.5671 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00529: loss did not improve from 1.56569\n",
      "Epoch 530/2000\n",
      "108/108 [==============================] - 0s 740us/step - loss: 1.5660 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00530: loss did not improve from 1.56569\n",
      "Epoch 531/2000\n",
      "108/108 [==============================] - 0s 734us/step - loss: 1.5656 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00531: loss improved from 1.56569 to 1.56564, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 532/2000\n",
      "108/108 [==============================] - 0s 766us/step - loss: 1.5658 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00532: loss did not improve from 1.56564\n",
      "Epoch 533/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 818us/step - loss: 1.5666 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00533: loss did not improve from 1.56564\n",
      "Epoch 534/2000\n",
      "108/108 [==============================] - 0s 687us/step - loss: 1.5684 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00534: loss did not improve from 1.56564\n",
      "Epoch 535/2000\n",
      "108/108 [==============================] - 0s 720us/step - loss: 1.5669 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00535: loss did not improve from 1.56564\n",
      "Epoch 536/2000\n",
      "108/108 [==============================] - 0s 670us/step - loss: 1.5661 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00536: loss did not improve from 1.56564\n",
      "Epoch 537/2000\n",
      "108/108 [==============================] - 0s 692us/step - loss: 1.5659 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00537: loss did not improve from 1.56564\n",
      "Epoch 538/2000\n",
      "108/108 [==============================] - 0s 698us/step - loss: 1.5661 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00538: loss did not improve from 1.56564\n",
      "Epoch 539/2000\n",
      "108/108 [==============================] - 0s 695us/step - loss: 1.5662 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00539: loss did not improve from 1.56564\n",
      "Epoch 540/2000\n",
      "108/108 [==============================] - 0s 705us/step - loss: 1.5673 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00540: loss did not improve from 1.56564\n",
      "Epoch 541/2000\n",
      "108/108 [==============================] - 0s 709us/step - loss: 1.5665 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00541: loss did not improve from 1.56564\n",
      "Epoch 542/2000\n",
      "108/108 [==============================] - 0s 713us/step - loss: 1.5661 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00542: loss did not improve from 1.56564\n",
      "Epoch 543/2000\n",
      "108/108 [==============================] - 0s 704us/step - loss: 1.5661 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00543: loss did not improve from 1.56564\n",
      "Epoch 544/2000\n",
      "108/108 [==============================] - 0s 717us/step - loss: 1.5659 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00544: loss did not improve from 1.56564\n",
      "Epoch 545/2000\n",
      "108/108 [==============================] - 0s 700us/step - loss: 1.5662 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00545: loss did not improve from 1.56564\n",
      "Epoch 546/2000\n",
      "108/108 [==============================] - 0s 700us/step - loss: 1.5659 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00546: loss did not improve from 1.56564\n",
      "Epoch 547/2000\n",
      "108/108 [==============================] - 0s 695us/step - loss: 1.5660 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00547: loss did not improve from 1.56564\n",
      "Epoch 548/2000\n",
      "108/108 [==============================] - 0s 710us/step - loss: 1.5655 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00548: loss improved from 1.56564 to 1.56546, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 549/2000\n",
      "108/108 [==============================] - 0s 729us/step - loss: 1.5661 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00549: loss did not improve from 1.56546\n",
      "Epoch 550/2000\n",
      "108/108 [==============================] - 0s 726us/step - loss: 1.5659 - accuracy: 0.3704\n",
      "\n",
      "Epoch 00550: loss did not improve from 1.56546\n",
      "Epoch 551/2000\n",
      "108/108 [==============================] - 0s 691us/step - loss: 1.5663 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00551: loss did not improve from 1.56546\n",
      "Epoch 552/2000\n",
      "108/108 [==============================] - 0s 690us/step - loss: 1.5656 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00552: loss did not improve from 1.56546\n",
      "Epoch 553/2000\n",
      "108/108 [==============================] - 0s 701us/step - loss: 1.5673 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00553: loss did not improve from 1.56546\n",
      "Epoch 554/2000\n",
      "108/108 [==============================] - 0s 740us/step - loss: 1.5668 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00554: loss did not improve from 1.56546\n",
      "Epoch 555/2000\n",
      "108/108 [==============================] - 0s 700us/step - loss: 1.5659 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00555: loss did not improve from 1.56546\n",
      "Epoch 556/2000\n",
      "108/108 [==============================] - 0s 729us/step - loss: 1.5660 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00556: loss did not improve from 1.56546\n",
      "Epoch 557/2000\n",
      "108/108 [==============================] - 0s 703us/step - loss: 1.5662 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00557: loss did not improve from 1.56546\n",
      "Epoch 558/2000\n",
      "108/108 [==============================] - 0s 694us/step - loss: 1.5665 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00558: loss did not improve from 1.56546\n",
      "Epoch 559/2000\n",
      "108/108 [==============================] - 0s 697us/step - loss: 1.5658 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00559: loss did not improve from 1.56546\n",
      "Epoch 560/2000\n",
      "108/108 [==============================] - 0s 705us/step - loss: 1.5663 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00560: loss did not improve from 1.56546\n",
      "Epoch 561/2000\n",
      "108/108 [==============================] - 0s 720us/step - loss: 1.5659 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00561: loss did not improve from 1.56546\n",
      "Epoch 562/2000\n",
      "108/108 [==============================] - 0s 702us/step - loss: 1.5654 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00562: loss improved from 1.56546 to 1.56542, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 563/2000\n",
      "108/108 [==============================] - 0s 720us/step - loss: 1.5659 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00563: loss did not improve from 1.56542\n",
      "Epoch 564/2000\n",
      "108/108 [==============================] - 0s 716us/step - loss: 1.5656 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00564: loss did not improve from 1.56542\n",
      "Epoch 565/2000\n",
      "108/108 [==============================] - 0s 720us/step - loss: 1.5654 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00565: loss did not improve from 1.56542\n",
      "Epoch 566/2000\n",
      "108/108 [==============================] - 0s 695us/step - loss: 1.5655 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00566: loss did not improve from 1.56542\n",
      "Epoch 567/2000\n",
      "108/108 [==============================] - 0s 706us/step - loss: 1.5657 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00567: loss did not improve from 1.56542\n",
      "Epoch 568/2000\n",
      "108/108 [==============================] - 0s 704us/step - loss: 1.5652 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00568: loss improved from 1.56542 to 1.56525, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 569/2000\n",
      "108/108 [==============================] - 0s 819us/step - loss: 1.5658 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00569: loss did not improve from 1.56525\n",
      "Epoch 570/2000\n",
      "108/108 [==============================] - 0s 827us/step - loss: 1.5655 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00570: loss did not improve from 1.56525\n",
      "Epoch 571/2000\n",
      "108/108 [==============================] - 0s 813us/step - loss: 1.5658 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00571: loss did not improve from 1.56525\n",
      "Epoch 572/2000\n",
      "108/108 [==============================] - 0s 850us/step - loss: 1.5655 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00572: loss did not improve from 1.56525\n",
      "Epoch 573/2000\n",
      "108/108 [==============================] - 0s 822us/step - loss: 1.5657 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00573: loss did not improve from 1.56525\n",
      "Epoch 574/2000\n",
      "108/108 [==============================] - 0s 775us/step - loss: 1.5677 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00574: loss did not improve from 1.56525\n",
      "Epoch 575/2000\n",
      "108/108 [==============================] - 0s 782us/step - loss: 1.5690 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00575: loss did not improve from 1.56525\n",
      "Epoch 576/2000\n",
      "108/108 [==============================] - 0s 763us/step - loss: 1.5681 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00576: loss did not improve from 1.56525\n",
      "Epoch 577/2000\n",
      "108/108 [==============================] - 0s 698us/step - loss: 1.5698 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00577: loss did not improve from 1.56525\n",
      "Epoch 578/2000\n",
      "108/108 [==============================] - 0s 629us/step - loss: 1.5673 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00578: loss did not improve from 1.56525\n",
      "Epoch 579/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5849 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00579: loss did not improve from 1.56525\n",
      "Epoch 580/2000\n",
      "108/108 [==============================] - 0s 614us/step - loss: 1.5692 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00580: loss did not improve from 1.56525\n",
      "Epoch 581/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5660 - accuracy: 0.4444\n",
      "\n",
      "Epoch 00581: loss did not improve from 1.56525\n",
      "Epoch 582/2000\n",
      "108/108 [==============================] - 0s 626us/step - loss: 1.5654 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00582: loss did not improve from 1.56525\n",
      "Epoch 583/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5658 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00583: loss did not improve from 1.56525\n",
      "Epoch 584/2000\n",
      "108/108 [==============================] - 0s 618us/step - loss: 1.5658 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00584: loss did not improve from 1.56525\n",
      "Epoch 585/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5650 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00585: loss improved from 1.56525 to 1.56505, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 586/2000\n",
      "108/108 [==============================] - 0s 648us/step - loss: 1.5654 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00586: loss did not improve from 1.56505\n",
      "Epoch 587/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5658 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00587: loss did not improve from 1.56505\n",
      "Epoch 588/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5656 - accuracy: 0.4352\n",
      "\n",
      "Epoch 00588: loss did not improve from 1.56505\n",
      "Epoch 589/2000\n",
      "108/108 [==============================] - 0s 661us/step - loss: 1.5656 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00589: loss did not improve from 1.56505\n",
      "Epoch 590/2000\n",
      "108/108 [==============================] - 0s 645us/step - loss: 1.5651 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00590: loss did not improve from 1.56505\n",
      "Epoch 591/2000\n",
      "108/108 [==============================] - 0s 645us/step - loss: 1.5656 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00591: loss did not improve from 1.56505\n",
      "Epoch 592/2000\n",
      "108/108 [==============================] - 0s 655us/step - loss: 1.5711 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00592: loss did not improve from 1.56505\n",
      "Epoch 593/2000\n",
      "108/108 [==============================] - 0s 661us/step - loss: 1.5696 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00593: loss did not improve from 1.56505\n",
      "Epoch 594/2000\n",
      "108/108 [==============================] - 0s 645us/step - loss: 1.5709 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00594: loss did not improve from 1.56505\n",
      "Epoch 595/2000\n",
      "108/108 [==============================] - 0s 655us/step - loss: 1.5662 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00595: loss did not improve from 1.56505\n",
      "Epoch 596/2000\n",
      "108/108 [==============================] - 0s 683us/step - loss: 1.5653 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00596: loss did not improve from 1.56505\n",
      "Epoch 597/2000\n",
      "108/108 [==============================] - 0s 652us/step - loss: 1.5655 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00597: loss did not improve from 1.56505\n",
      "Epoch 598/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5658 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00598: loss did not improve from 1.56505\n",
      "Epoch 599/2000\n",
      "108/108 [==============================] - 0s 661us/step - loss: 1.5655 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00599: loss did not improve from 1.56505\n",
      "Epoch 600/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5651 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00600: loss did not improve from 1.56505\n",
      "Epoch 601/2000\n",
      "108/108 [==============================] - 0s 655us/step - loss: 1.5654 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00601: loss did not improve from 1.56505\n",
      "Epoch 602/2000\n",
      "108/108 [==============================] - 0s 670us/step - loss: 1.5654 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00602: loss did not improve from 1.56505\n",
      "Epoch 603/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5653 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00603: loss did not improve from 1.56505\n",
      "Epoch 604/2000\n",
      "108/108 [==============================] - 0s 665us/step - loss: 1.5653 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00604: loss did not improve from 1.56505\n",
      "Epoch 605/2000\n",
      "108/108 [==============================] - 0s 652us/step - loss: 1.5657 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00605: loss did not improve from 1.56505\n",
      "Epoch 606/2000\n",
      "108/108 [==============================] - 0s 655us/step - loss: 1.5652 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00606: loss did not improve from 1.56505\n",
      "Epoch 607/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5686 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00607: loss did not improve from 1.56505\n",
      "Epoch 608/2000\n",
      "108/108 [==============================] - 0s 661us/step - loss: 1.5653 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00608: loss did not improve from 1.56505\n",
      "Epoch 609/2000\n",
      "108/108 [==============================] - 0s 655us/step - loss: 1.9779 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00609: loss did not improve from 1.56505\n",
      "Epoch 610/2000\n",
      "108/108 [==============================] - 0s 654us/step - loss: 1.9558 - accuracy: 0.3519\n",
      "\n",
      "Epoch 00610: loss did not improve from 1.56505\n",
      "Epoch 611/2000\n",
      "108/108 [==============================] - 0s 729us/step - loss: 1.6205 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00611: loss did not improve from 1.56505\n",
      "Epoch 612/2000\n",
      "108/108 [==============================] - 0s 674us/step - loss: 1.5680 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00612: loss did not improve from 1.56505\n",
      "Epoch 613/2000\n",
      "108/108 [==============================] - 0s 704us/step - loss: 1.5672 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00613: loss did not improve from 1.56505\n",
      "Epoch 614/2000\n",
      "108/108 [==============================] - 0s 683us/step - loss: 1.5670 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00614: loss did not improve from 1.56505\n",
      "Epoch 615/2000\n",
      "108/108 [==============================] - 0s 706us/step - loss: 1.5665 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00615: loss did not improve from 1.56505\n",
      "Epoch 616/2000\n",
      "108/108 [==============================] - 0s 665us/step - loss: 1.5663 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00616: loss did not improve from 1.56505\n",
      "Epoch 617/2000\n",
      "108/108 [==============================] - 0s 670us/step - loss: 1.5651 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00617: loss did not improve from 1.56505\n",
      "Epoch 618/2000\n",
      "108/108 [==============================] - 0s 655us/step - loss: 1.5636 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00618: loss improved from 1.56505 to 1.56364, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 619/2000\n",
      "108/108 [==============================] - 0s 733us/step - loss: 1.5776 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00619: loss did not improve from 1.56364\n",
      "Epoch 620/2000\n",
      "108/108 [==============================] - 0s 698us/step - loss: 1.5652 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00620: loss did not improve from 1.56364\n",
      "Epoch 621/2000\n",
      "108/108 [==============================] - 0s 706us/step - loss: 1.5654 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00621: loss did not improve from 1.56364\n",
      "Epoch 622/2000\n",
      "108/108 [==============================] - 0s 716us/step - loss: 1.5651 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00622: loss did not improve from 1.56364\n",
      "Epoch 623/2000\n",
      "108/108 [==============================] - 0s 717us/step - loss: 1.5649 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00623: loss did not improve from 1.56364\n",
      "Epoch 624/2000\n",
      "108/108 [==============================] - 0s 729us/step - loss: 1.5649 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00624: loss did not improve from 1.56364\n",
      "Epoch 625/2000\n",
      "108/108 [==============================] - 0s 736us/step - loss: 1.5651 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00625: loss did not improve from 1.56364\n",
      "Epoch 626/2000\n",
      "108/108 [==============================] - 0s 726us/step - loss: 1.5649 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00626: loss did not improve from 1.56364\n",
      "Epoch 627/2000\n",
      "108/108 [==============================] - 0s 703us/step - loss: 1.5648 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00627: loss did not improve from 1.56364\n",
      "Epoch 628/2000\n",
      "108/108 [==============================] - 0s 733us/step - loss: 1.5651 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00628: loss did not improve from 1.56364\n",
      "Epoch 629/2000\n",
      "108/108 [==============================] - 0s 720us/step - loss: 1.5649 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00629: loss did not improve from 1.56364\n",
      "Epoch 630/2000\n",
      "108/108 [==============================] - 0s 705us/step - loss: 1.5646 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00630: loss did not improve from 1.56364\n",
      "Epoch 631/2000\n",
      "108/108 [==============================] - 0s 706us/step - loss: 1.5648 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00631: loss did not improve from 1.56364\n",
      "Epoch 632/2000\n",
      "108/108 [==============================] - 0s 766us/step - loss: 1.5650 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00632: loss did not improve from 1.56364\n",
      "Epoch 633/2000\n",
      "108/108 [==============================] - 0s 690us/step - loss: 1.5650 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00633: loss did not improve from 1.56364\n",
      "Epoch 634/2000\n",
      "108/108 [==============================] - 0s 727us/step - loss: 1.5646 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00634: loss did not improve from 1.56364\n",
      "Epoch 635/2000\n",
      "108/108 [==============================] - 0s 746us/step - loss: 1.5649 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00635: loss did not improve from 1.56364\n",
      "Epoch 636/2000\n",
      "108/108 [==============================] - 0s 728us/step - loss: 1.5648 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00636: loss did not improve from 1.56364\n",
      "Epoch 637/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 707us/step - loss: 1.5647 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00637: loss did not improve from 1.56364\n",
      "Epoch 638/2000\n",
      "108/108 [==============================] - 0s 717us/step - loss: 1.5648 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00638: loss did not improve from 1.56364\n",
      "Epoch 639/2000\n",
      "108/108 [==============================] - 0s 720us/step - loss: 1.5647 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00639: loss did not improve from 1.56364\n",
      "Epoch 640/2000\n",
      "108/108 [==============================] - 0s 707us/step - loss: 1.5648 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00640: loss did not improve from 1.56364\n",
      "Epoch 641/2000\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.4810 - accuracy: 0.4306   - 0s 726us/step - loss: 1.5650 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00641: loss did not improve from 1.56364\n",
      "Epoch 642/2000\n",
      "108/108 [==============================] - 0s 726us/step - loss: 1.5651 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00642: loss did not improve from 1.56364\n",
      "Epoch 643/2000\n",
      "108/108 [==============================] - 0s 688us/step - loss: 1.5645 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00643: loss did not improve from 1.56364\n",
      "Epoch 644/2000\n",
      "108/108 [==============================] - 0s 702us/step - loss: 1.5651 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00644: loss did not improve from 1.56364\n",
      "Epoch 645/2000\n",
      "108/108 [==============================] - 0s 722us/step - loss: 1.5649 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00645: loss did not improve from 1.56364\n",
      "Epoch 646/2000\n",
      "108/108 [==============================] - 0s 704us/step - loss: 1.5650 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00646: loss did not improve from 1.56364\n",
      "Epoch 647/2000\n",
      "108/108 [==============================] - 0s 771us/step - loss: 1.5648 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00647: loss did not improve from 1.56364\n",
      "Epoch 648/2000\n",
      "108/108 [==============================] - 0s 746us/step - loss: 1.5646 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00648: loss did not improve from 1.56364\n",
      "Epoch 649/2000\n",
      "108/108 [==============================] - 0s 698us/step - loss: 1.5648 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00649: loss did not improve from 1.56364\n",
      "Epoch 650/2000\n",
      "108/108 [==============================] - 0s 735us/step - loss: 1.5647 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00650: loss did not improve from 1.56364\n",
      "Epoch 651/2000\n",
      "108/108 [==============================] - 0s 711us/step - loss: 1.5649 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00651: loss did not improve from 1.56364\n",
      "Epoch 652/2000\n",
      "108/108 [==============================] - 0s 708us/step - loss: 1.5652 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00652: loss did not improve from 1.56364\n",
      "Epoch 653/2000\n",
      "108/108 [==============================] - 0s 718us/step - loss: 1.5650 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00653: loss did not improve from 1.56364\n",
      "Epoch 654/2000\n",
      "108/108 [==============================] - 0s 702us/step - loss: 1.5648 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00654: loss did not improve from 1.56364\n",
      "Epoch 655/2000\n",
      "108/108 [==============================] - 0s 705us/step - loss: 1.5647 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00655: loss did not improve from 1.56364\n",
      "Epoch 656/2000\n",
      "108/108 [==============================] - 0s 711us/step - loss: 1.5646 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00656: loss did not improve from 1.56364\n",
      "Epoch 657/2000\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 1.5647 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00657: loss did not improve from 1.56364\n",
      "Epoch 658/2000\n",
      "108/108 [==============================] - 0s 859us/step - loss: 1.5647 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00658: loss did not improve from 1.56364\n",
      "Epoch 659/2000\n",
      "108/108 [==============================] - 0s 798us/step - loss: 1.5647 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00659: loss did not improve from 1.56364\n",
      "Epoch 660/2000\n",
      "108/108 [==============================] - 0s 785us/step - loss: 1.5646 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00660: loss did not improve from 1.56364\n",
      "Epoch 661/2000\n",
      "108/108 [==============================] - 0s 658us/step - loss: 1.5645 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00661: loss did not improve from 1.56364\n",
      "Epoch 662/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5648 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00662: loss did not improve from 1.56364\n",
      "Epoch 663/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5647 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00663: loss did not improve from 1.56364\n",
      "Epoch 664/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5646 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00664: loss did not improve from 1.56364\n",
      "Epoch 665/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5650 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00665: loss did not improve from 1.56364\n",
      "Epoch 666/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5644 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00666: loss did not improve from 1.56364\n",
      "Epoch 667/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5647 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00667: loss did not improve from 1.56364\n",
      "Epoch 668/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5648 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00668: loss did not improve from 1.56364\n",
      "Epoch 669/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5648 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00669: loss did not improve from 1.56364\n",
      "Epoch 670/2000\n",
      "108/108 [==============================] - 0s 711us/step - loss: 1.5647 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00670: loss did not improve from 1.56364\n",
      "Epoch 671/2000\n",
      "108/108 [==============================] - 0s 605us/step - loss: 1.5648 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00671: loss did not improve from 1.56364\n",
      "Epoch 672/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5646 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00672: loss did not improve from 1.56364\n",
      "Epoch 673/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5646 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00673: loss did not improve from 1.56364\n",
      "Epoch 674/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5647 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00674: loss did not improve from 1.56364\n",
      "Epoch 675/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5647 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00675: loss did not improve from 1.56364\n",
      "Epoch 676/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5649 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00676: loss did not improve from 1.56364\n",
      "Epoch 677/2000\n",
      "108/108 [==============================] - 0s 602us/step - loss: 1.5691 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00677: loss did not improve from 1.56364\n",
      "Epoch 678/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5649 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00678: loss did not improve from 1.56364\n",
      "Epoch 679/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5646 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00679: loss did not improve from 1.56364\n",
      "Epoch 680/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5645 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00680: loss did not improve from 1.56364\n",
      "Epoch 681/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5646 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00681: loss did not improve from 1.56364\n",
      "Epoch 682/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5646 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00682: loss did not improve from 1.56364\n",
      "Epoch 683/2000\n",
      "108/108 [==============================] - 0s 720us/step - loss: 1.5649 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00683: loss did not improve from 1.56364\n",
      "Epoch 684/2000\n",
      "108/108 [==============================] - 0s 720us/step - loss: 1.5643 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00684: loss did not improve from 1.56364\n",
      "Epoch 685/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5645 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00685: loss did not improve from 1.56364\n",
      "Epoch 686/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5644 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00686: loss did not improve from 1.56364\n",
      "Epoch 687/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5647 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00687: loss did not improve from 1.56364\n",
      "Epoch 688/2000\n",
      "108/108 [==============================] - 0s 693us/step - loss: 1.5651 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00688: loss did not improve from 1.56364\n",
      "Epoch 689/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5646 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00689: loss did not improve from 1.56364\n",
      "Epoch 690/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 637us/step - loss: 1.5644 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00690: loss did not improve from 1.56364\n",
      "Epoch 691/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5645 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00691: loss did not improve from 1.56364\n",
      "Epoch 692/2000\n",
      "108/108 [==============================] - 0s 656us/step - loss: 1.5640 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00692: loss did not improve from 1.56364\n",
      "Epoch 693/2000\n",
      "108/108 [==============================] - 0s 794us/step - loss: 1.5646 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00693: loss did not improve from 1.56364\n",
      "Epoch 694/2000\n",
      "108/108 [==============================] - 0s 776us/step - loss: 1.5643 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00694: loss did not improve from 1.56364\n",
      "Epoch 695/2000\n",
      "108/108 [==============================] - 0s 748us/step - loss: 1.5645 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00695: loss did not improve from 1.56364\n",
      "Epoch 696/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5647 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00696: loss did not improve from 1.56364\n",
      "Epoch 697/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5646 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00697: loss did not improve from 1.56364\n",
      "Epoch 698/2000\n",
      "108/108 [==============================] - 0s 665us/step - loss: 1.5642 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00698: loss did not improve from 1.56364\n",
      "Epoch 699/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5642 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00699: loss did not improve from 1.56364\n",
      "Epoch 700/2000\n",
      "108/108 [==============================] - 0s 621us/step - loss: 1.5646 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00700: loss did not improve from 1.56364\n",
      "Epoch 701/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5639 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00701: loss did not improve from 1.56364\n",
      "Epoch 702/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5645 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00702: loss did not improve from 1.56364\n",
      "Epoch 703/2000\n",
      "108/108 [==============================] - 0s 608us/step - loss: 1.5643 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00703: loss did not improve from 1.56364\n",
      "Epoch 704/2000\n",
      "108/108 [==============================] - 0s 610us/step - loss: 1.5643 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00704: loss did not improve from 1.56364\n",
      "Epoch 705/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5644 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00705: loss did not improve from 1.56364\n",
      "Epoch 706/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5645 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00706: loss did not improve from 1.56364\n",
      "Epoch 707/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5645 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00707: loss did not improve from 1.56364\n",
      "Epoch 708/2000\n",
      "108/108 [==============================] - 0s 766us/step - loss: 1.5644 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00708: loss did not improve from 1.56364\n",
      "Epoch 709/2000\n",
      "108/108 [==============================] - 0s 739us/step - loss: 1.5643 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00709: loss did not improve from 1.56364\n",
      "Epoch 710/2000\n",
      "108/108 [==============================] - 0s 656us/step - loss: 1.5661 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00710: loss did not improve from 1.56364\n",
      "Epoch 711/2000\n",
      "108/108 [==============================] - 0s 626us/step - loss: 1.5662 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00711: loss did not improve from 1.56364\n",
      "Epoch 712/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5652 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00712: loss did not improve from 1.56364\n",
      "Epoch 713/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5647 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00713: loss did not improve from 1.56364\n",
      "Epoch 714/2000\n",
      "108/108 [==============================] - 0s 598us/step - loss: 1.5644 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00714: loss did not improve from 1.56364\n",
      "Epoch 715/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5644 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00715: loss did not improve from 1.56364\n",
      "Epoch 716/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5646 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00716: loss did not improve from 1.56364\n",
      "Epoch 717/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5645 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00717: loss did not improve from 1.56364\n",
      "Epoch 718/2000\n",
      "108/108 [==============================] - 0s 683us/step - loss: 1.5639 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00718: loss did not improve from 1.56364\n",
      "Epoch 719/2000\n",
      "108/108 [==============================] - 0s 757us/step - loss: 1.5645 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00719: loss did not improve from 1.56364\n",
      "Epoch 720/2000\n",
      "108/108 [==============================] - 0s 748us/step - loss: 1.5646 - accuracy: 0.4444\n",
      "\n",
      "Epoch 00720: loss did not improve from 1.56364\n",
      "Epoch 721/2000\n",
      "108/108 [==============================] - 0s 748us/step - loss: 1.5646 - accuracy: 0.3704\n",
      "\n",
      "Epoch 00721: loss did not improve from 1.56364\n",
      "Epoch 722/2000\n",
      "108/108 [==============================] - 0s 618us/step - loss: 1.5642 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00722: loss did not improve from 1.56364\n",
      "Epoch 723/2000\n",
      "108/108 [==============================] - 0s 588us/step - loss: 1.5645 - accuracy: 0.3704\n",
      "\n",
      "Epoch 00723: loss did not improve from 1.56364\n",
      "Epoch 724/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5644 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00724: loss did not improve from 1.56364\n",
      "Epoch 725/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5644 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00725: loss did not improve from 1.56364\n",
      "Epoch 726/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5641 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00726: loss did not improve from 1.56364\n",
      "Epoch 727/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5641 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00727: loss did not improve from 1.56364\n",
      "Epoch 728/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5641 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00728: loss did not improve from 1.56364\n",
      "Epoch 729/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5642 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00729: loss did not improve from 1.56364\n",
      "Epoch 730/2000\n",
      "108/108 [==============================] - 0s 610us/step - loss: 1.5648 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00730: loss did not improve from 1.56364\n",
      "Epoch 731/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5640 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00731: loss did not improve from 1.56364\n",
      "Epoch 732/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5642 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00732: loss did not improve from 1.56364\n",
      "Epoch 733/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5642 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00733: loss did not improve from 1.56364\n",
      "Epoch 734/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5642 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00734: loss did not improve from 1.56364\n",
      "Epoch 735/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5649 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00735: loss did not improve from 1.56364\n",
      "Epoch 736/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5674 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00736: loss did not improve from 1.56364\n",
      "Epoch 737/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5657 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00737: loss did not improve from 1.56364\n",
      "Epoch 738/2000\n",
      "108/108 [==============================] - 0s 606us/step - loss: 1.5641 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00738: loss did not improve from 1.56364\n",
      "Epoch 739/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5654 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00739: loss did not improve from 1.56364\n",
      "Epoch 740/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5650 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00740: loss did not improve from 1.56364\n",
      "Epoch 741/2000\n",
      "108/108 [==============================] - 0s 606us/step - loss: 1.5643 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00741: loss did not improve from 1.56364\n",
      "Epoch 742/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5642 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00742: loss did not improve from 1.56364\n",
      "Epoch 743/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5640 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00743: loss did not improve from 1.56364\n",
      "Epoch 744/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5648 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00744: loss did not improve from 1.56364\n",
      "Epoch 745/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5640 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00745: loss did not improve from 1.56364\n",
      "Epoch 746/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5641 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00746: loss did not improve from 1.56364\n",
      "Epoch 747/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5638 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00747: loss did not improve from 1.56364\n",
      "Epoch 748/2000\n",
      "108/108 [==============================] - 0s 683us/step - loss: 1.5640 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00748: loss did not improve from 1.56364\n",
      "Epoch 749/2000\n",
      "108/108 [==============================] - 0s 748us/step - loss: 1.5639 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00749: loss did not improve from 1.56364\n",
      "Epoch 750/2000\n",
      "108/108 [==============================] - 0s 757us/step - loss: 1.5638 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00750: loss did not improve from 1.56364\n",
      "Epoch 751/2000\n",
      "108/108 [==============================] - 0s 518us/step - loss: 1.5639 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00751: loss did not improve from 1.56364\n",
      "Epoch 752/2000\n",
      "108/108 [==============================] - 0s 744us/step - loss: 1.5642 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00752: loss did not improve from 1.56364\n",
      "Epoch 753/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5639 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00753: loss did not improve from 1.56364\n",
      "Epoch 754/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5641 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00754: loss did not improve from 1.56364\n",
      "Epoch 755/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5641 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00755: loss did not improve from 1.56364\n",
      "Epoch 756/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5644 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00756: loss did not improve from 1.56364\n",
      "Epoch 757/2000\n",
      "108/108 [==============================] - 0s 739us/step - loss: 1.5667 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00757: loss did not improve from 1.56364\n",
      "Epoch 758/2000\n",
      "108/108 [==============================] - 0s 730us/step - loss: 1.5665 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00758: loss did not improve from 1.56364\n",
      "Epoch 759/2000\n",
      "108/108 [==============================] - 0s 657us/step - loss: 1.5650 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00759: loss did not improve from 1.56364\n",
      "Epoch 760/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5642 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00760: loss did not improve from 1.56364\n",
      "Epoch 761/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5640 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00761: loss did not improve from 1.56364\n",
      "Epoch 762/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5638 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00762: loss did not improve from 1.56364\n",
      "Epoch 763/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5639 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00763: loss did not improve from 1.56364\n",
      "Epoch 764/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5640 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00764: loss did not improve from 1.56364\n",
      "Epoch 765/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5642 - accuracy: 0.3704\n",
      "\n",
      "Epoch 00765: loss did not improve from 1.56364\n",
      "Epoch 766/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5641 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00766: loss did not improve from 1.56364\n",
      "Epoch 767/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5647 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00767: loss did not improve from 1.56364\n",
      "Epoch 768/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5642 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00768: loss did not improve from 1.56364\n",
      "Epoch 769/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5642 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00769: loss did not improve from 1.56364\n",
      "Epoch 770/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5644 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00770: loss did not improve from 1.56364\n",
      "Epoch 771/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5640 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00771: loss did not improve from 1.56364\n",
      "Epoch 772/2000\n",
      "108/108 [==============================] - 0s 601us/step - loss: 1.5639 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00772: loss did not improve from 1.56364\n",
      "Epoch 773/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5638 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00773: loss did not improve from 1.56364\n",
      "Epoch 774/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5637 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00774: loss did not improve from 1.56364\n",
      "Epoch 775/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5639 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00775: loss did not improve from 1.56364\n",
      "Epoch 776/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5640 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00776: loss did not improve from 1.56364\n",
      "Epoch 777/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5638 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00777: loss did not improve from 1.56364\n",
      "Epoch 778/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5637 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00778: loss did not improve from 1.56364\n",
      "Epoch 779/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5643 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00779: loss did not improve from 1.56364\n",
      "Epoch 780/2000\n",
      "108/108 [==============================] - 0s 603us/step - loss: 1.5639 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00780: loss did not improve from 1.56364\n",
      "Epoch 781/2000\n",
      "108/108 [==============================] - 0s 598us/step - loss: 1.5643 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00781: loss did not improve from 1.56364\n",
      "Epoch 782/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5638 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00782: loss did not improve from 1.56364\n",
      "Epoch 783/2000\n",
      "108/108 [==============================] - 0s 605us/step - loss: 1.5640 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00783: loss did not improve from 1.56364\n",
      "Epoch 784/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5637 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00784: loss did not improve from 1.56364\n",
      "Epoch 785/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5638 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00785: loss did not improve from 1.56364\n",
      "Epoch 786/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5642 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00786: loss did not improve from 1.56364\n",
      "Epoch 787/2000\n",
      "108/108 [==============================] - 0s 597us/step - loss: 1.5640 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00787: loss did not improve from 1.56364\n",
      "Epoch 788/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5635 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00788: loss improved from 1.56364 to 1.56353, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 789/2000\n",
      "108/108 [==============================] - 0s 750us/step - loss: 1.5639 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00789: loss did not improve from 1.56353\n",
      "Epoch 790/2000\n",
      "108/108 [==============================] - 0s 776us/step - loss: 1.5640 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00790: loss did not improve from 1.56353\n",
      "Epoch 791/2000\n",
      "108/108 [==============================] - 0s 771us/step - loss: 1.5637 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00791: loss did not improve from 1.56353\n",
      "Epoch 792/2000\n",
      "108/108 [==============================] - 0s 791us/step - loss: 1.5635 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00792: loss improved from 1.56353 to 1.56350, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 793/2000\n",
      "108/108 [==============================] - 0s 770us/step - loss: 1.5633 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00793: loss improved from 1.56350 to 1.56330, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 794/2000\n",
      "108/108 [==============================] - 0s 748us/step - loss: 1.5641 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00794: loss did not improve from 1.56330\n",
      "Epoch 795/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 785us/step - loss: 1.5639 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00795: loss did not improve from 1.56330\n",
      "Epoch 796/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5641 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00796: loss did not improve from 1.56330\n",
      "Epoch 797/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5639 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00797: loss did not improve from 1.56330\n",
      "Epoch 798/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5634 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00798: loss did not improve from 1.56330\n",
      "Epoch 799/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5638 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00799: loss did not improve from 1.56330\n",
      "Epoch 800/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5640 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00800: loss did not improve from 1.56330\n",
      "Epoch 801/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5638 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00801: loss did not improve from 1.56330\n",
      "Epoch 802/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5637 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00802: loss did not improve from 1.56330\n",
      "Epoch 803/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5639 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00803: loss did not improve from 1.56330\n",
      "Epoch 804/2000\n",
      "108/108 [==============================] - 0s 595us/step - loss: 1.5638 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00804: loss did not improve from 1.56330\n",
      "Epoch 805/2000\n",
      "108/108 [==============================] - 0s 603us/step - loss: 1.5636 - accuracy: 0.4352\n",
      "\n",
      "Epoch 00805: loss did not improve from 1.56330\n",
      "Epoch 806/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5638 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00806: loss did not improve from 1.56330\n",
      "Epoch 807/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5636 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00807: loss did not improve from 1.56330\n",
      "Epoch 808/2000\n",
      "108/108 [==============================] - 0s 598us/step - loss: 1.5639 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00808: loss did not improve from 1.56330\n",
      "Epoch 809/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5636 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00809: loss did not improve from 1.56330\n",
      "Epoch 810/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5637 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00810: loss did not improve from 1.56330\n",
      "Epoch 811/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5636 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00811: loss did not improve from 1.56330\n",
      "Epoch 812/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5635 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00812: loss did not improve from 1.56330\n",
      "Epoch 813/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5649 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00813: loss did not improve from 1.56330\n",
      "Epoch 814/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5644 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00814: loss did not improve from 1.56330\n",
      "Epoch 815/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5637 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00815: loss did not improve from 1.56330\n",
      "Epoch 816/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5644 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00816: loss did not improve from 1.56330\n",
      "Epoch 817/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5649 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00817: loss did not improve from 1.56330\n",
      "Epoch 818/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5640 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00818: loss did not improve from 1.56330\n",
      "Epoch 819/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5639 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00819: loss did not improve from 1.56330\n",
      "Epoch 820/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5640 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00820: loss did not improve from 1.56330\n",
      "Epoch 821/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5640 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00821: loss did not improve from 1.56330\n",
      "Epoch 822/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5637 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00822: loss did not improve from 1.56330\n",
      "Epoch 823/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5635 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00823: loss did not improve from 1.56330\n",
      "Epoch 824/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5634 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00824: loss did not improve from 1.56330\n",
      "Epoch 825/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5635 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00825: loss did not improve from 1.56330\n",
      "Epoch 826/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5634 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00826: loss did not improve from 1.56330\n",
      "Epoch 827/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5637 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00827: loss did not improve from 1.56330\n",
      "Epoch 828/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5636 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00828: loss did not improve from 1.56330\n",
      "Epoch 829/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5634 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00829: loss did not improve from 1.56330\n",
      "Epoch 830/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5634 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00830: loss did not improve from 1.56330\n",
      "Epoch 831/2000\n",
      "108/108 [==============================] - 0s 598us/step - loss: 1.5635 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00831: loss did not improve from 1.56330\n",
      "Epoch 832/2000\n",
      "108/108 [==============================] - 0s 602us/step - loss: 1.5636 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00832: loss did not improve from 1.56330\n",
      "Epoch 833/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5637 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00833: loss did not improve from 1.56330\n",
      "Epoch 834/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5643 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00834: loss did not improve from 1.56330\n",
      "Epoch 835/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5641 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00835: loss did not improve from 1.56330\n",
      "Epoch 836/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5684 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00836: loss did not improve from 1.56330\n",
      "Epoch 837/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5689 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00837: loss did not improve from 1.56330\n",
      "Epoch 838/2000\n",
      "108/108 [==============================] - 0s 595us/step - loss: 1.5655 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00838: loss did not improve from 1.56330\n",
      "Epoch 839/2000\n",
      "108/108 [==============================] - 0s 595us/step - loss: 1.5645 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00839: loss did not improve from 1.56330\n",
      "Epoch 840/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5640 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00840: loss did not improve from 1.56330\n",
      "Epoch 841/2000\n",
      "108/108 [==============================] - 0s 604us/step - loss: 1.5663 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00841: loss did not improve from 1.56330\n",
      "Epoch 842/2000\n",
      "108/108 [==============================] - 0s 595us/step - loss: 1.5637 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00842: loss did not improve from 1.56330\n",
      "Epoch 843/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5636 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00843: loss did not improve from 1.56330\n",
      "Epoch 844/2000\n",
      "108/108 [==============================] - 0s 604us/step - loss: 1.7371 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00844: loss did not improve from 1.56330\n",
      "Epoch 845/2000\n",
      "108/108 [==============================] - 0s 604us/step - loss: 2.1993 - accuracy: 0.3704\n",
      "\n",
      "Epoch 00845: loss did not improve from 1.56330\n",
      "Epoch 846/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.6872 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00846: loss did not improve from 1.56330\n",
      "Epoch 847/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5660 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00847: loss did not improve from 1.56330\n",
      "Epoch 848/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5646 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00848: loss did not improve from 1.56330\n",
      "Epoch 849/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5642 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00849: loss did not improve from 1.56330\n",
      "Epoch 850/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5641 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00850: loss did not improve from 1.56330\n",
      "Epoch 851/2000\n",
      "108/108 [==============================] - 0s 596us/step - loss: 1.5638 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00851: loss did not improve from 1.56330\n",
      "Epoch 852/2000\n",
      "108/108 [==============================] - 0s 613us/step - loss: 1.5638 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00852: loss did not improve from 1.56330\n",
      "Epoch 853/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5636 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00853: loss did not improve from 1.56330\n",
      "Epoch 854/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5637 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00854: loss did not improve from 1.56330\n",
      "Epoch 855/2000\n",
      "108/108 [==============================] - 0s 598us/step - loss: 1.5635 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00855: loss did not improve from 1.56330\n",
      "Epoch 856/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5634 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00856: loss did not improve from 1.56330\n",
      "Epoch 857/2000\n",
      "108/108 [==============================] - 0s 593us/step - loss: 1.5633 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00857: loss did not improve from 1.56330\n",
      "Epoch 858/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5632 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00858: loss improved from 1.56330 to 1.56325, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 859/2000\n",
      "108/108 [==============================] - 0s 667us/step - loss: 1.5633 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00859: loss did not improve from 1.56325\n",
      "Epoch 860/2000\n",
      "108/108 [==============================] - 0s 687us/step - loss: 1.5635 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00860: loss did not improve from 1.56325\n",
      "Epoch 861/2000\n",
      "108/108 [==============================] - 0s 664us/step - loss: 1.5635 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00861: loss did not improve from 1.56325\n",
      "Epoch 862/2000\n",
      "108/108 [==============================] - 0s 699us/step - loss: 1.5636 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00862: loss did not improve from 1.56325\n",
      "Epoch 863/2000\n",
      "108/108 [==============================] - 0s 676us/step - loss: 1.5635 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00863: loss did not improve from 1.56325\n",
      "Epoch 864/2000\n",
      "108/108 [==============================] - 0s 705us/step - loss: 1.5635 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00864: loss did not improve from 1.56325\n",
      "Epoch 865/2000\n",
      "108/108 [==============================] - 0s 692us/step - loss: 1.5636 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00865: loss did not improve from 1.56325\n",
      "Epoch 866/2000\n",
      "108/108 [==============================] - 0s 695us/step - loss: 1.5635 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00866: loss did not improve from 1.56325\n",
      "Epoch 867/2000\n",
      "108/108 [==============================] - 0s 677us/step - loss: 1.5633 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00867: loss did not improve from 1.56325\n",
      "Epoch 868/2000\n",
      "108/108 [==============================] - 0s 708us/step - loss: 1.5633 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00868: loss did not improve from 1.56325\n",
      "Epoch 869/2000\n",
      "108/108 [==============================] - 0s 711us/step - loss: 1.5633 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00869: loss did not improve from 1.56325\n",
      "Epoch 870/2000\n",
      "108/108 [==============================] - 0s 680us/step - loss: 1.5632 - accuracy: 0.4352\n",
      "\n",
      "Epoch 00870: loss improved from 1.56325 to 1.56322, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 871/2000\n",
      "108/108 [==============================] - 0s 704us/step - loss: 1.5633 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00871: loss did not improve from 1.56322\n",
      "Epoch 872/2000\n",
      "108/108 [==============================] - 0s 692us/step - loss: 1.5634 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00872: loss did not improve from 1.56322\n",
      "Epoch 873/2000\n",
      "108/108 [==============================] - 0s 707us/step - loss: 1.5634 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00873: loss did not improve from 1.56322\n",
      "Epoch 874/2000\n",
      "108/108 [==============================] - 0s 688us/step - loss: 1.5631 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00874: loss improved from 1.56322 to 1.56313, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 875/2000\n",
      "108/108 [==============================] - 0s 696us/step - loss: 1.5633 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00875: loss did not improve from 1.56313\n",
      "Epoch 876/2000\n",
      "108/108 [==============================] - 0s 689us/step - loss: 1.5634 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00876: loss did not improve from 1.56313\n",
      "Epoch 877/2000\n",
      "108/108 [==============================] - 0s 684us/step - loss: 1.5632 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00877: loss did not improve from 1.56313\n",
      "Epoch 878/2000\n",
      "108/108 [==============================] - 0s 697us/step - loss: 1.5633 - accuracy: 0.3704\n",
      "\n",
      "Epoch 00878: loss did not improve from 1.56313\n",
      "Epoch 879/2000\n",
      "108/108 [==============================] - 0s 685us/step - loss: 1.5636 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00879: loss did not improve from 1.56313\n",
      "Epoch 880/2000\n",
      "108/108 [==============================] - 0s 696us/step - loss: 1.5634 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00880: loss did not improve from 1.56313\n",
      "Epoch 881/2000\n",
      "108/108 [==============================] - 0s 696us/step - loss: 1.5633 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00881: loss did not improve from 1.56313\n",
      "Epoch 882/2000\n",
      "108/108 [==============================] - 0s 686us/step - loss: 1.5633 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00882: loss did not improve from 1.56313\n",
      "Epoch 883/2000\n",
      "108/108 [==============================] - 0s 708us/step - loss: 1.5631 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00883: loss did not improve from 1.56313\n",
      "Epoch 884/2000\n",
      "108/108 [==============================] - 0s 672us/step - loss: 1.5635 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00884: loss did not improve from 1.56313\n",
      "Epoch 885/2000\n",
      "108/108 [==============================] - 0s 681us/step - loss: 1.5635 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00885: loss did not improve from 1.56313\n",
      "Epoch 886/2000\n",
      "108/108 [==============================] - 0s 674us/step - loss: 1.5631 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00886: loss did not improve from 1.56313\n",
      "Epoch 887/2000\n",
      "108/108 [==============================] - 0s 690us/step - loss: 1.5632 - accuracy: 0.3704\n",
      "\n",
      "Epoch 00887: loss did not improve from 1.56313\n",
      "Epoch 888/2000\n",
      "108/108 [==============================] - 0s 684us/step - loss: 1.5635 - accuracy: 0.3704\n",
      "\n",
      "Epoch 00888: loss did not improve from 1.56313\n",
      "Epoch 889/2000\n",
      "108/108 [==============================] - 0s 698us/step - loss: 1.5634 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00889: loss did not improve from 1.56313\n",
      "Epoch 890/2000\n",
      "108/108 [==============================] - 0s 724us/step - loss: 1.5633 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00890: loss did not improve from 1.56313\n",
      "Epoch 891/2000\n",
      "108/108 [==============================] - 0s 674us/step - loss: 1.5634 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00891: loss did not improve from 1.56313\n",
      "Epoch 892/2000\n",
      "108/108 [==============================] - 0s 699us/step - loss: 1.5634 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00892: loss did not improve from 1.56313\n",
      "Epoch 893/2000\n",
      "108/108 [==============================] - 0s 688us/step - loss: 1.5634 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00893: loss did not improve from 1.56313\n",
      "Epoch 894/2000\n",
      "108/108 [==============================] - 0s 706us/step - loss: 1.5633 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00894: loss did not improve from 1.56313\n",
      "Epoch 895/2000\n",
      "108/108 [==============================] - 0s 759us/step - loss: 1.5631 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00895: loss did not improve from 1.56313\n",
      "Epoch 896/2000\n",
      "108/108 [==============================] - 0s 916us/step - loss: 1.5634 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00896: loss did not improve from 1.56313\n",
      "Epoch 897/2000\n",
      "108/108 [==============================] - 0s 868us/step - loss: 1.5632 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00897: loss did not improve from 1.56313\n",
      "Epoch 898/2000\n",
      "108/108 [==============================] - 0s 748us/step - loss: 1.5634 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00898: loss did not improve from 1.56313\n",
      "Epoch 899/2000\n",
      "108/108 [==============================] - 0s 694us/step - loss: 1.5631 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00899: loss did not improve from 1.56313\n",
      "Epoch 900/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 718us/step - loss: 1.5633 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00900: loss did not improve from 1.56313\n",
      "Epoch 901/2000\n",
      "108/108 [==============================] - 0s 716us/step - loss: 1.5632 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00901: loss did not improve from 1.56313\n",
      "Epoch 902/2000\n",
      "108/108 [==============================] - 0s 705us/step - loss: 1.5633 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00902: loss did not improve from 1.56313\n",
      "Epoch 903/2000\n",
      "108/108 [==============================] - 0s 713us/step - loss: 1.5634 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00903: loss did not improve from 1.56313\n",
      "Epoch 904/2000\n",
      "108/108 [==============================] - 0s 748us/step - loss: 1.5629 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00904: loss improved from 1.56313 to 1.56288, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 905/2000\n",
      "108/108 [==============================] - 0s 927us/step - loss: 1.5631 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00905: loss did not improve from 1.56288\n",
      "Epoch 906/2000\n",
      "108/108 [==============================] - 0s 947us/step - loss: 1.5634 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00906: loss did not improve from 1.56288\n",
      "Epoch 907/2000\n",
      "108/108 [==============================] - 0s 768us/step - loss: 1.5632 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00907: loss did not improve from 1.56288\n",
      "Epoch 908/2000\n",
      "108/108 [==============================] - 0s 717us/step - loss: 1.5635 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00908: loss did not improve from 1.56288\n",
      "Epoch 909/2000\n",
      "108/108 [==============================] - 0s 768us/step - loss: 1.5633 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00909: loss did not improve from 1.56288\n",
      "Epoch 910/2000\n",
      "108/108 [==============================] - 0s 764us/step - loss: 1.5632 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00910: loss did not improve from 1.56288\n",
      "Epoch 911/2000\n",
      "108/108 [==============================] - 0s 776us/step - loss: 1.5677 - accuracy: 0.3704\n",
      "\n",
      "Epoch 00911: loss did not improve from 1.56288\n",
      "Epoch 912/2000\n",
      "108/108 [==============================] - 0s 924us/step - loss: 1.5645 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00912: loss did not improve from 1.56288\n",
      "Epoch 913/2000\n",
      "108/108 [==============================] - 0s 852us/step - loss: 1.5648 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00913: loss did not improve from 1.56288\n",
      "Epoch 914/2000\n",
      "108/108 [==============================] - 0s 693us/step - loss: 1.5641 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00914: loss did not improve from 1.56288\n",
      "Epoch 915/2000\n",
      "108/108 [==============================] - 0s 656us/step - loss: 1.5669 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00915: loss did not improve from 1.56288\n",
      "Epoch 916/2000\n",
      "108/108 [==============================] - 0s 656us/step - loss: 1.5649 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00916: loss did not improve from 1.56288\n",
      "Epoch 917/2000\n",
      "108/108 [==============================] - 0s 642us/step - loss: 1.5660 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00917: loss did not improve from 1.56288\n",
      "Epoch 918/2000\n",
      "108/108 [==============================] - 0s 647us/step - loss: 1.5641 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00918: loss did not improve from 1.56288\n",
      "Epoch 919/2000\n",
      "108/108 [==============================] - 0s 831us/step - loss: 1.5647 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00919: loss did not improve from 1.56288\n",
      "Epoch 920/2000\n",
      "108/108 [==============================] - 0s 868us/step - loss: 1.5639 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00920: loss did not improve from 1.56288\n",
      "Epoch 921/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5635 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00921: loss did not improve from 1.56288\n",
      "Epoch 922/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5640 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00922: loss did not improve from 1.56288\n",
      "Epoch 923/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5637 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00923: loss did not improve from 1.56288\n",
      "Epoch 924/2000\n",
      "108/108 [==============================] - 0s 587us/step - loss: 1.5636 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00924: loss did not improve from 1.56288\n",
      "Epoch 925/2000\n",
      "108/108 [==============================] - 0s 665us/step - loss: 1.5635 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00925: loss did not improve from 1.56288\n",
      "Epoch 926/2000\n",
      "108/108 [==============================] - 0s 794us/step - loss: 1.5633 - accuracy: 0.3704\n",
      "\n",
      "Epoch 00926: loss did not improve from 1.56288\n",
      "Epoch 927/2000\n",
      "108/108 [==============================] - 0s 748us/step - loss: 1.5637 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00927: loss did not improve from 1.56288\n",
      "Epoch 928/2000\n",
      "108/108 [==============================] - 0s 608us/step - loss: 1.5631 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00928: loss did not improve from 1.56288\n",
      "Epoch 929/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5635 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00929: loss did not improve from 1.56288\n",
      "Epoch 930/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5633 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00930: loss did not improve from 1.56288\n",
      "Epoch 931/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5637 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00931: loss did not improve from 1.56288\n",
      "Epoch 932/2000\n",
      "108/108 [==============================] - 0s 739us/step - loss: 1.5639 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00932: loss did not improve from 1.56288\n",
      "Epoch 933/2000\n",
      "108/108 [==============================] - 0s 776us/step - loss: 1.5633 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00933: loss did not improve from 1.56288\n",
      "Epoch 934/2000\n",
      "108/108 [==============================] - 0s 702us/step - loss: 1.5672 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00934: loss did not improve from 1.56288\n",
      "Epoch 935/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5646 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00935: loss did not improve from 1.56288\n",
      "Epoch 936/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5651 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00936: loss did not improve from 1.56288\n",
      "Epoch 937/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5649 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00937: loss did not improve from 1.56288\n",
      "Epoch 938/2000\n",
      "108/108 [==============================] - 0s 589us/step - loss: 1.5648 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00938: loss did not improve from 1.56288\n",
      "Epoch 939/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5644 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00939: loss did not improve from 1.56288\n",
      "Epoch 940/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5642 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00940: loss did not improve from 1.56288\n",
      "Epoch 941/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5636 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00941: loss did not improve from 1.56288\n",
      "Epoch 942/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5635 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00942: loss did not improve from 1.56288\n",
      "Epoch 943/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5637 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00943: loss did not improve from 1.56288\n",
      "Epoch 944/2000\n",
      "108/108 [==============================] - 0s 720us/step - loss: 1.5635 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00944: loss did not improve from 1.56288\n",
      "Epoch 945/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5637 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00945: loss did not improve from 1.56288\n",
      "Epoch 946/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5635 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00946: loss did not improve from 1.56288\n",
      "Epoch 947/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5635 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00947: loss did not improve from 1.56288\n",
      "Epoch 948/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5633 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00948: loss did not improve from 1.56288\n",
      "Epoch 949/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5634 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00949: loss did not improve from 1.56288\n",
      "Epoch 950/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5632 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00950: loss did not improve from 1.56288\n",
      "Epoch 951/2000\n",
      "108/108 [==============================] - 0s 602us/step - loss: 1.5633 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00951: loss did not improve from 1.56288\n",
      "Epoch 952/2000\n",
      "108/108 [==============================] - 0s 730us/step - loss: 1.5632 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00952: loss did not improve from 1.56288\n",
      "Epoch 953/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 748us/step - loss: 1.5634 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00953: loss did not improve from 1.56288\n",
      "Epoch 954/2000\n",
      "108/108 [==============================] - 0s 739us/step - loss: 1.5633 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00954: loss did not improve from 1.56288\n",
      "Epoch 955/2000\n",
      "108/108 [==============================] - 0s 645us/step - loss: 1.5633 - accuracy: 0.3704\n",
      "\n",
      "Epoch 00955: loss did not improve from 1.56288\n",
      "Epoch 956/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5632 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00956: loss did not improve from 1.56288\n",
      "Epoch 957/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5632 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00957: loss did not improve from 1.56288\n",
      "Epoch 958/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5641 - accuracy: 0.4352\n",
      "\n",
      "Epoch 00958: loss did not improve from 1.56288\n",
      "Epoch 959/2000\n",
      "108/108 [==============================] - 0s 618us/step - loss: 1.5632 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00959: loss did not improve from 1.56288\n",
      "Epoch 960/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5632 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00960: loss did not improve from 1.56288\n",
      "Epoch 961/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5631 - accuracy: 0.4352\n",
      "\n",
      "Epoch 00961: loss did not improve from 1.56288\n",
      "Epoch 962/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5630 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00962: loss did not improve from 1.56288\n",
      "Epoch 963/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5630 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00963: loss did not improve from 1.56288\n",
      "Epoch 964/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5630 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00964: loss did not improve from 1.56288\n",
      "Epoch 965/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5631 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00965: loss did not improve from 1.56288\n",
      "Epoch 966/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5633 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00966: loss did not improve from 1.56288\n",
      "Epoch 967/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5630 - accuracy: 0.3704\n",
      "\n",
      "Epoch 00967: loss did not improve from 1.56288\n",
      "Epoch 968/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5632 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00968: loss did not improve from 1.56288\n",
      "Epoch 969/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5631 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00969: loss did not improve from 1.56288\n",
      "Epoch 970/2000\n",
      "108/108 [==============================] - 0s 580us/step - loss: 1.5632 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00970: loss did not improve from 1.56288\n",
      "Epoch 971/2000\n",
      "108/108 [==============================] - 0s 601us/step - loss: 1.5629 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00971: loss did not improve from 1.56288\n",
      "Epoch 972/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5630 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00972: loss did not improve from 1.56288\n",
      "Epoch 973/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5633 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00973: loss did not improve from 1.56288\n",
      "Epoch 974/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5681 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00974: loss did not improve from 1.56288\n",
      "Epoch 975/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5638 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00975: loss did not improve from 1.56288\n",
      "Epoch 976/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5636 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00976: loss did not improve from 1.56288\n",
      "Epoch 977/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5632 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00977: loss did not improve from 1.56288\n",
      "Epoch 978/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5632 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00978: loss did not improve from 1.56288\n",
      "Epoch 979/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5631 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00979: loss did not improve from 1.56288\n",
      "Epoch 980/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5630 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00980: loss did not improve from 1.56288\n",
      "Epoch 981/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5631 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00981: loss did not improve from 1.56288\n",
      "Epoch 982/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5630 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00982: loss did not improve from 1.56288\n",
      "Epoch 983/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5631 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00983: loss did not improve from 1.56288\n",
      "Epoch 984/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5630 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00984: loss did not improve from 1.56288\n",
      "Epoch 985/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5630 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00985: loss did not improve from 1.56288\n",
      "Epoch 986/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5628 - accuracy: 0.3796\n",
      "\n",
      "Epoch 00986: loss improved from 1.56288 to 1.56277, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 987/2000\n",
      "108/108 [==============================] - 0s 644us/step - loss: 1.5628 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00987: loss did not improve from 1.56277\n",
      "Epoch 988/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5631 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00988: loss did not improve from 1.56277\n",
      "Epoch 989/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5628 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00989: loss did not improve from 1.56277\n",
      "Epoch 990/2000\n",
      "108/108 [==============================] - 0s 633us/step - loss: 1.5632 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00990: loss did not improve from 1.56277\n",
      "Epoch 991/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5633 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00991: loss did not improve from 1.56277\n",
      "Epoch 992/2000\n",
      "108/108 [==============================] - 0s 645us/step - loss: 1.5630 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00992: loss did not improve from 1.56277\n",
      "Epoch 993/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5631 - accuracy: 0.4074\n",
      "\n",
      "Epoch 00993: loss did not improve from 1.56277\n",
      "Epoch 994/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5627 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00994: loss improved from 1.56277 to 1.56275, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 995/2000\n",
      "108/108 [==============================] - 0s 691us/step - loss: 1.5660 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00995: loss did not improve from 1.56275\n",
      "Epoch 996/2000\n",
      "108/108 [==============================] - 0s 687us/step - loss: 1.5644 - accuracy: 0.3889\n",
      "\n",
      "Epoch 00996: loss did not improve from 1.56275\n",
      "Epoch 997/2000\n",
      "108/108 [==============================] - 0s 689us/step - loss: 1.5636 - accuracy: 0.3981\n",
      "\n",
      "Epoch 00997: loss did not improve from 1.56275\n",
      "Epoch 998/2000\n",
      "108/108 [==============================] - 0s 679us/step - loss: 1.5634 - accuracy: 0.4259\n",
      "\n",
      "Epoch 00998: loss did not improve from 1.56275\n",
      "Epoch 999/2000\n",
      "108/108 [==============================] - 0s 684us/step - loss: 1.5634 - accuracy: 0.4167\n",
      "\n",
      "Epoch 00999: loss did not improve from 1.56275\n",
      "Epoch 1000/2000\n",
      "108/108 [==============================] - 0s 673us/step - loss: 1.5633 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01000: loss did not improve from 1.56275\n",
      "Epoch 1001/2000\n",
      "108/108 [==============================] - 0s 690us/step - loss: 1.5633 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01001: loss did not improve from 1.56275\n",
      "Epoch 1002/2000\n",
      "108/108 [==============================] - 0s 666us/step - loss: 1.5634 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01002: loss did not improve from 1.56275\n",
      "Epoch 1003/2000\n",
      "108/108 [==============================] - 0s 679us/step - loss: 1.5633 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01003: loss did not improve from 1.56275\n",
      "Epoch 1004/2000\n",
      "108/108 [==============================] - 0s 699us/step - loss: 1.5634 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01004: loss did not improve from 1.56275\n",
      "Epoch 1005/2000\n",
      "108/108 [==============================] - 0s 691us/step - loss: 1.5633 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01005: loss did not improve from 1.56275\n",
      "Epoch 1006/2000\n",
      "108/108 [==============================] - 0s 673us/step - loss: 1.5629 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01006: loss did not improve from 1.56275\n",
      "Epoch 1007/2000\n",
      "108/108 [==============================] - 0s 680us/step - loss: 1.5627 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01007: loss improved from 1.56275 to 1.56271, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 1008/2000\n",
      "108/108 [==============================] - 0s 716us/step - loss: 1.5631 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01008: loss did not improve from 1.56271\n",
      "Epoch 1009/2000\n",
      "108/108 [==============================] - 0s 844us/step - loss: 1.5628 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01009: loss did not improve from 1.56271\n",
      "Epoch 1010/2000\n",
      "108/108 [==============================] - 0s 779us/step - loss: 1.5629 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01010: loss did not improve from 1.56271\n",
      "Epoch 1011/2000\n",
      "108/108 [==============================] - 0s 736us/step - loss: 1.5631 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01011: loss did not improve from 1.56271\n",
      "Epoch 1012/2000\n",
      "108/108 [==============================] - 0s 776us/step - loss: 1.5630 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01012: loss did not improve from 1.56271\n",
      "Epoch 1013/2000\n",
      "108/108 [==============================] - 0s 797us/step - loss: 1.5628 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01013: loss did not improve from 1.56271\n",
      "Epoch 1014/2000\n",
      "108/108 [==============================] - 0s 745us/step - loss: 1.5722 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01014: loss did not improve from 1.56271\n",
      "Epoch 1015/2000\n",
      "108/108 [==============================] - 0s 745us/step - loss: 1.5650 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01015: loss did not improve from 1.56271\n",
      "Epoch 1016/2000\n",
      "108/108 [==============================] - 0s 736us/step - loss: 1.5640 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01016: loss did not improve from 1.56271\n",
      "Epoch 1017/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5637 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01017: loss did not improve from 1.56271\n",
      "Epoch 1018/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5632 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01018: loss did not improve from 1.56271\n",
      "Epoch 1019/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5630 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01019: loss did not improve from 1.56271\n",
      "Epoch 1020/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5630 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01020: loss did not improve from 1.56271\n",
      "Epoch 1021/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5631 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01021: loss did not improve from 1.56271\n",
      "Epoch 1022/2000\n",
      "108/108 [==============================] - 0s 592us/step - loss: 1.5627 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01022: loss improved from 1.56271 to 1.56271, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 1023/2000\n",
      "108/108 [==============================] - 0s 623us/step - loss: 1.5656 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01023: loss did not improve from 1.56271\n",
      "Epoch 1024/2000\n",
      "108/108 [==============================] - 0s 653us/step - loss: 1.5628 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01024: loss did not improve from 1.56271\n",
      "Epoch 1025/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5663 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01025: loss did not improve from 1.56271\n",
      "Epoch 1026/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5634 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01026: loss did not improve from 1.56271\n",
      "Epoch 1027/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5633 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01027: loss did not improve from 1.56271\n",
      "Epoch 1028/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5630 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01028: loss did not improve from 1.56271\n",
      "Epoch 1029/2000\n",
      "108/108 [==============================] - 0s 656us/step - loss: 1.5631 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01029: loss did not improve from 1.56271\n",
      "Epoch 1030/2000\n",
      "108/108 [==============================] - 0s 650us/step - loss: 1.5632 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01030: loss did not improve from 1.56271\n",
      "Epoch 1031/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5630 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01031: loss did not improve from 1.56271\n",
      "Epoch 1032/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5630 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01032: loss did not improve from 1.56271\n",
      "Epoch 1033/2000\n",
      "108/108 [==============================] - 0s 631us/step - loss: 1.5630 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01033: loss did not improve from 1.56271\n",
      "Epoch 1034/2000\n",
      "108/108 [==============================] - 0s 648us/step - loss: 1.5628 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01034: loss did not improve from 1.56271\n",
      "Epoch 1035/2000\n",
      "108/108 [==============================] - 0s 655us/step - loss: 1.5629 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01035: loss did not improve from 1.56271\n",
      "Epoch 1036/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5627 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01036: loss did not improve from 1.56271\n",
      "Epoch 1037/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5631 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01037: loss did not improve from 1.56271\n",
      "Epoch 1038/2000\n",
      "108/108 [==============================] - 0s 641us/step - loss: 1.5630 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01038: loss did not improve from 1.56271\n",
      "Epoch 1039/2000\n",
      "108/108 [==============================] - 0s 664us/step - loss: 1.5629 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01039: loss did not improve from 1.56271\n",
      "Epoch 1040/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5627 - accuracy: 0.3704\n",
      "\n",
      "Epoch 01040: loss did not improve from 1.56271\n",
      "Epoch 1041/2000\n",
      "108/108 [==============================] - 0s 655us/step - loss: 1.5628 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01041: loss did not improve from 1.56271\n",
      "Epoch 1042/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5642 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01042: loss did not improve from 1.56271\n",
      "Epoch 1043/2000\n",
      "108/108 [==============================] - 0s 633us/step - loss: 1.5644 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01043: loss did not improve from 1.56271\n",
      "Epoch 1044/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5630 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01044: loss did not improve from 1.56271\n",
      "Epoch 1045/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5629 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01045: loss did not improve from 1.56271\n",
      "Epoch 1046/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5629 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01046: loss did not improve from 1.56271\n",
      "Epoch 1047/2000\n",
      "108/108 [==============================] - 0s 642us/step - loss: 1.5630 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01047: loss did not improve from 1.56271\n",
      "Epoch 1048/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5632 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01048: loss did not improve from 1.56271\n",
      "Epoch 1049/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5628 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01049: loss did not improve from 1.56271\n",
      "Epoch 1050/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5629 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01050: loss did not improve from 1.56271\n",
      "Epoch 1051/2000\n",
      "108/108 [==============================] - 0s 644us/step - loss: 1.5629 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01051: loss did not improve from 1.56271\n",
      "Epoch 1052/2000\n",
      "108/108 [==============================] - 0s 643us/step - loss: 1.5628 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01052: loss did not improve from 1.56271\n",
      "Epoch 1053/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5629 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01053: loss did not improve from 1.56271\n",
      "Epoch 1054/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5636 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01054: loss did not improve from 1.56271\n",
      "Epoch 1055/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5632 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01055: loss did not improve from 1.56271\n",
      "Epoch 1056/2000\n",
      "108/108 [==============================] - 0s 629us/step - loss: 1.5629 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01056: loss did not improve from 1.56271\n",
      "Epoch 1057/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 655us/step - loss: 1.5630 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01057: loss did not improve from 1.56271\n",
      "Epoch 1058/2000\n",
      "108/108 [==============================] - 0s 640us/step - loss: 1.5629 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01058: loss did not improve from 1.56271\n",
      "Epoch 1059/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5630 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01059: loss did not improve from 1.56271\n",
      "Epoch 1060/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5630 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01060: loss did not improve from 1.56271\n",
      "Epoch 1061/2000\n",
      "108/108 [==============================] - 0s 626us/step - loss: 1.5631 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01061: loss did not improve from 1.56271\n",
      "Epoch 1062/2000\n",
      "108/108 [==============================] - 0s 656us/step - loss: 1.5629 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01062: loss did not improve from 1.56271\n",
      "Epoch 1063/2000\n",
      "108/108 [==============================] - 0s 642us/step - loss: 1.5628 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01063: loss did not improve from 1.56271\n",
      "Epoch 1064/2000\n",
      "108/108 [==============================] - 0s 656us/step - loss: 1.5628 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01064: loss did not improve from 1.56271\n",
      "Epoch 1065/2000\n",
      "108/108 [==============================] - 0s 665us/step - loss: 1.5627 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01065: loss improved from 1.56271 to 1.56271, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 1066/2000\n",
      "108/108 [==============================] - 0s 665us/step - loss: 1.5630 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01066: loss did not improve from 1.56271\n",
      "Epoch 1067/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5628 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01067: loss did not improve from 1.56271\n",
      "Epoch 1068/2000\n",
      "108/108 [==============================] - 0s 656us/step - loss: 1.5629 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01068: loss did not improve from 1.56271\n",
      "Epoch 1069/2000\n",
      "108/108 [==============================] - 0s 643us/step - loss: 1.5627 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01069: loss did not improve from 1.56271\n",
      "Epoch 1070/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5629 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01070: loss did not improve from 1.56271\n",
      "Epoch 1071/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5626 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01071: loss improved from 1.56271 to 1.56261, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 1072/2000\n",
      "108/108 [==============================] - 0s 697us/step - loss: 1.5624 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01072: loss improved from 1.56261 to 1.56243, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 1073/2000\n",
      "108/108 [==============================] - 0s 690us/step - loss: 1.5628 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01073: loss did not improve from 1.56243\n",
      "Epoch 1074/2000\n",
      "108/108 [==============================] - 0s 744us/step - loss: 1.5670 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01074: loss did not improve from 1.56243\n",
      "Epoch 1075/2000\n",
      "108/108 [==============================] - 0s 769us/step - loss: 2.4412 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01075: loss did not improve from 1.56243\n",
      "Epoch 1076/2000\n",
      "108/108 [==============================] - 0s 660us/step - loss: 2.0136 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01076: loss did not improve from 1.56243\n",
      "Epoch 1077/2000\n",
      "108/108 [==============================] - 0s 665us/step - loss: 1.6481 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01077: loss did not improve from 1.56243\n",
      "Epoch 1078/2000\n",
      "108/108 [==============================] - 0s 661us/step - loss: 1.5655 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01078: loss did not improve from 1.56243\n",
      "Epoch 1079/2000\n",
      "108/108 [==============================] - 0s 655us/step - loss: 1.5642 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01079: loss did not improve from 1.56243\n",
      "Epoch 1080/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5641 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01080: loss did not improve from 1.56243\n",
      "Epoch 1081/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5637 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01081: loss did not improve from 1.56243\n",
      "Epoch 1082/2000\n",
      "108/108 [==============================] - 0s 665us/step - loss: 1.5634 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01082: loss did not improve from 1.56243\n",
      "Epoch 1083/2000\n",
      "108/108 [==============================] - 0s 651us/step - loss: 1.5634 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01083: loss did not improve from 1.56243\n",
      "Epoch 1084/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5633 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01084: loss did not improve from 1.56243\n",
      "Epoch 1085/2000\n",
      "108/108 [==============================] - 0s 638us/step - loss: 1.5633 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01085: loss did not improve from 1.56243\n",
      "Epoch 1086/2000\n",
      "108/108 [==============================] - 0s 641us/step - loss: 1.5628 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01086: loss did not improve from 1.56243\n",
      "Epoch 1087/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5629 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01087: loss did not improve from 1.56243\n",
      "Epoch 1088/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5631 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01088: loss did not improve from 1.56243\n",
      "Epoch 1089/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5630 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01089: loss did not improve from 1.56243\n",
      "Epoch 1090/2000\n",
      "108/108 [==============================] - 0s 683us/step - loss: 1.5629 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01090: loss did not improve from 1.56243\n",
      "Epoch 1091/2000\n",
      "108/108 [==============================] - 0s 651us/step - loss: 1.5629 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01091: loss did not improve from 1.56243\n",
      "Epoch 1092/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5628 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01092: loss did not improve from 1.56243\n",
      "Epoch 1093/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5628 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01093: loss did not improve from 1.56243\n",
      "Epoch 1094/2000\n",
      "108/108 [==============================] - 0s 661us/step - loss: 1.5629 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01094: loss did not improve from 1.56243\n",
      "Epoch 1095/2000\n",
      "108/108 [==============================] - 0s 654us/step - loss: 1.5630 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01095: loss did not improve from 1.56243\n",
      "Epoch 1096/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5631 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01096: loss did not improve from 1.56243\n",
      "Epoch 1097/2000\n",
      "108/108 [==============================] - 0s 647us/step - loss: 1.5630 - accuracy: 0.3704\n",
      "\n",
      "Epoch 01097: loss did not improve from 1.56243\n",
      "Epoch 1098/2000\n",
      "108/108 [==============================] - 0s 757us/step - loss: 1.5629 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01098: loss did not improve from 1.56243\n",
      "Epoch 1099/2000\n",
      "108/108 [==============================] - 0s 668us/step - loss: 1.5626 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01099: loss did not improve from 1.56243\n",
      "Epoch 1100/2000\n",
      "108/108 [==============================] - 0s 635us/step - loss: 1.5627 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01100: loss did not improve from 1.56243\n",
      "Epoch 1101/2000\n",
      "108/108 [==============================] - 0s 702us/step - loss: 1.5627 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01101: loss did not improve from 1.56243\n",
      "Epoch 1102/2000\n",
      "108/108 [==============================] - 0s 675us/step - loss: 1.5626 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01102: loss did not improve from 1.56243\n",
      "Epoch 1103/2000\n",
      "108/108 [==============================] - 0s 648us/step - loss: 1.5629 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01103: loss did not improve from 1.56243\n",
      "Epoch 1104/2000\n",
      "108/108 [==============================] - 0s 674us/step - loss: 1.5627 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01104: loss did not improve from 1.56243\n",
      "Epoch 1105/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5629 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01105: loss did not improve from 1.56243\n",
      "Epoch 1106/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5626 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01106: loss did not improve from 1.56243\n",
      "Epoch 1107/2000\n",
      "108/108 [==============================] - 0s 506us/step - loss: 1.5626 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01107: loss did not improve from 1.56243\n",
      "Epoch 1108/2000\n",
      "108/108 [==============================] - 0s 724us/step - loss: 1.5626 - accuracy: 0.3704\n",
      "\n",
      "Epoch 01108: loss did not improve from 1.56243\n",
      "Epoch 1109/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 646us/step - loss: 1.5628 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01109: loss did not improve from 1.56243\n",
      "Epoch 1110/2000\n",
      "108/108 [==============================] - 0s 638us/step - loss: 1.5626 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01110: loss did not improve from 1.56243\n",
      "Epoch 1111/2000\n",
      "108/108 [==============================] - 0s 702us/step - loss: 1.5625 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01111: loss did not improve from 1.56243\n",
      "Epoch 1112/2000\n",
      "108/108 [==============================] - 0s 680us/step - loss: 1.5630 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01112: loss did not improve from 1.56243\n",
      "Epoch 1113/2000\n",
      "108/108 [==============================] - 0s 673us/step - loss: 1.5626 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01113: loss did not improve from 1.56243\n",
      "Epoch 1114/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5627 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01114: loss did not improve from 1.56243\n",
      "Epoch 1115/2000\n",
      "108/108 [==============================] - 0s 688us/step - loss: 1.5627 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01115: loss did not improve from 1.56243\n",
      "Epoch 1116/2000\n",
      "108/108 [==============================] - 0s 645us/step - loss: 1.5627 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01116: loss did not improve from 1.56243\n",
      "Epoch 1117/2000\n",
      "108/108 [==============================] - 0s 720us/step - loss: 1.5627 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01117: loss did not improve from 1.56243\n",
      "Epoch 1118/2000\n",
      "108/108 [==============================] - 0s 652us/step - loss: 1.5628 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01118: loss did not improve from 1.56243\n",
      "Epoch 1119/2000\n",
      "108/108 [==============================] - 0s 647us/step - loss: 1.5624 - accuracy: 0.3704\n",
      "\n",
      "Epoch 01119: loss improved from 1.56243 to 1.56239, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 1120/2000\n",
      "108/108 [==============================] - 0s 684us/step - loss: 1.5629 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01120: loss did not improve from 1.56239\n",
      "Epoch 1121/2000\n",
      "108/108 [==============================] - 0s 666us/step - loss: 1.5627 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01121: loss did not improve from 1.56239\n",
      "Epoch 1122/2000\n",
      "108/108 [==============================] - 0s 701us/step - loss: 1.5626 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01122: loss did not improve from 1.56239\n",
      "Epoch 1123/2000\n",
      "108/108 [==============================] - 0s 703us/step - loss: 1.5628 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01123: loss did not improve from 1.56239\n",
      "Epoch 1124/2000\n",
      "108/108 [==============================] - 0s 711us/step - loss: 1.5627 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01124: loss did not improve from 1.56239\n",
      "Epoch 1125/2000\n",
      "108/108 [==============================] - 0s 678us/step - loss: 1.5627 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01125: loss did not improve from 1.56239\n",
      "Epoch 1126/2000\n",
      "108/108 [==============================] - 0s 684us/step - loss: 1.5627 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01126: loss did not improve from 1.56239\n",
      "Epoch 1127/2000\n",
      "108/108 [==============================] - 0s 662us/step - loss: 1.5628 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01127: loss did not improve from 1.56239\n",
      "Epoch 1128/2000\n",
      "108/108 [==============================] - 0s 729us/step - loss: 1.5628 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01128: loss did not improve from 1.56239\n",
      "Epoch 1129/2000\n",
      "108/108 [==============================] - 0s 696us/step - loss: 1.5628 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01129: loss did not improve from 1.56239\n",
      "Epoch 1130/2000\n",
      "108/108 [==============================] - 0s 702us/step - loss: 1.5628 - accuracy: 0.3704\n",
      "\n",
      "Epoch 01130: loss did not improve from 1.56239\n",
      "Epoch 1131/2000\n",
      "108/108 [==============================] - 0s 718us/step - loss: 1.5628 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01131: loss did not improve from 1.56239\n",
      "Epoch 1132/2000\n",
      "108/108 [==============================] - 0s 701us/step - loss: 1.5626 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01132: loss did not improve from 1.56239\n",
      "Epoch 1133/2000\n",
      "108/108 [==============================] - 0s 693us/step - loss: 1.5628 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01133: loss did not improve from 1.56239\n",
      "Epoch 1134/2000\n",
      "108/108 [==============================] - 0s 705us/step - loss: 1.5628 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01134: loss did not improve from 1.56239\n",
      "Epoch 1135/2000\n",
      "108/108 [==============================] - 0s 671us/step - loss: 1.5628 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01135: loss did not improve from 1.56239\n",
      "Epoch 1136/2000\n",
      "108/108 [==============================] - 0s 726us/step - loss: 1.5627 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01136: loss did not improve from 1.56239\n",
      "Epoch 1137/2000\n",
      "108/108 [==============================] - 0s 685us/step - loss: 1.5629 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01137: loss did not improve from 1.56239\n",
      "Epoch 1138/2000\n",
      "108/108 [==============================] - 0s 687us/step - loss: 1.5627 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01138: loss did not improve from 1.56239\n",
      "Epoch 1139/2000\n",
      "108/108 [==============================] - 0s 664us/step - loss: 1.5628 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01139: loss did not improve from 1.56239\n",
      "Epoch 1140/2000\n",
      "108/108 [==============================] - 0s 699us/step - loss: 1.5627 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01140: loss did not improve from 1.56239\n",
      "Epoch 1141/2000\n",
      "108/108 [==============================] - 0s 671us/step - loss: 1.5628 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01141: loss did not improve from 1.56239\n",
      "Epoch 1142/2000\n",
      "108/108 [==============================] - 0s 693us/step - loss: 1.5629 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01142: loss did not improve from 1.56239\n",
      "Epoch 1143/2000\n",
      "108/108 [==============================] - 0s 747us/step - loss: 1.5630 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01143: loss did not improve from 1.56239\n",
      "Epoch 1144/2000\n",
      "108/108 [==============================] - 0s 708us/step - loss: 1.5628 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01144: loss did not improve from 1.56239\n",
      "Epoch 1145/2000\n",
      "108/108 [==============================] - 0s 692us/step - loss: 1.5627 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01145: loss did not improve from 1.56239\n",
      "Epoch 1146/2000\n",
      "108/108 [==============================] - 0s 687us/step - loss: 1.5627 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01146: loss did not improve from 1.56239\n",
      "Epoch 1147/2000\n",
      "108/108 [==============================] - 0s 693us/step - loss: 1.5627 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01147: loss did not improve from 1.56239\n",
      "Epoch 1148/2000\n",
      "108/108 [==============================] - 0s 669us/step - loss: 1.5626 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01148: loss did not improve from 1.56239\n",
      "Epoch 1149/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5627 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01149: loss did not improve from 1.56239\n",
      "Epoch 1150/2000\n",
      "108/108 [==============================] - 0s 692us/step - loss: 1.5626 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01150: loss did not improve from 1.56239\n",
      "Epoch 1151/2000\n",
      "108/108 [==============================] - 0s 652us/step - loss: 1.5627 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01151: loss did not improve from 1.56239\n",
      "Epoch 1152/2000\n",
      "108/108 [==============================] - 0s 639us/step - loss: 1.5627 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01152: loss did not improve from 1.56239\n",
      "Epoch 1153/2000\n",
      "108/108 [==============================] - 0s 711us/step - loss: 1.5627 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01153: loss did not improve from 1.56239\n",
      "Epoch 1154/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5628 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01154: loss did not improve from 1.56239\n",
      "Epoch 1155/2000\n",
      "108/108 [==============================] - 0s 645us/step - loss: 1.5627 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01155: loss did not improve from 1.56239\n",
      "Epoch 1156/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5624 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01156: loss did not improve from 1.56239\n",
      "Epoch 1157/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5626 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01157: loss did not improve from 1.56239\n",
      "Epoch 1158/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5626 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01158: loss did not improve from 1.56239\n",
      "Epoch 1159/2000\n",
      "108/108 [==============================] - 0s 664us/step - loss: 1.5671 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01159: loss did not improve from 1.56239\n",
      "Epoch 1160/2000\n",
      "108/108 [==============================] - 0s 652us/step - loss: 1.5636 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01160: loss did not improve from 1.56239\n",
      "Epoch 1161/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5632 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01161: loss did not improve from 1.56239\n",
      "Epoch 1162/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5632 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01162: loss did not improve from 1.56239\n",
      "Epoch 1163/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5631 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01163: loss did not improve from 1.56239\n",
      "Epoch 1164/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5630 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01164: loss did not improve from 1.56239\n",
      "Epoch 1165/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5633 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01165: loss did not improve from 1.56239\n",
      "Epoch 1166/2000\n",
      "108/108 [==============================] - 0s 645us/step - loss: 1.5632 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01166: loss did not improve from 1.56239\n",
      "Epoch 1167/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5630 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01167: loss did not improve from 1.56239\n",
      "Epoch 1168/2000\n",
      "108/108 [==============================] - 0s 642us/step - loss: 1.5631 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01168: loss did not improve from 1.56239\n",
      "Epoch 1169/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5629 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01169: loss did not improve from 1.56239\n",
      "Epoch 1170/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5629 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01170: loss did not improve from 1.56239\n",
      "Epoch 1171/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5634 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01171: loss did not improve from 1.56239\n",
      "Epoch 1172/2000\n",
      "108/108 [==============================] - 0s 639us/step - loss: 1.5645 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01172: loss did not improve from 1.56239\n",
      "Epoch 1173/2000\n",
      "108/108 [==============================] - 0s 650us/step - loss: 1.5632 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01173: loss did not improve from 1.56239\n",
      "Epoch 1174/2000\n",
      "108/108 [==============================] - 0s 655us/step - loss: 1.5630 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01174: loss did not improve from 1.56239\n",
      "Epoch 1175/2000\n",
      "108/108 [==============================] - 0s 664us/step - loss: 1.5631 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01175: loss did not improve from 1.56239\n",
      "Epoch 1176/2000\n",
      "108/108 [==============================] - 0s 642us/step - loss: 1.5628 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01176: loss did not improve from 1.56239\n",
      "Epoch 1177/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5628 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01177: loss did not improve from 1.56239\n",
      "Epoch 1178/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5627 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01178: loss did not improve from 1.56239\n",
      "Epoch 1179/2000\n",
      "108/108 [==============================] - 0s 720us/step - loss: 1.5630 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01179: loss did not improve from 1.56239\n",
      "Epoch 1180/2000\n",
      "108/108 [==============================] - 0s 651us/step - loss: 1.5631 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01180: loss did not improve from 1.56239\n",
      "Epoch 1181/2000\n",
      "108/108 [==============================] - 0s 645us/step - loss: 1.5628 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01181: loss did not improve from 1.56239\n",
      "Epoch 1182/2000\n",
      "108/108 [==============================] - 0s 683us/step - loss: 1.5625 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01182: loss did not improve from 1.56239\n",
      "Epoch 1183/2000\n",
      "108/108 [==============================] - 0s 643us/step - loss: 1.5626 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01183: loss did not improve from 1.56239\n",
      "Epoch 1184/2000\n",
      "108/108 [==============================] - 0s 665us/step - loss: 1.5626 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01184: loss did not improve from 1.56239\n",
      "Epoch 1185/2000\n",
      "108/108 [==============================] - 0s 660us/step - loss: 1.5624 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01185: loss did not improve from 1.56239\n",
      "Epoch 1186/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5628 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01186: loss did not improve from 1.56239\n",
      "Epoch 1187/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5627 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01187: loss did not improve from 1.56239\n",
      "Epoch 1188/2000\n",
      "108/108 [==============================] - 0s 692us/step - loss: 1.5629 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01188: loss did not improve from 1.56239\n",
      "Epoch 1189/2000\n",
      "108/108 [==============================] - 0s 680us/step - loss: 1.5625 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01189: loss did not improve from 1.56239\n",
      "Epoch 1190/2000\n",
      "108/108 [==============================] - 0s 630us/step - loss: 1.5628 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01190: loss did not improve from 1.56239\n",
      "Epoch 1191/2000\n",
      "108/108 [==============================] - 0s 639us/step - loss: 1.5627 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01191: loss did not improve from 1.56239\n",
      "Epoch 1192/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5628 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01192: loss did not improve from 1.56239\n",
      "Epoch 1193/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5627 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01193: loss did not improve from 1.56239\n",
      "Epoch 1194/2000\n",
      "108/108 [==============================] - 0s 656us/step - loss: 1.5629 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01194: loss did not improve from 1.56239\n",
      "Epoch 1195/2000\n",
      "108/108 [==============================] - 0s 670us/step - loss: 1.5626 - accuracy: 0.4352\n",
      "\n",
      "Epoch 01195: loss did not improve from 1.56239\n",
      "Epoch 1196/2000\n",
      "108/108 [==============================] - 0s 645us/step - loss: 1.5626 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01196: loss did not improve from 1.56239\n",
      "Epoch 1197/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5625 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01197: loss did not improve from 1.56239\n",
      "Epoch 1198/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5626 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01198: loss did not improve from 1.56239\n",
      "Epoch 1199/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5626 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01199: loss did not improve from 1.56239\n",
      "Epoch 1200/2000\n",
      "108/108 [==============================] - 0s 642us/step - loss: 1.5625 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01200: loss did not improve from 1.56239\n",
      "Epoch 1201/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5626 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01201: loss did not improve from 1.56239\n",
      "Epoch 1202/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5625 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01202: loss did not improve from 1.56239\n",
      "Epoch 1203/2000\n",
      "108/108 [==============================] - 0s 642us/step - loss: 1.5626 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01203: loss did not improve from 1.56239\n",
      "Epoch 1204/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5628 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01204: loss did not improve from 1.56239\n",
      "Epoch 1205/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5669 - accuracy: 0.4352\n",
      "\n",
      "Epoch 01205: loss did not improve from 1.56239\n",
      "Epoch 1206/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5634 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01206: loss did not improve from 1.56239\n",
      "Epoch 1207/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5626 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01207: loss did not improve from 1.56239\n",
      "Epoch 1208/2000\n",
      "108/108 [==============================] - 0s 647us/step - loss: 1.5628 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01208: loss did not improve from 1.56239\n",
      "Epoch 1209/2000\n",
      "108/108 [==============================] - 0s 641us/step - loss: 1.5626 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01209: loss did not improve from 1.56239\n",
      "Epoch 1210/2000\n",
      "108/108 [==============================] - 0s 655us/step - loss: 1.5626 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01210: loss did not improve from 1.56239\n",
      "Epoch 1211/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5624 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01211: loss did not improve from 1.56239\n",
      "Epoch 1212/2000\n",
      "108/108 [==============================] - 0s 642us/step - loss: 1.5625 - accuracy: 0.3704\n",
      "\n",
      "Epoch 01212: loss did not improve from 1.56239\n",
      "Epoch 1213/2000\n",
      "108/108 [==============================] - 0s 655us/step - loss: 1.5625 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01213: loss did not improve from 1.56239\n",
      "Epoch 1214/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 654us/step - loss: 1.5626 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01214: loss did not improve from 1.56239\n",
      "Epoch 1215/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5625 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01215: loss did not improve from 1.56239\n",
      "Epoch 1216/2000\n",
      "108/108 [==============================] - 0s 643us/step - loss: 1.5629 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01216: loss did not improve from 1.56239\n",
      "Epoch 1217/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5706 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01217: loss did not improve from 1.56239\n",
      "Epoch 1218/2000\n",
      "108/108 [==============================] - 0s 672us/step - loss: 1.5640 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01218: loss did not improve from 1.56239\n",
      "Epoch 1219/2000\n",
      "108/108 [==============================] - 0s 652us/step - loss: 1.5627 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01219: loss did not improve from 1.56239\n",
      "Epoch 1220/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5627 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01220: loss did not improve from 1.56239\n",
      "Epoch 1221/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5625 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01221: loss did not improve from 1.56239\n",
      "Epoch 1222/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5625 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01222: loss did not improve from 1.56239\n",
      "Epoch 1223/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5625 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01223: loss did not improve from 1.56239\n",
      "Epoch 1224/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5627 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01224: loss did not improve from 1.56239\n",
      "Epoch 1225/2000\n",
      "108/108 [==============================] - 0s 655us/step - loss: 1.5622 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01225: loss improved from 1.56239 to 1.56224, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 1226/2000\n",
      "108/108 [==============================] - 0s 643us/step - loss: 1.5625 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01226: loss did not improve from 1.56224\n",
      "Epoch 1227/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5626 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01227: loss did not improve from 1.56224\n",
      "Epoch 1228/2000\n",
      "108/108 [==============================] - 0s 784us/step - loss: 1.5626 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01228: loss did not improve from 1.56224\n",
      "Epoch 1229/2000\n",
      "108/108 [==============================] - 0s 831us/step - loss: 1.5624 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01229: loss did not improve from 1.56224\n",
      "Epoch 1230/2000\n",
      "108/108 [==============================] - 0s 739us/step - loss: 1.5625 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01230: loss did not improve from 1.56224\n",
      "Epoch 1231/2000\n",
      "108/108 [==============================] - 0s 794us/step - loss: 1.5627 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01231: loss did not improve from 1.56224\n",
      "Epoch 1232/2000\n",
      "108/108 [==============================] - 0s 770us/step - loss: 1.5623 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01232: loss did not improve from 1.56224\n",
      "Epoch 1233/2000\n",
      "108/108 [==============================] - 0s 726us/step - loss: 1.5624 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01233: loss did not improve from 1.56224\n",
      "Epoch 1234/2000\n",
      "108/108 [==============================] - 0s 776us/step - loss: 1.5625 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01234: loss did not improve from 1.56224\n",
      "Epoch 1235/2000\n",
      "108/108 [==============================] - 0s 739us/step - loss: 1.5625 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01235: loss did not improve from 1.56224\n",
      "Epoch 1236/2000\n",
      "108/108 [==============================] - 0s 674us/step - loss: 1.5623 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01236: loss did not improve from 1.56224\n",
      "Epoch 1237/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5634 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01237: loss did not improve from 1.56224\n",
      "Epoch 1238/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5630 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01238: loss did not improve from 1.56224\n",
      "Epoch 1239/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5625 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01239: loss did not improve from 1.56224\n",
      "Epoch 1240/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5629 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01240: loss did not improve from 1.56224\n",
      "Epoch 1241/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5624 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01241: loss did not improve from 1.56224\n",
      "Epoch 1242/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5626 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01242: loss did not improve from 1.56224\n",
      "Epoch 1243/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5622 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01243: loss improved from 1.56224 to 1.56221, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 1244/2000\n",
      "108/108 [==============================] - 0s 655us/step - loss: 1.5626 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01244: loss did not improve from 1.56221\n",
      "Epoch 1245/2000\n",
      "108/108 [==============================] - 0s 683us/step - loss: 1.5626 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01245: loss did not improve from 1.56221\n",
      "Epoch 1246/2000\n",
      "108/108 [==============================] - 0s 696us/step - loss: 1.5626 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01246: loss did not improve from 1.56221\n",
      "Epoch 1247/2000\n",
      "108/108 [==============================] - 0s 722us/step - loss: 1.5626 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01247: loss did not improve from 1.56221\n",
      "Epoch 1248/2000\n",
      "108/108 [==============================] - 0s 689us/step - loss: 1.5625 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01248: loss did not improve from 1.56221\n",
      "Epoch 1249/2000\n",
      "108/108 [==============================] - 0s 688us/step - loss: 1.5623 - accuracy: 0.4352\n",
      "\n",
      "Epoch 01249: loss did not improve from 1.56221\n",
      "Epoch 1250/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5623 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01250: loss did not improve from 1.56221\n",
      "Epoch 1251/2000\n",
      "108/108 [==============================] - 0s 751us/step - loss: 1.5678 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01251: loss did not improve from 1.56221\n",
      "Epoch 1252/2000\n",
      "108/108 [==============================] - 0s 676us/step - loss: 1.5628 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01252: loss did not improve from 1.56221\n",
      "Epoch 1253/2000\n",
      "108/108 [==============================] - 0s 684us/step - loss: 1.5627 - accuracy: 0.4352\n",
      "\n",
      "Epoch 01253: loss did not improve from 1.56221\n",
      "Epoch 1254/2000\n",
      "108/108 [==============================] - 0s 752us/step - loss: 1.5626 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01254: loss did not improve from 1.56221\n",
      "Epoch 1255/2000\n",
      "108/108 [==============================] - 0s 669us/step - loss: 1.5623 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01255: loss did not improve from 1.56221\n",
      "Epoch 1256/2000\n",
      "108/108 [==============================] - 0s 694us/step - loss: 1.5625 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01256: loss did not improve from 1.56221\n",
      "Epoch 1257/2000\n",
      "108/108 [==============================] - 0s 675us/step - loss: 1.5623 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01257: loss did not improve from 1.56221\n",
      "Epoch 1258/2000\n",
      "108/108 [==============================] - 0s 676us/step - loss: 1.5625 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01258: loss did not improve from 1.56221\n",
      "Epoch 1259/2000\n",
      "108/108 [==============================] - 0s 673us/step - loss: 1.5625 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01259: loss did not improve from 1.56221\n",
      "Epoch 1260/2000\n",
      "108/108 [==============================] - 0s 691us/step - loss: 1.5624 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01260: loss did not improve from 1.56221\n",
      "Epoch 1261/2000\n",
      "108/108 [==============================] - 0s 689us/step - loss: 1.5625 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01261: loss did not improve from 1.56221\n",
      "Epoch 1262/2000\n",
      "108/108 [==============================] - 0s 684us/step - loss: 1.5623 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01262: loss did not improve from 1.56221\n",
      "Epoch 1263/2000\n",
      "108/108 [==============================] - 0s 667us/step - loss: 1.5624 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01263: loss did not improve from 1.56221\n",
      "Epoch 1264/2000\n",
      "108/108 [==============================] - 0s 658us/step - loss: 1.5627 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01264: loss did not improve from 1.56221\n",
      "Epoch 1265/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5628 - accuracy: 0.4444\n",
      "\n",
      "Epoch 01265: loss did not improve from 1.56221\n",
      "Epoch 1266/2000\n",
      "108/108 [==============================] - 0s 645us/step - loss: 1.5624 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01266: loss did not improve from 1.56221\n",
      "Epoch 1267/2000\n",
      "108/108 [==============================] - 0s 652us/step - loss: 1.5623 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01267: loss did not improve from 1.56221\n",
      "Epoch 1268/2000\n",
      "108/108 [==============================] - 0s 665us/step - loss: 1.5681 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01268: loss did not improve from 1.56221\n",
      "Epoch 1269/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5647 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01269: loss did not improve from 1.56221\n",
      "Epoch 1270/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5632 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01270: loss did not improve from 1.56221\n",
      "Epoch 1271/2000\n",
      "108/108 [==============================] - 0s 638us/step - loss: 1.5631 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01271: loss did not improve from 1.56221\n",
      "Epoch 1272/2000\n",
      "108/108 [==============================] - 0s 642us/step - loss: 1.5627 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01272: loss did not improve from 1.56221\n",
      "Epoch 1273/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5626 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01273: loss did not improve from 1.56221\n",
      "Epoch 1274/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5627 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01274: loss did not improve from 1.56221\n",
      "Epoch 1275/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5626 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01275: loss did not improve from 1.56221\n",
      "Epoch 1276/2000\n",
      "108/108 [==============================] - 0s 656us/step - loss: 1.5626 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01276: loss did not improve from 1.56221\n",
      "Epoch 1277/2000\n",
      "108/108 [==============================] - 0s 651us/step - loss: 1.5625 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01277: loss did not improve from 1.56221\n",
      "Epoch 1278/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5626 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01278: loss did not improve from 1.56221\n",
      "Epoch 1279/2000\n",
      "108/108 [==============================] - 0s 632us/step - loss: 1.5625 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01279: loss did not improve from 1.56221\n",
      "Epoch 1280/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5633 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01280: loss did not improve from 1.56221\n",
      "Epoch 1281/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5626 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01281: loss did not improve from 1.56221\n",
      "Epoch 1282/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5626 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01282: loss did not improve from 1.56221\n",
      "Epoch 1283/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5626 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01283: loss did not improve from 1.56221\n",
      "Epoch 1284/2000\n",
      "108/108 [==============================] - 0s 702us/step - loss: 1.5626 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01284: loss did not improve from 1.56221\n",
      "Epoch 1285/2000\n",
      "108/108 [==============================] - 0s 661us/step - loss: 1.5625 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01285: loss did not improve from 1.56221\n",
      "Epoch 1286/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5625 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01286: loss did not improve from 1.56221\n",
      "Epoch 1287/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5623 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01287: loss did not improve from 1.56221\n",
      "Epoch 1288/2000\n",
      "108/108 [==============================] - 0s 626us/step - loss: 1.5626 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01288: loss did not improve from 1.56221\n",
      "Epoch 1289/2000\n",
      "108/108 [==============================] - 0s 644us/step - loss: 1.5626 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01289: loss did not improve from 1.56221\n",
      "Epoch 1290/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5625 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01290: loss did not improve from 1.56221\n",
      "Epoch 1291/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5925 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01291: loss did not improve from 1.56221\n",
      "Epoch 1292/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5739 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01292: loss did not improve from 1.56221\n",
      "Epoch 1293/2000\n",
      "108/108 [==============================] - 0s 647us/step - loss: 1.5671 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01293: loss did not improve from 1.56221\n",
      "Epoch 1294/2000\n",
      "108/108 [==============================] - 0s 641us/step - loss: 1.5662 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01294: loss did not improve from 1.56221\n",
      "Epoch 1295/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5659 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01295: loss did not improve from 1.56221\n",
      "Epoch 1296/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5650 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01296: loss did not improve from 1.56221\n",
      "Epoch 1297/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5650 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01297: loss did not improve from 1.56221\n",
      "Epoch 1298/2000\n",
      "108/108 [==============================] - 0s 650us/step - loss: 1.5644 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01298: loss did not improve from 1.56221\n",
      "Epoch 1299/2000\n",
      "108/108 [==============================] - 0s 674us/step - loss: 1.5679 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01299: loss did not improve from 1.56221\n",
      "Epoch 1300/2000\n",
      "108/108 [==============================] - 0s 635us/step - loss: 1.5709 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01300: loss did not improve from 1.56221\n",
      "Epoch 1301/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5642 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01301: loss did not improve from 1.56221\n",
      "Epoch 1302/2000\n",
      "108/108 [==============================] - 0s 634us/step - loss: 1.5633 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01302: loss did not improve from 1.56221\n",
      "Epoch 1303/2000\n",
      "108/108 [==============================] - 0s 645us/step - loss: 1.5633 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01303: loss did not improve from 1.56221\n",
      "Epoch 1304/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5633 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01304: loss did not improve from 1.56221\n",
      "Epoch 1305/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5631 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01305: loss did not improve from 1.56221\n",
      "Epoch 1306/2000\n",
      "108/108 [==============================] - 0s 643us/step - loss: 1.5634 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01306: loss did not improve from 1.56221\n",
      "Epoch 1307/2000\n",
      "108/108 [==============================] - 0s 655us/step - loss: 1.5631 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01307: loss did not improve from 1.56221\n",
      "Epoch 1308/2000\n",
      "108/108 [==============================] - 0s 655us/step - loss: 1.5631 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01308: loss did not improve from 1.56221\n",
      "Epoch 1309/2000\n",
      "108/108 [==============================] - 0s 639us/step - loss: 1.5632 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01309: loss did not improve from 1.56221\n",
      "Epoch 1310/2000\n",
      "108/108 [==============================] - 0s 650us/step - loss: 1.5630 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01310: loss did not improve from 1.56221\n",
      "Epoch 1311/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5634 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01311: loss did not improve from 1.56221\n",
      "Epoch 1312/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5631 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01312: loss did not improve from 1.56221\n",
      "Epoch 1313/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5631 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01313: loss did not improve from 1.56221\n",
      "Epoch 1314/2000\n",
      "108/108 [==============================] - 0s 643us/step - loss: 1.5629 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01314: loss did not improve from 1.56221\n",
      "Epoch 1315/2000\n",
      "108/108 [==============================] - 0s 645us/step - loss: 1.5630 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01315: loss did not improve from 1.56221\n",
      "Epoch 1316/2000\n",
      "108/108 [==============================] - 0s 665us/step - loss: 1.5650 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01316: loss did not improve from 1.56221\n",
      "Epoch 1317/2000\n",
      "108/108 [==============================] - 0s 660us/step - loss: 1.5633 - accuracy: 0.3704\n",
      "\n",
      "Epoch 01317: loss did not improve from 1.56221\n",
      "Epoch 1318/2000\n",
      "108/108 [==============================] - 0s 664us/step - loss: 1.5636 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01318: loss did not improve from 1.56221\n",
      "Epoch 1319/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 665us/step - loss: 1.5635 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01319: loss did not improve from 1.56221\n",
      "Epoch 1320/2000\n",
      "108/108 [==============================] - 0s 652us/step - loss: 1.5640 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01320: loss did not improve from 1.56221\n",
      "Epoch 1321/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5660 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01321: loss did not improve from 1.56221\n",
      "Epoch 1322/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5626 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01322: loss did not improve from 1.56221\n",
      "Epoch 1323/2000\n",
      "108/108 [==============================] - 0s 655us/step - loss: 1.5622 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01323: loss improved from 1.56221 to 1.56220, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 1324/2000\n",
      "108/108 [==============================] - 0s 658us/step - loss: 1.5650 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01324: loss did not improve from 1.56220\n",
      "Epoch 1325/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5629 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01325: loss did not improve from 1.56220\n",
      "Epoch 1326/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5631 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01326: loss did not improve from 1.56220\n",
      "Epoch 1327/2000\n",
      "108/108 [==============================] - 0s 661us/step - loss: 1.5629 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01327: loss did not improve from 1.56220\n",
      "Epoch 1328/2000\n",
      "108/108 [==============================] - 0s 655us/step - loss: 1.5626 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01328: loss did not improve from 1.56220\n",
      "Epoch 1329/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5629 - accuracy: 0.3704\n",
      "\n",
      "Epoch 01329: loss did not improve from 1.56220\n",
      "Epoch 1330/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5626 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01330: loss did not improve from 1.56220\n",
      "Epoch 1331/2000\n",
      "108/108 [==============================] - 0s 720us/step - loss: 1.5626 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01331: loss did not improve from 1.56220\n",
      "Epoch 1332/2000\n",
      "108/108 [==============================] - 0s 632us/step - loss: 1.5628 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01332: loss did not improve from 1.56220\n",
      "Epoch 1333/2000\n",
      "108/108 [==============================] - 0s 655us/step - loss: 1.5626 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01333: loss did not improve from 1.56220\n",
      "Epoch 1334/2000\n",
      "108/108 [==============================] - 0s 665us/step - loss: 1.5626 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01334: loss did not improve from 1.56220\n",
      "Epoch 1335/2000\n",
      "108/108 [==============================] - 0s 642us/step - loss: 1.5628 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01335: loss did not improve from 1.56220\n",
      "Epoch 1336/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5626 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01336: loss did not improve from 1.56220\n",
      "Epoch 1337/2000\n",
      "108/108 [==============================] - 0s 661us/step - loss: 1.5625 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01337: loss did not improve from 1.56220\n",
      "Epoch 1338/2000\n",
      "108/108 [==============================] - 0s 655us/step - loss: 1.5619 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01338: loss improved from 1.56220 to 1.56185, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 1339/2000\n",
      "108/108 [==============================] - 0s 630us/step - loss: 1.5689 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01339: loss did not improve from 1.56185\n",
      "Epoch 1340/2000\n",
      "108/108 [==============================] - 0s 670us/step - loss: 1.5947 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01340: loss did not improve from 1.56185\n",
      "Epoch 1341/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5689 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01341: loss did not improve from 1.56185\n",
      "Epoch 1342/2000\n",
      "108/108 [==============================] - 0s 665us/step - loss: 1.5632 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01342: loss did not improve from 1.56185\n",
      "Epoch 1343/2000\n",
      "108/108 [==============================] - 0s 652us/step - loss: 1.5629 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01343: loss did not improve from 1.56185\n",
      "Epoch 1344/2000\n",
      "108/108 [==============================] - 0s 665us/step - loss: 1.5629 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01344: loss did not improve from 1.56185\n",
      "Epoch 1345/2000\n",
      "108/108 [==============================] - 0s 645us/step - loss: 1.5630 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01345: loss did not improve from 1.56185\n",
      "Epoch 1346/2000\n",
      "108/108 [==============================] - 0s 624us/step - loss: 1.5627 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01346: loss did not improve from 1.56185\n",
      "Epoch 1347/2000\n",
      "108/108 [==============================] - 0s 674us/step - loss: 1.5628 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01347: loss did not improve from 1.56185\n",
      "Epoch 1348/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5626 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01348: loss did not improve from 1.56185\n",
      "Epoch 1349/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5629 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01349: loss did not improve from 1.56185\n",
      "Epoch 1350/2000\n",
      "108/108 [==============================] - 0s 693us/step - loss: 1.5628 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01350: loss did not improve from 1.56185\n",
      "Epoch 1351/2000\n",
      "108/108 [==============================] - 0s 711us/step - loss: 1.5630 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01351: loss did not improve from 1.56185\n",
      "Epoch 1352/2000\n",
      "108/108 [==============================] - 0s 641us/step - loss: 1.5629 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01352: loss did not improve from 1.56185\n",
      "Epoch 1353/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5628 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01353: loss did not improve from 1.56185\n",
      "Epoch 1354/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5625 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01354: loss did not improve from 1.56185\n",
      "Epoch 1355/2000\n",
      "108/108 [==============================] - 0s 626us/step - loss: 1.5630 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01355: loss did not improve from 1.56185\n",
      "Epoch 1356/2000\n",
      "108/108 [==============================] - 0s 689us/step - loss: 1.5642 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01356: loss did not improve from 1.56185\n",
      "Epoch 1357/2000\n",
      "108/108 [==============================] - 0s 664us/step - loss: 1.5633 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01357: loss did not improve from 1.56185\n",
      "Epoch 1358/2000\n",
      "108/108 [==============================] - 0s 640us/step - loss: 1.5625 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01358: loss did not improve from 1.56185\n",
      "Epoch 1359/2000\n",
      "108/108 [==============================] - 0s 666us/step - loss: 1.5628 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01359: loss did not improve from 1.56185\n",
      "Epoch 1360/2000\n",
      "108/108 [==============================] - 0s 655us/step - loss: 1.5626 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01360: loss did not improve from 1.56185\n",
      "Epoch 1361/2000\n",
      "108/108 [==============================] - 0s 639us/step - loss: 2.0728 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01361: loss did not improve from 1.56185\n",
      "Epoch 1362/2000\n",
      "108/108 [==============================] - 0s 659us/step - loss: 1.5794 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01362: loss did not improve from 1.56185\n",
      "Epoch 1363/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5647 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01363: loss did not improve from 1.56185\n",
      "Epoch 1364/2000\n",
      "108/108 [==============================] - 0s 665us/step - loss: 1.5637 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01364: loss did not improve from 1.56185\n",
      "Epoch 1365/2000\n",
      "108/108 [==============================] - 0s 652us/step - loss: 1.5636 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01365: loss did not improve from 1.56185\n",
      "Epoch 1366/2000\n",
      "108/108 [==============================] - 0s 645us/step - loss: 1.5632 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01366: loss did not improve from 1.56185\n",
      "Epoch 1367/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5630 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01367: loss did not improve from 1.56185\n",
      "Epoch 1368/2000\n",
      "108/108 [==============================] - 0s 683us/step - loss: 1.5630 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01368: loss did not improve from 1.56185\n",
      "Epoch 1369/2000\n",
      "108/108 [==============================] - 0s 669us/step - loss: 1.5631 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01369: loss did not improve from 1.56185\n",
      "Epoch 1370/2000\n",
      "108/108 [==============================] - 0s 655us/step - loss: 1.5632 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01370: loss did not improve from 1.56185\n",
      "Epoch 1371/2000\n",
      "108/108 [==============================] - 0s 662us/step - loss: 1.5629 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01371: loss did not improve from 1.56185\n",
      "Epoch 1372/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5628 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01372: loss did not improve from 1.56185\n",
      "Epoch 1373/2000\n",
      "108/108 [==============================] - 0s 674us/step - loss: 1.5629 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01373: loss did not improve from 1.56185\n",
      "Epoch 1374/2000\n",
      "108/108 [==============================] - 0s 652us/step - loss: 1.5628 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01374: loss did not improve from 1.56185\n",
      "Epoch 1375/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5626 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01375: loss did not improve from 1.56185\n",
      "Epoch 1376/2000\n",
      "108/108 [==============================] - 0s 674us/step - loss: 1.5631 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01376: loss did not improve from 1.56185\n",
      "Epoch 1377/2000\n",
      "108/108 [==============================] - 0s 642us/step - loss: 1.5642 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01377: loss did not improve from 1.56185\n",
      "Epoch 1378/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5626 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01378: loss did not improve from 1.56185\n",
      "Epoch 1379/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5628 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01379: loss did not improve from 1.56185\n",
      "Epoch 1380/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5628 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01380: loss did not improve from 1.56185\n",
      "Epoch 1381/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5625 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01381: loss did not improve from 1.56185\n",
      "Epoch 1382/2000\n",
      "108/108 [==============================] - 0s 642us/step - loss: 1.5626 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01382: loss did not improve from 1.56185\n",
      "Epoch 1383/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5625 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01383: loss did not improve from 1.56185\n",
      "Epoch 1384/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5626 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01384: loss did not improve from 1.56185\n",
      "Epoch 1385/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5626 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01385: loss did not improve from 1.56185\n",
      "Epoch 1386/2000\n",
      "108/108 [==============================] - 0s 665us/step - loss: 1.5625 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01386: loss did not improve from 1.56185\n",
      "Epoch 1387/2000\n",
      "108/108 [==============================] - 0s 642us/step - loss: 1.5624 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01387: loss did not improve from 1.56185\n",
      "Epoch 1388/2000\n",
      "108/108 [==============================] - 0s 655us/step - loss: 1.5626 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01388: loss did not improve from 1.56185\n",
      "Epoch 1389/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5622 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01389: loss did not improve from 1.56185\n",
      "Epoch 1390/2000\n",
      "108/108 [==============================] - 0s 702us/step - loss: 1.5625 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01390: loss did not improve from 1.56185\n",
      "Epoch 1391/2000\n",
      "108/108 [==============================] - 0s 631us/step - loss: 1.5623 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01391: loss did not improve from 1.56185\n",
      "Epoch 1392/2000\n",
      "108/108 [==============================] - 0s 645us/step - loss: 1.5623 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01392: loss did not improve from 1.56185\n",
      "Epoch 1393/2000\n",
      "108/108 [==============================] - 0s 680us/step - loss: 1.5623 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01393: loss did not improve from 1.56185\n",
      "Epoch 1394/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5626 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01394: loss did not improve from 1.56185\n",
      "Epoch 1395/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5624 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01395: loss did not improve from 1.56185\n",
      "Epoch 1396/2000\n",
      "108/108 [==============================] - 0s 670us/step - loss: 1.5623 - accuracy: 0.3704\n",
      "\n",
      "Epoch 01396: loss did not improve from 1.56185\n",
      "Epoch 1397/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5622 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01397: loss did not improve from 1.56185\n",
      "Epoch 1398/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5623 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01398: loss did not improve from 1.56185\n",
      "Epoch 1399/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5624 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01399: loss did not improve from 1.56185\n",
      "Epoch 1400/2000\n",
      "108/108 [==============================] - 0s 680us/step - loss: 1.5624 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01400: loss did not improve from 1.56185\n",
      "Epoch 1401/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5623 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01401: loss did not improve from 1.56185\n",
      "Epoch 1402/2000\n",
      "108/108 [==============================] - 0s 674us/step - loss: 1.5624 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01402: loss did not improve from 1.56185\n",
      "Epoch 1403/2000\n",
      "108/108 [==============================] - 0s 661us/step - loss: 1.5624 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01403: loss did not improve from 1.56185\n",
      "Epoch 1404/2000\n",
      "108/108 [==============================] - 0s 665us/step - loss: 1.5623 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01404: loss did not improve from 1.56185\n",
      "Epoch 1405/2000\n",
      "108/108 [==============================] - 0s 643us/step - loss: 1.5623 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01405: loss did not improve from 1.56185\n",
      "Epoch 1406/2000\n",
      "108/108 [==============================] - 0s 683us/step - loss: 1.5623 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01406: loss did not improve from 1.56185\n",
      "Epoch 1407/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5621 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01407: loss did not improve from 1.56185\n",
      "Epoch 1408/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5625 - accuracy: 0.4444\n",
      "\n",
      "Epoch 01408: loss did not improve from 1.56185\n",
      "Epoch 1409/2000\n",
      "108/108 [==============================] - 0s 665us/step - loss: 1.5623 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01409: loss did not improve from 1.56185\n",
      "Epoch 1410/2000\n",
      "108/108 [==============================] - 0s 626us/step - loss: 1.5622 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01410: loss did not improve from 1.56185\n",
      "Epoch 1411/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5622 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01411: loss did not improve from 1.56185\n",
      "Epoch 1412/2000\n",
      "108/108 [==============================] - 0s 643us/step - loss: 1.5623 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01412: loss did not improve from 1.56185\n",
      "Epoch 1413/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5624 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01413: loss did not improve from 1.56185\n",
      "Epoch 1414/2000\n",
      "108/108 [==============================] - 0s 635us/step - loss: 1.5624 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01414: loss did not improve from 1.56185\n",
      "Epoch 1415/2000\n",
      "108/108 [==============================] - 0s 645us/step - loss: 1.5622 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01415: loss did not improve from 1.56185\n",
      "Epoch 1416/2000\n",
      "108/108 [==============================] - 0s 674us/step - loss: 1.5624 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01416: loss did not improve from 1.56185\n",
      "Epoch 1417/2000\n",
      "108/108 [==============================] - 0s 643us/step - loss: 1.5624 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01417: loss did not improve from 1.56185\n",
      "Epoch 1418/2000\n",
      "108/108 [==============================] - 0s 645us/step - loss: 1.5622 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01418: loss did not improve from 1.56185\n",
      "Epoch 1419/2000\n",
      "108/108 [==============================] - 0s 629us/step - loss: 1.5624 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01419: loss did not improve from 1.56185\n",
      "Epoch 1420/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5623 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01420: loss did not improve from 1.56185\n",
      "Epoch 1421/2000\n",
      "108/108 [==============================] - 0s 641us/step - loss: 1.5623 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01421: loss did not improve from 1.56185\n",
      "Epoch 1422/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5623 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01422: loss did not improve from 1.56185\n",
      "Epoch 1423/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5622 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01423: loss did not improve from 1.56185\n",
      "Epoch 1424/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 628us/step - loss: 1.5626 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01424: loss did not improve from 1.56185\n",
      "Epoch 1425/2000\n",
      "108/108 [==============================] - 0s 632us/step - loss: 1.5622 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01425: loss did not improve from 1.56185\n",
      "Epoch 1426/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5621 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01426: loss did not improve from 1.56185\n",
      "Epoch 1427/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5623 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01427: loss did not improve from 1.56185\n",
      "Epoch 1428/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5623 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01428: loss did not improve from 1.56185\n",
      "Epoch 1429/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5623 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01429: loss did not improve from 1.56185\n",
      "Epoch 1430/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5622 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01430: loss did not improve from 1.56185\n",
      "Epoch 1431/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5624 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01431: loss did not improve from 1.56185\n",
      "Epoch 1432/2000\n",
      "108/108 [==============================] - 0s 655us/step - loss: 1.5624 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01432: loss did not improve from 1.56185\n",
      "Epoch 1433/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5622 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01433: loss did not improve from 1.56185\n",
      "Epoch 1434/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5625 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01434: loss did not improve from 1.56185\n",
      "Epoch 1435/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5622 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01435: loss did not improve from 1.56185\n",
      "Epoch 1436/2000\n",
      "108/108 [==============================] - 0s 665us/step - loss: 1.5624 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01436: loss did not improve from 1.56185\n",
      "Epoch 1437/2000\n",
      "108/108 [==============================] - 0s 641us/step - loss: 1.5623 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01437: loss did not improve from 1.56185\n",
      "Epoch 1438/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5624 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01438: loss did not improve from 1.56185\n",
      "Epoch 1439/2000\n",
      "108/108 [==============================] - 0s 629us/step - loss: 1.5621 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01439: loss did not improve from 1.56185\n",
      "Epoch 1440/2000\n",
      "108/108 [==============================] - 0s 695us/step - loss: 1.5623 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01440: loss did not improve from 1.56185\n",
      "Epoch 1441/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5624 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01441: loss did not improve from 1.56185\n",
      "Epoch 1442/2000\n",
      "108/108 [==============================] - 0s 683us/step - loss: 1.5624 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01442: loss did not improve from 1.56185\n",
      "Epoch 1443/2000\n",
      "108/108 [==============================] - 0s 651us/step - loss: 1.5621 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01443: loss did not improve from 1.56185\n",
      "Epoch 1444/2000\n",
      "108/108 [==============================] - 0s 655us/step - loss: 1.5626 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01444: loss did not improve from 1.56185\n",
      "Epoch 1445/2000\n",
      "108/108 [==============================] - 0s 661us/step - loss: 1.5622 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01445: loss did not improve from 1.56185\n",
      "Epoch 1446/2000\n",
      "108/108 [==============================] - 0s 674us/step - loss: 1.5623 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01446: loss did not improve from 1.56185\n",
      "Epoch 1447/2000\n",
      "108/108 [==============================] - 0s 655us/step - loss: 1.5622 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01447: loss did not improve from 1.56185\n",
      "Epoch 1448/2000\n",
      "108/108 [==============================] - 0s 651us/step - loss: 1.5624 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01448: loss did not improve from 1.56185\n",
      "Epoch 1449/2000\n",
      "108/108 [==============================] - 0s 701us/step - loss: 1.5622 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01449: loss did not improve from 1.56185\n",
      "Epoch 1450/2000\n",
      "108/108 [==============================] - 0s 818us/step - loss: 1.5623 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01450: loss did not improve from 1.56185\n",
      "Epoch 1451/2000\n",
      "108/108 [==============================] - 0s 782us/step - loss: 1.5627 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01451: loss did not improve from 1.56185\n",
      "Epoch 1452/2000\n",
      "108/108 [==============================] - 0s 754us/step - loss: 1.5623 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01452: loss did not improve from 1.56185\n",
      "Epoch 1453/2000\n",
      "108/108 [==============================] - 0s 785us/step - loss: 1.5623 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01453: loss did not improve from 1.56185\n",
      "Epoch 1454/2000\n",
      "108/108 [==============================] - 0s 733us/step - loss: 1.5623 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01454: loss did not improve from 1.56185\n",
      "Epoch 1455/2000\n",
      "108/108 [==============================] - 0s 785us/step - loss: 1.5622 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01455: loss did not improve from 1.56185\n",
      "Epoch 1456/2000\n",
      "108/108 [==============================] - 0s 750us/step - loss: 1.5623 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01456: loss did not improve from 1.56185\n",
      "Epoch 1457/2000\n",
      "108/108 [==============================] - 0s 745us/step - loss: 1.5624 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01457: loss did not improve from 1.56185\n",
      "Epoch 1458/2000\n",
      "108/108 [==============================] - 0s 615us/step - loss: 1.5623 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01458: loss did not improve from 1.56185\n",
      "Epoch 1459/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5622 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01459: loss did not improve from 1.56185\n",
      "Epoch 1460/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5621 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01460: loss did not improve from 1.56185\n",
      "Epoch 1461/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5620 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01461: loss did not improve from 1.56185\n",
      "Epoch 1462/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5622 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01462: loss did not improve from 1.56185\n",
      "Epoch 1463/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5623 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01463: loss did not improve from 1.56185\n",
      "Epoch 1464/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5623 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01464: loss did not improve from 1.56185\n",
      "Epoch 1465/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5623 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01465: loss did not improve from 1.56185\n",
      "Epoch 1466/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5624 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01466: loss did not improve from 1.56185\n",
      "Epoch 1467/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5621 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01467: loss did not improve from 1.56185\n",
      "Epoch 1468/2000\n",
      "108/108 [==============================] - 0s 618us/step - loss: 1.5636 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01468: loss did not improve from 1.56185\n",
      "Epoch 1469/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5622 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01469: loss did not improve from 1.56185\n",
      "Epoch 1470/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5623 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01470: loss did not improve from 1.56185\n",
      "Epoch 1471/2000\n",
      "108/108 [==============================] - 0s 608us/step - loss: 1.5623 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01471: loss did not improve from 1.56185\n",
      "Epoch 1472/2000\n",
      "108/108 [==============================] - 0s 595us/step - loss: 1.5623 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01472: loss did not improve from 1.56185\n",
      "Epoch 1473/2000\n",
      "108/108 [==============================] - 0s 604us/step - loss: 1.5622 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01473: loss did not improve from 1.56185\n",
      "Epoch 1474/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5622 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01474: loss did not improve from 1.56185\n",
      "Epoch 1475/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5621 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01475: loss did not improve from 1.56185\n",
      "Epoch 1476/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5621 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01476: loss did not improve from 1.56185\n",
      "Epoch 1477/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 600us/step - loss: 1.5623 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01477: loss did not improve from 1.56185\n",
      "Epoch 1478/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5623 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01478: loss did not improve from 1.56185\n",
      "Epoch 1479/2000\n",
      "108/108 [==============================] - 0s 604us/step - loss: 1.5624 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01479: loss did not improve from 1.56185\n",
      "Epoch 1480/2000\n",
      "108/108 [==============================] - 0s 604us/step - loss: 1.5622 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01480: loss did not improve from 1.56185\n",
      "Epoch 1481/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5623 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01481: loss did not improve from 1.56185\n",
      "Epoch 1482/2000\n",
      "108/108 [==============================] - 0s 595us/step - loss: 1.5624 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01482: loss did not improve from 1.56185\n",
      "Epoch 1483/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5630 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01483: loss did not improve from 1.56185\n",
      "Epoch 1484/2000\n",
      "108/108 [==============================] - 0s 595us/step - loss: 1.5626 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01484: loss did not improve from 1.56185\n",
      "Epoch 1485/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5622 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01485: loss did not improve from 1.56185\n",
      "Epoch 1486/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5623 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01486: loss did not improve from 1.56185\n",
      "Epoch 1487/2000\n",
      "108/108 [==============================] - 0s 598us/step - loss: 1.5622 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01487: loss did not improve from 1.56185\n",
      "Epoch 1488/2000\n",
      "108/108 [==============================] - 0s 699us/step - loss: 1.5623 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01488: loss did not improve from 1.56185\n",
      "Epoch 1489/2000\n",
      "108/108 [==============================] - 0s 476us/step - loss: 1.5624 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01489: loss did not improve from 1.56185\n",
      "Epoch 1490/2000\n",
      "108/108 [==============================] - 0s 639us/step - loss: 1.5636 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01490: loss did not improve from 1.56185\n",
      "Epoch 1491/2000\n",
      "108/108 [==============================] - 0s 757us/step - loss: 1.5625 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01491: loss did not improve from 1.56185\n",
      "Epoch 1492/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5622 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01492: loss did not improve from 1.56185\n",
      "Epoch 1493/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5623 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01493: loss did not improve from 1.56185\n",
      "Epoch 1494/2000\n",
      "108/108 [==============================] - 0s 656us/step - loss: 1.5622 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01494: loss did not improve from 1.56185\n",
      "Epoch 1495/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5626 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01495: loss did not improve from 1.56185\n",
      "Epoch 1496/2000\n",
      "108/108 [==============================] - 0s 598us/step - loss: 1.5622 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01496: loss did not improve from 1.56185\n",
      "Epoch 1497/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5621 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01497: loss did not improve from 1.56185\n",
      "Epoch 1498/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5621 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01498: loss did not improve from 1.56185\n",
      "Epoch 1499/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5620 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01499: loss did not improve from 1.56185\n",
      "Epoch 1500/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5623 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01500: loss did not improve from 1.56185\n",
      "Epoch 1501/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5622 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01501: loss did not improve from 1.56185\n",
      "Epoch 1502/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5621 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01502: loss did not improve from 1.56185\n",
      "Epoch 1503/2000\n",
      "108/108 [==============================] - 0s 693us/step - loss: 1.5621 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01503: loss did not improve from 1.56185\n",
      "Epoch 1504/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5620 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01504: loss did not improve from 1.56185\n",
      "Epoch 1505/2000\n",
      "108/108 [==============================] - 0s 620us/step - loss: 1.5623 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01505: loss did not improve from 1.56185\n",
      "Epoch 1506/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5621 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01506: loss did not improve from 1.56185\n",
      "Epoch 1507/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5622 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01507: loss did not improve from 1.56185\n",
      "Epoch 1508/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5629 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01508: loss did not improve from 1.56185\n",
      "Epoch 1509/2000\n",
      "108/108 [==============================] - 0s 592us/step - loss: 1.5631 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01509: loss did not improve from 1.56185\n",
      "Epoch 1510/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5626 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01510: loss did not improve from 1.56185\n",
      "Epoch 1511/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5628 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01511: loss did not improve from 1.56185\n",
      "Epoch 1512/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5623 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01512: loss did not improve from 1.56185\n",
      "Epoch 1513/2000\n",
      "108/108 [==============================] - 0s 588us/step - loss: 1.5623 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01513: loss did not improve from 1.56185\n",
      "Epoch 1514/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5627 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01514: loss did not improve from 1.56185\n",
      "Epoch 1515/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5624 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01515: loss did not improve from 1.56185\n",
      "Epoch 1516/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5623 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01516: loss did not improve from 1.56185\n",
      "Epoch 1517/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5635 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01517: loss did not improve from 1.56185\n",
      "Epoch 1518/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5625 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01518: loss did not improve from 1.56185\n",
      "Epoch 1519/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5664 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01519: loss did not improve from 1.56185\n",
      "Epoch 1520/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5651 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01520: loss did not improve from 1.56185\n",
      "Epoch 1521/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5645 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01521: loss did not improve from 1.56185\n",
      "Epoch 1522/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5633 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01522: loss did not improve from 1.56185\n",
      "Epoch 1523/2000\n",
      "108/108 [==============================] - 0s 596us/step - loss: 1.5647 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01523: loss did not improve from 1.56185\n",
      "Epoch 1524/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5628 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01524: loss did not improve from 1.56185\n",
      "Epoch 1525/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5629 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01525: loss did not improve from 1.56185\n",
      "Epoch 1526/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5628 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01526: loss did not improve from 1.56185\n",
      "Epoch 1527/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5640 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01527: loss did not improve from 1.56185\n",
      "Epoch 1528/2000\n",
      "108/108 [==============================] - 0s 608us/step - loss: 1.5632 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01528: loss did not improve from 1.56185\n",
      "Epoch 1529/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5634 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01529: loss did not improve from 1.56185\n",
      "Epoch 1530/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 600us/step - loss: 1.5626 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01530: loss did not improve from 1.56185\n",
      "Epoch 1531/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5624 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01531: loss did not improve from 1.56185\n",
      "Epoch 1532/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5623 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01532: loss did not improve from 1.56185\n",
      "Epoch 1533/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5624 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01533: loss did not improve from 1.56185\n",
      "Epoch 1534/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5624 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01534: loss did not improve from 1.56185\n",
      "Epoch 1535/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5623 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01535: loss did not improve from 1.56185\n",
      "Epoch 1536/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5622 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01536: loss did not improve from 1.56185\n",
      "Epoch 1537/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5624 - accuracy: 0.3704\n",
      "\n",
      "Epoch 01537: loss did not improve from 1.56185\n",
      "Epoch 1538/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5624 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01538: loss did not improve from 1.56185\n",
      "Epoch 1539/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5623 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01539: loss did not improve from 1.56185\n",
      "Epoch 1540/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5621 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01540: loss did not improve from 1.56185\n",
      "Epoch 1541/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5623 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01541: loss did not improve from 1.56185\n",
      "Epoch 1542/2000\n",
      "108/108 [==============================] - 0s 586us/step - loss: 1.5623 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01542: loss did not improve from 1.56185\n",
      "Epoch 1543/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5622 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01543: loss did not improve from 1.56185\n",
      "Epoch 1544/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5621 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01544: loss did not improve from 1.56185\n",
      "Epoch 1545/2000\n",
      "108/108 [==============================] - 0s 598us/step - loss: 1.5623 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01545: loss did not improve from 1.56185\n",
      "Epoch 1546/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5623 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01546: loss did not improve from 1.56185\n",
      "Epoch 1547/2000\n",
      "108/108 [==============================] - 0s 581us/step - loss: 1.5624 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01547: loss did not improve from 1.56185\n",
      "Epoch 1548/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5622 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01548: loss did not improve from 1.56185\n",
      "Epoch 1549/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5620 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01549: loss did not improve from 1.56185\n",
      "Epoch 1550/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5622 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01550: loss did not improve from 1.56185\n",
      "Epoch 1551/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5629 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01551: loss did not improve from 1.56185\n",
      "Epoch 1552/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5621 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01552: loss did not improve from 1.56185\n",
      "Epoch 1553/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5622 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01553: loss did not improve from 1.56185\n",
      "Epoch 1554/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5622 - accuracy: 0.3704\n",
      "\n",
      "Epoch 01554: loss did not improve from 1.56185\n",
      "Epoch 1555/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5623 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01555: loss did not improve from 1.56185\n",
      "Epoch 1556/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5660 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01556: loss did not improve from 1.56185\n",
      "Epoch 1557/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5657 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01557: loss did not improve from 1.56185\n",
      "Epoch 1558/2000\n",
      "108/108 [==============================] - 0s 585us/step - loss: 2.8478 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01558: loss did not improve from 1.56185\n",
      "Epoch 1559/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 2.0439 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01559: loss did not improve from 1.56185\n",
      "Epoch 1560/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5641 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01560: loss did not improve from 1.56185\n",
      "Epoch 1561/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5632 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01561: loss did not improve from 1.56185\n",
      "Epoch 1562/2000\n",
      "108/108 [==============================] - 0s 593us/step - loss: 1.5624 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01562: loss did not improve from 1.56185\n",
      "Epoch 1563/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5623 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01563: loss did not improve from 1.56185\n",
      "Epoch 1564/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5626 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01564: loss did not improve from 1.56185\n",
      "Epoch 1565/2000\n",
      "108/108 [==============================] - 0s 581us/step - loss: 1.5623 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01565: loss did not improve from 1.56185\n",
      "Epoch 1566/2000\n",
      "108/108 [==============================] - 0s 581us/step - loss: 1.5619 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01566: loss did not improve from 1.56185\n",
      "Epoch 1567/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5623 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01567: loss did not improve from 1.56185\n",
      "Epoch 1568/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5622 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01568: loss did not improve from 1.56185\n",
      "Epoch 1569/2000\n",
      "108/108 [==============================] - 0s 561us/step - loss: 1.5627 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01569: loss did not improve from 1.56185\n",
      "Epoch 1570/2000\n",
      "108/108 [==============================] - 0s 593us/step - loss: 1.5626 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01570: loss did not improve from 1.56185\n",
      "Epoch 1571/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5626 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01571: loss did not improve from 1.56185\n",
      "Epoch 1572/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5625 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01572: loss did not improve from 1.56185\n",
      "Epoch 1573/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5624 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01573: loss did not improve from 1.56185\n",
      "Epoch 1574/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5625 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01574: loss did not improve from 1.56185\n",
      "Epoch 1575/2000\n",
      "108/108 [==============================] - 0s 618us/step - loss: 1.5621 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01575: loss did not improve from 1.56185\n",
      "Epoch 1576/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5623 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01576: loss did not improve from 1.56185\n",
      "Epoch 1577/2000\n",
      "108/108 [==============================] - 0s 598us/step - loss: 1.5621 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01577: loss did not improve from 1.56185\n",
      "Epoch 1578/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5620 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01578: loss did not improve from 1.56185\n",
      "Epoch 1579/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5621 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01579: loss did not improve from 1.56185\n",
      "Epoch 1580/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5620 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01580: loss did not improve from 1.56185\n",
      "Epoch 1581/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5622 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01581: loss did not improve from 1.56185\n",
      "Epoch 1582/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5620 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01582: loss did not improve from 1.56185\n",
      "Epoch 1583/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 619us/step - loss: 1.5621 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01583: loss did not improve from 1.56185\n",
      "Epoch 1584/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5621 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01584: loss did not improve from 1.56185\n",
      "Epoch 1585/2000\n",
      "108/108 [==============================] - 0s 598us/step - loss: 1.5622 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01585: loss did not improve from 1.56185\n",
      "Epoch 1586/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5623 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01586: loss did not improve from 1.56185\n",
      "Epoch 1587/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5621 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01587: loss did not improve from 1.56185\n",
      "Epoch 1588/2000\n",
      "108/108 [==============================] - 0s 594us/step - loss: 1.5624 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01588: loss did not improve from 1.56185\n",
      "Epoch 1589/2000\n",
      "108/108 [==============================] - 0s 587us/step - loss: 1.5621 - accuracy: 0.3704\n",
      "\n",
      "Epoch 01589: loss did not improve from 1.56185\n",
      "Epoch 1590/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5621 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01590: loss did not improve from 1.56185\n",
      "Epoch 1591/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5621 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01591: loss did not improve from 1.56185\n",
      "Epoch 1592/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5621 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01592: loss did not improve from 1.56185\n",
      "Epoch 1593/2000\n",
      "108/108 [==============================] - 0s 612us/step - loss: 1.5622 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01593: loss did not improve from 1.56185\n",
      "Epoch 1594/2000\n",
      "108/108 [==============================] - 0s 595us/step - loss: 1.5621 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01594: loss did not improve from 1.56185\n",
      "Epoch 1595/2000\n",
      "108/108 [==============================] - 0s 562us/step - loss: 1.5620 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01595: loss did not improve from 1.56185\n",
      "Epoch 1596/2000\n",
      "108/108 [==============================] - 0s 589us/step - loss: 1.5620 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01596: loss did not improve from 1.56185\n",
      "Epoch 1597/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5621 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01597: loss did not improve from 1.56185\n",
      "Epoch 1598/2000\n",
      "108/108 [==============================] - 0s 618us/step - loss: 1.5622 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01598: loss did not improve from 1.56185\n",
      "Epoch 1599/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5622 - accuracy: 0.3704\n",
      "\n",
      "Epoch 01599: loss did not improve from 1.56185\n",
      "Epoch 1600/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5619 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01600: loss did not improve from 1.56185\n",
      "Epoch 1601/2000\n",
      "108/108 [==============================] - 0s 617us/step - loss: 1.5622 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01601: loss did not improve from 1.56185\n",
      "Epoch 1602/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5622 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01602: loss did not improve from 1.56185\n",
      "Epoch 1603/2000\n",
      "108/108 [==============================] - 0s 608us/step - loss: 1.5622 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01603: loss did not improve from 1.56185\n",
      "Epoch 1604/2000\n",
      "108/108 [==============================] - 0s 608us/step - loss: 1.5622 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01604: loss did not improve from 1.56185\n",
      "Epoch 1605/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5623 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01605: loss did not improve from 1.56185\n",
      "Epoch 1606/2000\n",
      "108/108 [==============================] - 0s 598us/step - loss: 1.5622 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01606: loss did not improve from 1.56185\n",
      "Epoch 1607/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5621 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01607: loss did not improve from 1.56185\n",
      "Epoch 1608/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5623 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01608: loss did not improve from 1.56185\n",
      "Epoch 1609/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5622 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01609: loss did not improve from 1.56185\n",
      "Epoch 1610/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5622 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01610: loss did not improve from 1.56185\n",
      "Epoch 1611/2000\n",
      "108/108 [==============================] - 0s 608us/step - loss: 1.5620 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01611: loss did not improve from 1.56185\n",
      "Epoch 1612/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5622 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01612: loss did not improve from 1.56185\n",
      "Epoch 1613/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5621 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01613: loss did not improve from 1.56185\n",
      "Epoch 1614/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5622 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01614: loss did not improve from 1.56185\n",
      "Epoch 1615/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5621 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01615: loss did not improve from 1.56185\n",
      "Epoch 1616/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5621 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01616: loss did not improve from 1.56185\n",
      "Epoch 1617/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5666 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01617: loss did not improve from 1.56185\n",
      "Epoch 1618/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5631 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01618: loss did not improve from 1.56185\n",
      "Epoch 1619/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5628 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01619: loss did not improve from 1.56185\n",
      "Epoch 1620/2000\n",
      "108/108 [==============================] - 0s 606us/step - loss: 1.5628 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01620: loss did not improve from 1.56185\n",
      "Epoch 1621/2000\n",
      "108/108 [==============================] - 0s 608us/step - loss: 1.5626 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01621: loss did not improve from 1.56185\n",
      "Epoch 1622/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5625 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01622: loss did not improve from 1.56185\n",
      "Epoch 1623/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5623 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01623: loss did not improve from 1.56185\n",
      "Epoch 1624/2000\n",
      "108/108 [==============================] - 0s 588us/step - loss: 1.5621 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01624: loss did not improve from 1.56185\n",
      "Epoch 1625/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5625 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01625: loss did not improve from 1.56185\n",
      "Epoch 1626/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5622 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01626: loss did not improve from 1.56185\n",
      "Epoch 1627/2000\n",
      "108/108 [==============================] - 0s 601us/step - loss: 1.5625 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01627: loss did not improve from 1.56185\n",
      "Epoch 1628/2000\n",
      "108/108 [==============================] - 0s 608us/step - loss: 1.5624 - accuracy: 0.3704\n",
      "\n",
      "Epoch 01628: loss did not improve from 1.56185\n",
      "Epoch 1629/2000\n",
      "108/108 [==============================] - 0s 581us/step - loss: 1.5622 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01629: loss did not improve from 1.56185\n",
      "Epoch 1630/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5624 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01630: loss did not improve from 1.56185\n",
      "Epoch 1631/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5623 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01631: loss did not improve from 1.56185\n",
      "Epoch 1632/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5624 - accuracy: 0.4352\n",
      "\n",
      "Epoch 01632: loss did not improve from 1.56185\n",
      "Epoch 1633/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5624 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01633: loss did not improve from 1.56185\n",
      "Epoch 1634/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5622 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01634: loss did not improve from 1.56185\n",
      "Epoch 1635/2000\n",
      "108/108 [==============================] - 0s 608us/step - loss: 1.5622 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01635: loss did not improve from 1.56185\n",
      "Epoch 1636/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 600us/step - loss: 1.5623 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01636: loss did not improve from 1.56185\n",
      "Epoch 1637/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5624 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01637: loss did not improve from 1.56185\n",
      "Epoch 1638/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5621 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01638: loss did not improve from 1.56185\n",
      "Epoch 1639/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5623 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01639: loss did not improve from 1.56185\n",
      "Epoch 1640/2000\n",
      "108/108 [==============================] - 0s 589us/step - loss: 1.5627 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01640: loss did not improve from 1.56185\n",
      "Epoch 1641/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5626 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01641: loss did not improve from 1.56185\n",
      "Epoch 1642/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5626 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01642: loss did not improve from 1.56185\n",
      "Epoch 1643/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5622 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01643: loss did not improve from 1.56185\n",
      "Epoch 1644/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5622 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01644: loss did not improve from 1.56185\n",
      "Epoch 1645/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5620 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01645: loss did not improve from 1.56185\n",
      "Epoch 1646/2000\n",
      "108/108 [==============================] - 0s 582us/step - loss: 1.5623 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01646: loss did not improve from 1.56185\n",
      "Epoch 1647/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5621 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01647: loss did not improve from 1.56185\n",
      "Epoch 1648/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5623 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01648: loss did not improve from 1.56185\n",
      "Epoch 1649/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5621 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01649: loss did not improve from 1.56185\n",
      "Epoch 1650/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5622 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01650: loss did not improve from 1.56185\n",
      "Epoch 1651/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5620 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01651: loss did not improve from 1.56185\n",
      "Epoch 1652/2000\n",
      "108/108 [==============================] - 0s 595us/step - loss: 1.5622 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01652: loss did not improve from 1.56185\n",
      "Epoch 1653/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5621 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01653: loss did not improve from 1.56185\n",
      "Epoch 1654/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5620 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01654: loss did not improve from 1.56185\n",
      "Epoch 1655/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5621 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01655: loss did not improve from 1.56185\n",
      "Epoch 1656/2000\n",
      "108/108 [==============================] - 0s 595us/step - loss: 1.5621 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01656: loss did not improve from 1.56185\n",
      "Epoch 1657/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5621 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01657: loss did not improve from 1.56185\n",
      "Epoch 1658/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5621 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01658: loss did not improve from 1.56185\n",
      "Epoch 1659/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5620 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01659: loss did not improve from 1.56185\n",
      "Epoch 1660/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5621 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01660: loss did not improve from 1.56185\n",
      "Epoch 1661/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5620 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01661: loss did not improve from 1.56185\n",
      "Epoch 1662/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5620 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01662: loss did not improve from 1.56185\n",
      "Epoch 1663/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5621 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01663: loss did not improve from 1.56185\n",
      "Epoch 1664/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5621 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01664: loss did not improve from 1.56185\n",
      "Epoch 1665/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5621 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01665: loss did not improve from 1.56185\n",
      "Epoch 1666/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5620 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01666: loss did not improve from 1.56185\n",
      "Epoch 1667/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5621 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01667: loss did not improve from 1.56185\n",
      "Epoch 1668/2000\n",
      "108/108 [==============================] - 0s 597us/step - loss: 1.5621 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01668: loss did not improve from 1.56185\n",
      "Epoch 1669/2000\n",
      "108/108 [==============================] - 0s 588us/step - loss: 1.5622 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01669: loss did not improve from 1.56185\n",
      "Epoch 1670/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5622 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01670: loss did not improve from 1.56185\n",
      "Epoch 1671/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5625 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01671: loss did not improve from 1.56185\n",
      "Epoch 1672/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5622 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01672: loss did not improve from 1.56185\n",
      "Epoch 1673/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5622 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01673: loss did not improve from 1.56185\n",
      "Epoch 1674/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5622 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01674: loss did not improve from 1.56185\n",
      "Epoch 1675/2000\n",
      "108/108 [==============================] - 0s 580us/step - loss: 1.5621 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01675: loss did not improve from 1.56185\n",
      "Epoch 1676/2000\n",
      "108/108 [==============================] - 0s 601us/step - loss: 1.5621 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01676: loss did not improve from 1.56185\n",
      "Epoch 1677/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5620 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01677: loss did not improve from 1.56185\n",
      "Epoch 1678/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5619 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01678: loss did not improve from 1.56185\n",
      "Epoch 1679/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5618 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01679: loss improved from 1.56185 to 1.56177, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 1680/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5621 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01680: loss did not improve from 1.56177\n",
      "Epoch 1681/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5624 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01681: loss did not improve from 1.56177\n",
      "Epoch 1682/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5622 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01682: loss did not improve from 1.56177\n",
      "Epoch 1683/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5619 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01683: loss did not improve from 1.56177\n",
      "Epoch 1684/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5621 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01684: loss did not improve from 1.56177\n",
      "Epoch 1685/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5620 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01685: loss did not improve from 1.56177\n",
      "Epoch 1686/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5621 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01686: loss did not improve from 1.56177\n",
      "Epoch 1687/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5621 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01687: loss did not improve from 1.56177\n",
      "Epoch 1688/2000\n",
      "108/108 [==============================] - 0s 775us/step - loss: 1.5620 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01688: loss did not improve from 1.56177\n",
      "Epoch 1689/2000\n",
      "108/108 [==============================] - 0s 874us/step - loss: 1.5619 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01689: loss did not improve from 1.56177\n",
      "Epoch 1690/2000\n",
      "108/108 [==============================] - 0s 770us/step - loss: 1.5620 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01690: loss did not improve from 1.56177\n",
      "Epoch 1691/2000\n",
      "108/108 [==============================] - 0s 850us/step - loss: 1.5621 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01691: loss did not improve from 1.56177\n",
      "Epoch 1692/2000\n",
      "108/108 [==============================] - 0s 758us/step - loss: 1.5621 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01692: loss did not improve from 1.56177\n",
      "Epoch 1693/2000\n",
      "108/108 [==============================] - 0s 754us/step - loss: 1.5622 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01693: loss did not improve from 1.56177\n",
      "Epoch 1694/2000\n",
      "108/108 [==============================] - 0s 745us/step - loss: 1.5622 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01694: loss did not improve from 1.56177\n",
      "Epoch 1695/2000\n",
      "108/108 [==============================] - 0s 701us/step - loss: 1.5622 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01695: loss did not improve from 1.56177\n",
      "Epoch 1696/2000\n",
      "108/108 [==============================] - 0s 643us/step - loss: 1.5621 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01696: loss did not improve from 1.56177\n",
      "Epoch 1697/2000\n",
      "108/108 [==============================] - 0s 618us/step - loss: 1.5621 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01697: loss did not improve from 1.56177\n",
      "Epoch 1698/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5622 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01698: loss did not improve from 1.56177\n",
      "Epoch 1699/2000\n",
      "108/108 [==============================] - 0s 589us/step - loss: 1.5619 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01699: loss did not improve from 1.56177\n",
      "Epoch 1700/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5620 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01700: loss did not improve from 1.56177\n",
      "Epoch 1701/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5620 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01701: loss did not improve from 1.56177\n",
      "Epoch 1702/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5620 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01702: loss did not improve from 1.56177\n",
      "Epoch 1703/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5619 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01703: loss did not improve from 1.56177\n",
      "Epoch 1704/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5621 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01704: loss did not improve from 1.56177\n",
      "Epoch 1705/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5621 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01705: loss did not improve from 1.56177\n",
      "Epoch 1706/2000\n",
      "108/108 [==============================] - 0s 665us/step - loss: 1.5620 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01706: loss did not improve from 1.56177\n",
      "Epoch 1707/2000\n",
      "108/108 [==============================] - 0s 608us/step - loss: 1.5620 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01707: loss did not improve from 1.56177\n",
      "Epoch 1708/2000\n",
      "108/108 [==============================] - 0s 581us/step - loss: 1.5620 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01708: loss did not improve from 1.56177\n",
      "Epoch 1709/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5874 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01709: loss did not improve from 1.56177\n",
      "Epoch 1710/2000\n",
      "108/108 [==============================] - 0s 622us/step - loss: 1.5636 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01710: loss did not improve from 1.56177\n",
      "Epoch 1711/2000\n",
      "108/108 [==============================] - 0s 614us/step - loss: 1.5630 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01711: loss did not improve from 1.56177\n",
      "Epoch 1712/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5628 - accuracy: 0.4352\n",
      "\n",
      "Epoch 01712: loss did not improve from 1.56177\n",
      "Epoch 1713/2000\n",
      "108/108 [==============================] - 0s 595us/step - loss: 1.5629 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01713: loss did not improve from 1.56177\n",
      "Epoch 1714/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5625 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01714: loss did not improve from 1.56177\n",
      "Epoch 1715/2000\n",
      "108/108 [==============================] - 0s 603us/step - loss: 1.5626 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01715: loss did not improve from 1.56177\n",
      "Epoch 1716/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5627 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01716: loss did not improve from 1.56177\n",
      "Epoch 1717/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5625 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01717: loss did not improve from 1.56177\n",
      "Epoch 1718/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5627 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01718: loss did not improve from 1.56177\n",
      "Epoch 1719/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5624 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01719: loss did not improve from 1.56177\n",
      "Epoch 1720/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5624 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01720: loss did not improve from 1.56177\n",
      "Epoch 1721/2000\n",
      "108/108 [==============================] - 0s 596us/step - loss: 1.5623 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01721: loss did not improve from 1.56177\n",
      "Epoch 1722/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5622 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01722: loss did not improve from 1.56177\n",
      "Epoch 1723/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5623 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01723: loss did not improve from 1.56177\n",
      "Epoch 1724/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5623 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01724: loss did not improve from 1.56177\n",
      "Epoch 1725/2000\n",
      "108/108 [==============================] - 0s 588us/step - loss: 1.5619 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01725: loss did not improve from 1.56177\n",
      "Epoch 1726/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5628 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01726: loss did not improve from 1.56177\n",
      "Epoch 1727/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5634 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01727: loss did not improve from 1.56177\n",
      "Epoch 1728/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5648 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01728: loss did not improve from 1.56177\n",
      "Epoch 1729/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5624 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01729: loss did not improve from 1.56177\n",
      "Epoch 1730/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5622 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01730: loss did not improve from 1.56177\n",
      "Epoch 1731/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5621 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01731: loss did not improve from 1.56177\n",
      "Epoch 1732/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5620 - accuracy: 0.3704\n",
      "\n",
      "Epoch 01732: loss did not improve from 1.56177\n",
      "Epoch 1733/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5622 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01733: loss did not improve from 1.56177\n",
      "Epoch 1734/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5620 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01734: loss did not improve from 1.56177\n",
      "Epoch 1735/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5619 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01735: loss did not improve from 1.56177\n",
      "Epoch 1736/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5619 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01736: loss did not improve from 1.56177\n",
      "Epoch 1737/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5620 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01737: loss did not improve from 1.56177\n",
      "Epoch 1738/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5621 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01738: loss did not improve from 1.56177\n",
      "Epoch 1739/2000\n",
      "108/108 [==============================] - 0s 656us/step - loss: 1.5621 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01739: loss did not improve from 1.56177\n",
      "Epoch 1740/2000\n",
      "108/108 [==============================] - 0s 617us/step - loss: 1.5621 - accuracy: 0.3704\n",
      "\n",
      "Epoch 01740: loss did not improve from 1.56177\n",
      "Epoch 1741/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 608us/step - loss: 1.5619 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01741: loss did not improve from 1.56177\n",
      "Epoch 1742/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5621 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01742: loss did not improve from 1.56177\n",
      "Epoch 1743/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5621 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01743: loss did not improve from 1.56177\n",
      "Epoch 1744/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5621 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01744: loss did not improve from 1.56177\n",
      "Epoch 1745/2000\n",
      "108/108 [==============================] - 0s 618us/step - loss: 1.5621 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01745: loss did not improve from 1.56177\n",
      "Epoch 1746/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5621 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01746: loss did not improve from 1.56177\n",
      "Epoch 1747/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5621 - accuracy: 0.4352\n",
      "\n",
      "Epoch 01747: loss did not improve from 1.56177\n",
      "Epoch 1748/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5621 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01748: loss did not improve from 1.56177\n",
      "Epoch 1749/2000\n",
      "108/108 [==============================] - 0s 597us/step - loss: 1.5620 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01749: loss did not improve from 1.56177\n",
      "Epoch 1750/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5631 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01750: loss did not improve from 1.56177\n",
      "Epoch 1751/2000\n",
      "108/108 [==============================] - 0s 581us/step - loss: 1.5638 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01751: loss did not improve from 1.56177\n",
      "Epoch 1752/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5652 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01752: loss did not improve from 1.56177\n",
      "Epoch 1753/2000\n",
      "108/108 [==============================] - 0s 608us/step - loss: 1.5647 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01753: loss did not improve from 1.56177\n",
      "Epoch 1754/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5633 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01754: loss did not improve from 1.56177\n",
      "Epoch 1755/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5629 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01755: loss did not improve from 1.56177\n",
      "Epoch 1756/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5628 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01756: loss did not improve from 1.56177\n",
      "Epoch 1757/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5622 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01757: loss did not improve from 1.56177\n",
      "Epoch 1758/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5621 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01758: loss did not improve from 1.56177\n",
      "Epoch 1759/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5622 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01759: loss did not improve from 1.56177\n",
      "Epoch 1760/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5623 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01760: loss did not improve from 1.56177\n",
      "Epoch 1761/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5622 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01761: loss did not improve from 1.56177\n",
      "Epoch 1762/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5623 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01762: loss did not improve from 1.56177\n",
      "Epoch 1763/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5622 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01763: loss did not improve from 1.56177\n",
      "Epoch 1764/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5625 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01764: loss did not improve from 1.56177\n",
      "Epoch 1765/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5622 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01765: loss did not improve from 1.56177\n",
      "Epoch 1766/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5623 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01766: loss did not improve from 1.56177\n",
      "Epoch 1767/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5622 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01767: loss did not improve from 1.56177\n",
      "Epoch 1768/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5622 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01768: loss did not improve from 1.56177\n",
      "Epoch 1769/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5621 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01769: loss did not improve from 1.56177\n",
      "Epoch 1770/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5618 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01770: loss did not improve from 1.56177\n",
      "Epoch 1771/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5620 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01771: loss did not improve from 1.56177\n",
      "Epoch 1772/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5620 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01772: loss did not improve from 1.56177\n",
      "Epoch 1773/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5627 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01773: loss did not improve from 1.56177\n",
      "Epoch 1774/2000\n",
      "108/108 [==============================] - 0s 593us/step - loss: 1.5628 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01774: loss did not improve from 1.56177\n",
      "Epoch 1775/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5625 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01775: loss did not improve from 1.56177\n",
      "Epoch 1776/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5621 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01776: loss did not improve from 1.56177\n",
      "Epoch 1777/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5621 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01777: loss did not improve from 1.56177\n",
      "Epoch 1778/2000\n",
      "108/108 [==============================] - 0s 595us/step - loss: 1.5623 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01778: loss did not improve from 1.56177\n",
      "Epoch 1779/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5622 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01779: loss did not improve from 1.56177\n",
      "Epoch 1780/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5620 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01780: loss did not improve from 1.56177\n",
      "Epoch 1781/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5622 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01781: loss did not improve from 1.56177\n",
      "Epoch 1782/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5620 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01782: loss did not improve from 1.56177\n",
      "Epoch 1783/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5620 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01783: loss did not improve from 1.56177\n",
      "Epoch 1784/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5621 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01784: loss did not improve from 1.56177\n",
      "Epoch 1785/2000\n",
      "108/108 [==============================] - 0s 589us/step - loss: 1.5618 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01785: loss did not improve from 1.56177\n",
      "Epoch 1786/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5618 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01786: loss did not improve from 1.56177\n",
      "Epoch 1787/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5619 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01787: loss did not improve from 1.56177\n",
      "Epoch 1788/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5618 - accuracy: 0.3704\n",
      "\n",
      "Epoch 01788: loss improved from 1.56177 to 1.56177, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 1789/2000\n",
      "108/108 [==============================] - 0s 597us/step - loss: 1.5620 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01789: loss did not improve from 1.56177\n",
      "Epoch 1790/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5620 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01790: loss did not improve from 1.56177\n",
      "Epoch 1791/2000\n",
      "108/108 [==============================] - 0s 608us/step - loss: 1.5620 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01791: loss did not improve from 1.56177\n",
      "Epoch 1792/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5620 - accuracy: 0.4352\n",
      "\n",
      "Epoch 01792: loss did not improve from 1.56177\n",
      "Epoch 1793/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5621 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01793: loss did not improve from 1.56177\n",
      "Epoch 1794/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5621 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01794: loss did not improve from 1.56177\n",
      "Epoch 1795/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5620 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01795: loss did not improve from 1.56177\n",
      "Epoch 1796/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5679 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01796: loss did not improve from 1.56177\n",
      "Epoch 1797/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5626 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01797: loss did not improve from 1.56177\n",
      "Epoch 1798/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5636 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01798: loss did not improve from 1.56177\n",
      "Epoch 1799/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5623 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01799: loss did not improve from 1.56177\n",
      "Epoch 1800/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5622 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01800: loss did not improve from 1.56177\n",
      "Epoch 1801/2000\n",
      "108/108 [==============================] - 0s 598us/step - loss: 1.5624 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01801: loss did not improve from 1.56177\n",
      "Epoch 1802/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5621 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01802: loss did not improve from 1.56177\n",
      "Epoch 1803/2000\n",
      "108/108 [==============================] - 0s 596us/step - loss: 1.5623 - accuracy: 0.3704\n",
      "\n",
      "Epoch 01803: loss did not improve from 1.56177\n",
      "Epoch 1804/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5620 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01804: loss did not improve from 1.56177\n",
      "Epoch 1805/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5619 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01805: loss did not improve from 1.56177\n",
      "Epoch 1806/2000\n",
      "108/108 [==============================] - 0s 601us/step - loss: 1.5621 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01806: loss did not improve from 1.56177\n",
      "Epoch 1807/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5619 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01807: loss did not improve from 1.56177\n",
      "Epoch 1808/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5620 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01808: loss did not improve from 1.56177\n",
      "Epoch 1809/2000\n",
      "108/108 [==============================] - 0s 582us/step - loss: 1.5620 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01809: loss did not improve from 1.56177\n",
      "Epoch 1810/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5618 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01810: loss did not improve from 1.56177\n",
      "Epoch 1811/2000\n",
      "108/108 [==============================] - 0s 620us/step - loss: 1.5620 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01811: loss did not improve from 1.56177\n",
      "Epoch 1812/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5620 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01812: loss did not improve from 1.56177\n",
      "Epoch 1813/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5618 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01813: loss did not improve from 1.56177\n",
      "Epoch 1814/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5619 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01814: loss did not improve from 1.56177\n",
      "Epoch 1815/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5695 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01815: loss did not improve from 1.56177\n",
      "Epoch 1816/2000\n",
      "108/108 [==============================] - 0s 591us/step - loss: 1.5647 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01816: loss did not improve from 1.56177\n",
      "Epoch 1817/2000\n",
      "108/108 [==============================] - 0s 593us/step - loss: 1.5618 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01817: loss did not improve from 1.56177\n",
      "Epoch 1818/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5620 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01818: loss did not improve from 1.56177\n",
      "Epoch 1819/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5619 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01819: loss did not improve from 1.56177\n",
      "Epoch 1820/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5619 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01820: loss did not improve from 1.56177\n",
      "Epoch 1821/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5620 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01821: loss did not improve from 1.56177\n",
      "Epoch 1822/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5619 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01822: loss did not improve from 1.56177\n",
      "Epoch 1823/2000\n",
      "108/108 [==============================] - 0s 608us/step - loss: 1.5626 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01823: loss did not improve from 1.56177\n",
      "Epoch 1824/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5620 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01824: loss did not improve from 1.56177\n",
      "Epoch 1825/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5636 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01825: loss did not improve from 1.56177\n",
      "Epoch 1826/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5622 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01826: loss did not improve from 1.56177\n",
      "Epoch 1827/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5621 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01827: loss did not improve from 1.56177\n",
      "Epoch 1828/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5619 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01828: loss did not improve from 1.56177\n",
      "Epoch 1829/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5619 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01829: loss did not improve from 1.56177\n",
      "Epoch 1830/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5617 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01830: loss improved from 1.56177 to 1.56172, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 1831/2000\n",
      "108/108 [==============================] - 0s 629us/step - loss: 1.5618 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01831: loss did not improve from 1.56172\n",
      "Epoch 1832/2000\n",
      "108/108 [==============================] - 0s 645us/step - loss: 1.5618 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01832: loss did not improve from 1.56172\n",
      "Epoch 1833/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5621 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01833: loss did not improve from 1.56172\n",
      "Epoch 1834/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5616 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01834: loss improved from 1.56172 to 1.56163, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 1835/2000\n",
      "108/108 [==============================] - 0s 466us/step - loss: 1.5621 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01835: loss did not improve from 1.56163\n",
      "Epoch 1836/2000\n",
      "108/108 [==============================] - 0s 644us/step - loss: 1.5623 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01836: loss did not improve from 1.56163\n",
      "Epoch 1837/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5624 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01837: loss did not improve from 1.56163\n",
      "Epoch 1838/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5619 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01838: loss did not improve from 1.56163\n",
      "Epoch 1839/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5621 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01839: loss did not improve from 1.56163\n",
      "Epoch 1840/2000\n",
      "108/108 [==============================] - 0s 651us/step - loss: 1.5618 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01840: loss did not improve from 1.56163\n",
      "Epoch 1841/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5619 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01841: loss did not improve from 1.56163\n",
      "Epoch 1842/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5620 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01842: loss did not improve from 1.56163\n",
      "Epoch 1843/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5620 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01843: loss did not improve from 1.56163\n",
      "Epoch 1844/2000\n",
      "108/108 [==============================] - 0s 645us/step - loss: 1.5619 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01844: loss did not improve from 1.56163\n",
      "Epoch 1845/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 665us/step - loss: 1.5617 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01845: loss did not improve from 1.56163\n",
      "Epoch 1846/2000\n",
      "108/108 [==============================] - 0s 643us/step - loss: 1.5617 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01846: loss did not improve from 1.56163\n",
      "Epoch 1847/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5620 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01847: loss did not improve from 1.56163\n",
      "Epoch 1848/2000\n",
      "108/108 [==============================] - 0s 711us/step - loss: 1.5618 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01848: loss did not improve from 1.56163\n",
      "Epoch 1849/2000\n",
      "108/108 [==============================] - 0s 642us/step - loss: 1.5619 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01849: loss did not improve from 1.56163\n",
      "Epoch 1850/2000\n",
      "108/108 [==============================] - 0s 674us/step - loss: 1.5620 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01850: loss did not improve from 1.56163\n",
      "Epoch 1851/2000\n",
      "108/108 [==============================] - 0s 651us/step - loss: 1.5619 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01851: loss did not improve from 1.56163\n",
      "Epoch 1852/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5619 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01852: loss did not improve from 1.56163\n",
      "Epoch 1853/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5620 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01853: loss did not improve from 1.56163\n",
      "Epoch 1854/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5618 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01854: loss did not improve from 1.56163\n",
      "Epoch 1855/2000\n",
      "108/108 [==============================] - 0s 693us/step - loss: 1.5618 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01855: loss did not improve from 1.56163\n",
      "Epoch 1856/2000\n",
      "108/108 [==============================] - 0s 670us/step - loss: 1.5620 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01856: loss did not improve from 1.56163\n",
      "Epoch 1857/2000\n",
      "108/108 [==============================] - 0s 643us/step - loss: 1.5618 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01857: loss did not improve from 1.56163\n",
      "Epoch 1858/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5620 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01858: loss did not improve from 1.56163\n",
      "Epoch 1859/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5619 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01859: loss did not improve from 1.56163\n",
      "Epoch 1860/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5619 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01860: loss did not improve from 1.56163\n",
      "Epoch 1861/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5618 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01861: loss did not improve from 1.56163\n",
      "Epoch 1862/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5631 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01862: loss did not improve from 1.56163\n",
      "Epoch 1863/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5648 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01863: loss did not improve from 1.56163\n",
      "Epoch 1864/2000\n",
      "108/108 [==============================] - 0s 632us/step - loss: 1.5618 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01864: loss did not improve from 1.56163\n",
      "Epoch 1865/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5619 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01865: loss did not improve from 1.56163\n",
      "Epoch 1866/2000\n",
      "108/108 [==============================] - 0s 635us/step - loss: 1.5617 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01866: loss did not improve from 1.56163\n",
      "Epoch 1867/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5619 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01867: loss did not improve from 1.56163\n",
      "Epoch 1868/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5620 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01868: loss did not improve from 1.56163\n",
      "Epoch 1869/2000\n",
      "108/108 [==============================] - 0s 642us/step - loss: 1.5618 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01869: loss did not improve from 1.56163\n",
      "Epoch 1870/2000\n",
      "108/108 [==============================] - 0s 665us/step - loss: 1.5619 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01870: loss did not improve from 1.56163\n",
      "Epoch 1871/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5620 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01871: loss did not improve from 1.56163\n",
      "Epoch 1872/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5619 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01872: loss did not improve from 1.56163\n",
      "Epoch 1873/2000\n",
      "108/108 [==============================] - 0s 638us/step - loss: 1.5623 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01873: loss did not improve from 1.56163\n",
      "Epoch 1874/2000\n",
      "108/108 [==============================] - 0s 650us/step - loss: 1.5650 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01874: loss did not improve from 1.56163\n",
      "Epoch 1875/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5617 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01875: loss did not improve from 1.56163\n",
      "Epoch 1876/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5619 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01876: loss did not improve from 1.56163\n",
      "Epoch 1877/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5618 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01877: loss did not improve from 1.56163\n",
      "Epoch 1878/2000\n",
      "108/108 [==============================] - 0s 652us/step - loss: 1.5620 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01878: loss did not improve from 1.56163\n",
      "Epoch 1879/2000\n",
      "108/108 [==============================] - 0s 654us/step - loss: 1.5619 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01879: loss did not improve from 1.56163\n",
      "Epoch 1880/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5619 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01880: loss did not improve from 1.56163\n",
      "Epoch 1881/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5619 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01881: loss did not improve from 1.56163\n",
      "Epoch 1882/2000\n",
      "108/108 [==============================] - 0s 629us/step - loss: 1.5620 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01882: loss did not improve from 1.56163\n",
      "Epoch 1883/2000\n",
      "108/108 [==============================] - 0s 641us/step - loss: 1.5620 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01883: loss did not improve from 1.56163\n",
      "Epoch 1884/2000\n",
      "108/108 [==============================] - 0s 641us/step - loss: 1.5619 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01884: loss did not improve from 1.56163\n",
      "Epoch 1885/2000\n",
      "108/108 [==============================] - 0s 627us/step - loss: 1.5660 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01885: loss did not improve from 1.56163\n",
      "Epoch 1886/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5621 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01886: loss did not improve from 1.56163\n",
      "Epoch 1887/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5621 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01887: loss did not improve from 1.56163\n",
      "Epoch 1888/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5623 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01888: loss did not improve from 1.56163\n",
      "Epoch 1889/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5622 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01889: loss did not improve from 1.56163\n",
      "Epoch 1890/2000\n",
      "108/108 [==============================] - 0s 645us/step - loss: 1.5618 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01890: loss did not improve from 1.56163\n",
      "Epoch 1891/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5619 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01891: loss did not improve from 1.56163\n",
      "Epoch 1892/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5620 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01892: loss did not improve from 1.56163\n",
      "Epoch 1893/2000\n",
      "108/108 [==============================] - 0s 665us/step - loss: 1.5618 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01893: loss did not improve from 1.56163\n",
      "Epoch 1894/2000\n",
      "108/108 [==============================] - 0s 643us/step - loss: 1.5618 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01894: loss did not improve from 1.56163\n",
      "Epoch 1895/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5620 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01895: loss did not improve from 1.56163\n",
      "Epoch 1896/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5619 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01896: loss did not improve from 1.56163\n",
      "Epoch 1897/2000\n",
      "108/108 [==============================] - 0s 665us/step - loss: 1.5620 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01897: loss did not improve from 1.56163\n",
      "Epoch 1898/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 642us/step - loss: 1.5619 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01898: loss did not improve from 1.56163\n",
      "Epoch 1899/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5619 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01899: loss did not improve from 1.56163\n",
      "Epoch 1900/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5625 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01900: loss did not improve from 1.56163\n",
      "Epoch 1901/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5630 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01901: loss did not improve from 1.56163\n",
      "Epoch 1902/2000\n",
      "108/108 [==============================] - 0s 653us/step - loss: 1.5636 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01902: loss did not improve from 1.56163\n",
      "Epoch 1903/2000\n",
      "108/108 [==============================] - 0s 636us/step - loss: 1.5620 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01903: loss did not improve from 1.56163\n",
      "Epoch 1904/2000\n",
      "108/108 [==============================] - 0s 581us/step - loss: 1.5620 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01904: loss did not improve from 1.56163\n",
      "Epoch 1905/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5620 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01905: loss did not improve from 1.56163\n",
      "Epoch 1906/2000\n",
      "108/108 [==============================] - 0s 645us/step - loss: 1.5619 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01906: loss did not improve from 1.56163\n",
      "Epoch 1907/2000\n",
      "108/108 [==============================] - 0s 647us/step - loss: 1.5618 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01907: loss did not improve from 1.56163\n",
      "Epoch 1908/2000\n",
      "108/108 [==============================] - 0s 699us/step - loss: 1.5618 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01908: loss did not improve from 1.56163\n",
      "Epoch 1909/2000\n",
      "108/108 [==============================] - 0s 700us/step - loss: 1.5620 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01909: loss did not improve from 1.56163\n",
      "Epoch 1910/2000\n",
      "108/108 [==============================] - 0s 698us/step - loss: 1.5618 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01910: loss did not improve from 1.56163\n",
      "Epoch 1911/2000\n",
      "108/108 [==============================] - 0s 660us/step - loss: 1.5618 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01911: loss did not improve from 1.56163\n",
      "Epoch 1912/2000\n",
      "108/108 [==============================] - 0s 704us/step - loss: 1.5618 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01912: loss did not improve from 1.56163\n",
      "Epoch 1913/2000\n",
      "108/108 [==============================] - 0s 669us/step - loss: 1.5618 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01913: loss did not improve from 1.56163\n",
      "Epoch 1914/2000\n",
      "108/108 [==============================] - 0s 714us/step - loss: 1.5621 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01914: loss did not improve from 1.56163\n",
      "Epoch 1915/2000\n",
      "108/108 [==============================] - 0s 671us/step - loss: 1.5620 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01915: loss did not improve from 1.56163\n",
      "Epoch 1916/2000\n",
      "108/108 [==============================] - 0s 699us/step - loss: 1.5620 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01916: loss did not improve from 1.56163\n",
      "Epoch 1917/2000\n",
      "108/108 [==============================] - 0s 673us/step - loss: 1.5616 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01917: loss improved from 1.56163 to 1.56157, saving model to ./model/1000001000100010011.hdf5\n",
      "Epoch 1918/2000\n",
      "108/108 [==============================] - 0s 690us/step - loss: 1.5619 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01918: loss did not improve from 1.56157\n",
      "Epoch 1919/2000\n",
      "108/108 [==============================] - 0s 811us/step - loss: 1.5620 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01919: loss did not improve from 1.56157\n",
      "Epoch 1920/2000\n",
      "108/108 [==============================] - 0s 807us/step - loss: 1.5618 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01920: loss did not improve from 1.56157\n",
      "Epoch 1921/2000\n",
      "108/108 [==============================] - 0s 754us/step - loss: 1.5620 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01921: loss did not improve from 1.56157\n",
      "Epoch 1922/2000\n",
      "108/108 [==============================] - 0s 763us/step - loss: 1.5630 - accuracy: 0.4352\n",
      "\n",
      "Epoch 01922: loss did not improve from 1.56157\n",
      "Epoch 1923/2000\n",
      "108/108 [==============================] - 0s 810us/step - loss: 1.5640 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01923: loss did not improve from 1.56157\n",
      "Epoch 1924/2000\n",
      "108/108 [==============================] - 0s 745us/step - loss: 1.5639 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01924: loss did not improve from 1.56157\n",
      "Epoch 1925/2000\n",
      "108/108 [==============================] - 0s 754us/step - loss: 1.5625 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01925: loss did not improve from 1.56157\n",
      "Epoch 1926/2000\n",
      "108/108 [==============================] - 0s 736us/step - loss: 1.5619 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01926: loss did not improve from 1.56157\n",
      "Epoch 1927/2000\n",
      "108/108 [==============================] - 0s 708us/step - loss: 1.5620 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01927: loss did not improve from 1.56157\n",
      "Epoch 1928/2000\n",
      "108/108 [==============================] - 0s 619us/step - loss: 1.5618 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01928: loss did not improve from 1.56157\n",
      "Epoch 1929/2000\n",
      "108/108 [==============================] - 0s 606us/step - loss: 1.5618 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01929: loss did not improve from 1.56157\n",
      "Epoch 1930/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5618 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01930: loss did not improve from 1.56157\n",
      "Epoch 1931/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5619 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01931: loss did not improve from 1.56157\n",
      "Epoch 1932/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5618 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01932: loss did not improve from 1.56157\n",
      "Epoch 1933/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5618 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01933: loss did not improve from 1.56157\n",
      "Epoch 1934/2000\n",
      "108/108 [==============================] - 0s 589us/step - loss: 1.5619 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01934: loss did not improve from 1.56157\n",
      "Epoch 1935/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5619 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01935: loss did not improve from 1.56157\n",
      "Epoch 1936/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5618 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01936: loss did not improve from 1.56157\n",
      "Epoch 1937/2000\n",
      "108/108 [==============================] - 0s 601us/step - loss: 1.5618 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01937: loss did not improve from 1.56157\n",
      "Epoch 1938/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5620 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01938: loss did not improve from 1.56157\n",
      "Epoch 1939/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5618 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01939: loss did not improve from 1.56157\n",
      "Epoch 1940/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5619 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01940: loss did not improve from 1.56157\n",
      "Epoch 1941/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5619 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01941: loss did not improve from 1.56157\n",
      "Epoch 1942/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5629 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01942: loss did not improve from 1.56157\n",
      "Epoch 1943/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5620 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01943: loss did not improve from 1.56157\n",
      "Epoch 1944/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5618 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01944: loss did not improve from 1.56157\n",
      "Epoch 1945/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5626 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01945: loss did not improve from 1.56157\n",
      "Epoch 1946/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5621 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01946: loss did not improve from 1.56157\n",
      "Epoch 1947/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5621 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01947: loss did not improve from 1.56157\n",
      "Epoch 1948/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5620 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01948: loss did not improve from 1.56157\n",
      "Epoch 1949/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5616 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01949: loss did not improve from 1.56157\n",
      "Epoch 1950/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5618 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01950: loss did not improve from 1.56157\n",
      "Epoch 1951/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5617 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01951: loss did not improve from 1.56157\n",
      "Epoch 1952/2000\n",
      "108/108 [==============================] - 0s 635us/step - loss: 1.5620 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01952: loss did not improve from 1.56157\n",
      "Epoch 1953/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5617 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01953: loss did not improve from 1.56157\n",
      "Epoch 1954/2000\n",
      "108/108 [==============================] - 0s 608us/step - loss: 1.5619 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01954: loss did not improve from 1.56157\n",
      "Epoch 1955/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5617 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01955: loss did not improve from 1.56157\n",
      "Epoch 1956/2000\n",
      "108/108 [==============================] - 0s 592us/step - loss: 1.5617 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01956: loss did not improve from 1.56157\n",
      "Epoch 1957/2000\n",
      "108/108 [==============================] - 0s 598us/step - loss: 1.5619 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01957: loss did not improve from 1.56157\n",
      "Epoch 1958/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5619 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01958: loss did not improve from 1.56157\n",
      "Epoch 1959/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5618 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01959: loss did not improve from 1.56157\n",
      "Epoch 1960/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5619 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01960: loss did not improve from 1.56157\n",
      "Epoch 1961/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5618 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01961: loss did not improve from 1.56157\n",
      "Epoch 1962/2000\n",
      "108/108 [==============================] - 0s 711us/step - loss: 2.4020 - accuracy: 0.3333\n",
      "\n",
      "Epoch 01962: loss did not improve from 1.56157\n",
      "Epoch 1963/2000\n",
      "108/108 [==============================] - 0s 595us/step - loss: 1.8053 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01963: loss did not improve from 1.56157\n",
      "Epoch 1964/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.7284 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01964: loss did not improve from 1.56157\n",
      "Epoch 1965/2000\n",
      "108/108 [==============================] - 0s 618us/step - loss: 1.5631 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01965: loss did not improve from 1.56157\n",
      "Epoch 1966/2000\n",
      "108/108 [==============================] - 0s 646us/step - loss: 1.5629 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01966: loss did not improve from 1.56157\n",
      "Epoch 1967/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5628 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01967: loss did not improve from 1.56157\n",
      "Epoch 1968/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5625 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01968: loss did not improve from 1.56157\n",
      "Epoch 1969/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5624 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01969: loss did not improve from 1.56157\n",
      "Epoch 1970/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5625 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01970: loss did not improve from 1.56157\n",
      "Epoch 1971/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5622 - accuracy: 0.4259\n",
      "\n",
      "Epoch 01971: loss did not improve from 1.56157\n",
      "Epoch 1972/2000\n",
      "108/108 [==============================] - 0s 628us/step - loss: 1.5624 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01972: loss did not improve from 1.56157\n",
      "Epoch 1973/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5622 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01973: loss did not improve from 1.56157\n",
      "Epoch 1974/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5621 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01974: loss did not improve from 1.56157\n",
      "Epoch 1975/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5623 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01975: loss did not improve from 1.56157\n",
      "Epoch 1976/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5622 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01976: loss did not improve from 1.56157\n",
      "Epoch 1977/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5622 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01977: loss did not improve from 1.56157\n",
      "Epoch 1978/2000\n",
      "108/108 [==============================] - 0s 588us/step - loss: 1.5621 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01978: loss did not improve from 1.56157\n",
      "Epoch 1979/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5621 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01979: loss did not improve from 1.56157\n",
      "Epoch 1980/2000\n",
      "108/108 [==============================] - 0s 612us/step - loss: 1.5621 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01980: loss did not improve from 1.56157\n",
      "Epoch 1981/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5618 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01981: loss did not improve from 1.56157\n",
      "Epoch 1982/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5620 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01982: loss did not improve from 1.56157\n",
      "Epoch 1983/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5622 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01983: loss did not improve from 1.56157\n",
      "Epoch 1984/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5621 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01984: loss did not improve from 1.56157\n",
      "Epoch 1985/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5617 - accuracy: 0.4167\n",
      "\n",
      "Epoch 01985: loss did not improve from 1.56157\n",
      "Epoch 1986/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5618 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01986: loss did not improve from 1.56157\n",
      "Epoch 1987/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5619 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01987: loss did not improve from 1.56157\n",
      "Epoch 1988/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5622 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01988: loss did not improve from 1.56157\n",
      "Epoch 1989/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5619 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01989: loss did not improve from 1.56157\n",
      "Epoch 1990/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5619 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01990: loss did not improve from 1.56157\n",
      "Epoch 1991/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5618 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01991: loss did not improve from 1.56157\n",
      "Epoch 1992/2000\n",
      "108/108 [==============================] - 0s 608us/step - loss: 1.5620 - accuracy: 0.3704\n",
      "\n",
      "Epoch 01992: loss did not improve from 1.56157\n",
      "Epoch 1993/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5618 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01993: loss did not improve from 1.56157\n",
      "Epoch 1994/2000\n",
      "108/108 [==============================] - 0s 637us/step - loss: 1.5617 - accuracy: 0.3796\n",
      "\n",
      "Epoch 01994: loss did not improve from 1.56157\n",
      "Epoch 1995/2000\n",
      "108/108 [==============================] - 0s 608us/step - loss: 1.5619 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01995: loss did not improve from 1.56157\n",
      "Epoch 1996/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5619 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01996: loss did not improve from 1.56157\n",
      "Epoch 1997/2000\n",
      "108/108 [==============================] - 0s 590us/step - loss: 1.5619 - accuracy: 0.4074\n",
      "\n",
      "Epoch 01997: loss did not improve from 1.56157\n",
      "Epoch 1998/2000\n",
      "108/108 [==============================] - 0s 600us/step - loss: 1.5618 - accuracy: 0.3981\n",
      "\n",
      "Epoch 01998: loss did not improve from 1.56157\n",
      "Epoch 1999/2000\n",
      "108/108 [==============================] - 0s 609us/step - loss: 1.5619 - accuracy: 0.3889\n",
      "\n",
      "Epoch 01999: loss did not improve from 1.56157\n",
      "Epoch 2000/2000\n",
      "108/108 [==============================] - 0s 599us/step - loss: 1.5617 - accuracy: 0.4167\n",
      "\n",
      "Epoch 02000: loss did not improve from 1.56157\n"
     ]
    }
   ],
   "source": [
    "readdata_and_savemodel(\"1000001000100010011.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '1000001000100010001.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-5d51d9dac283>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_code_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"크림\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"복합성\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'쿨톤'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-bf8fc72ea9e2>\u001b[0m in \u001b[0;36mpredict_code_value\u001b[1;34m(category_name, input_value)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_onehot_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model_hdf5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategoryno\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.hdf5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m     \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload_from_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategoryno\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategoryno\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[0mcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-bf8fc72ea9e2>\u001b[0m in \u001b[0;36mload_from_dataset\u001b[1;34m(string)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;31m# INPUT : string은 csv 파일 이름\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m     \u001b[0my_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[0mx_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kimsuhyun\\.conda\\envs\\tensor_21\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kimsuhyun\\.conda\\envs\\tensor_21\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kimsuhyun\\.conda\\envs\\tensor_21\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 936\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kimsuhyun\\.conda\\envs\\tensor_21\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1168\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1169\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1170\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kimsuhyun\\.conda\\envs\\tensor_21\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1979\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1980\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1981\u001b[1;33m                 \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1982\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1000001000100010001.csv'"
     ]
    }
   ],
   "source": [
    "a,b,c = predict_code_value(\"크림\",[\"복합성\",'쿨톤'])\n",
    "print(a,b,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 간이 프로그래밍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '1000001000100010001.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-4927bdf2862c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategory_tag_to_dictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"크림\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"지성\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'웜톤'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'트러블'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'미백'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-bf8fc72ea9e2>\u001b[0m in \u001b[0;36mcategory_tag_to_dictionary\u001b[1;34m(category_name, input_value)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcategory_tag_to_dictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategory_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m     \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_code_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategory_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m     result = {\n\u001b[0;32m    188\u001b[0m         \u001b[1;34m'name'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-bf8fc72ea9e2>\u001b[0m in \u001b[0;36mpredict_code_value\u001b[1;34m(category_name, input_value)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_onehot_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model_hdf5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategoryno\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.hdf5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m     \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload_from_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategoryno\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategoryno\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[0mcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-bf8fc72ea9e2>\u001b[0m in \u001b[0;36mload_from_dataset\u001b[1;34m(string)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;31m# INPUT : string은 csv 파일 이름\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m     \u001b[0my_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[0mx_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kimsuhyun\\.conda\\envs\\tensor_21\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kimsuhyun\\.conda\\envs\\tensor_21\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kimsuhyun\\.conda\\envs\\tensor_21\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 936\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kimsuhyun\\.conda\\envs\\tensor_21\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1168\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1169\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1170\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kimsuhyun\\.conda\\envs\\tensor_21\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1979\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1980\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1981\u001b[1;33m                 \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1982\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1000001000100010001.csv'"
     ]
    }
   ],
   "source": [
    "print(category_tag_to_dictionary(\"크림\",[\"지성\",'웜톤','트러블','미백']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    Flag = True\n",
    "    name_list = [\"복합성\",\"건성\",\"지성\",\"쿨톤\",\"웜톤\",\"잡티\",\"미백\",\"주름\",\"각질\",\"트러블\",\"블랙헤드\",\"피지과다\",\"민감성\",\"모공\",\"탄력\",\"홍조\",\"아토피\"]\n",
    "    while Flag:\n",
    "        category = input(\"제품 카테고리를 입력해 주세요(스킨/토너, 크림, 로션, 에센스/세럼, 앰플) : \")\n",
    "        print(\"\\n {}을 선택 해 주셨네요. \".format(category))\n",
    "        type_ = input(\"피부 타입을 입력해 주세요. (지성,건성,복합성)\")\n",
    "        print(\"\\n {}을 선택 해 주셨네요. \".format(type_))\n",
    "        ton = input(\"피부 톤을 입력해 주세요 (웜톤,쿨톤)\")\n",
    "        print(\"\\n {} 을 선택해 주셨네요. \".format(ton))\n",
    "#         print(\"이제 당신의 피부 고민을 선택해 주세요.\")\n",
    "        trouble = [type_,ton]\n",
    "        while True :\n",
    "            print(\"당신의 피부 고민을 선택해 주세요. (없을 0 입력)\")\n",
    "            print(name_list[5:])\n",
    "            print(\"선택 listt = \",trouble)\n",
    "            string = input()\n",
    "            if string=='0':\n",
    "                break\n",
    "            else:\n",
    "                trouble.append(string)\n",
    "        a,b,c = predict_code_value(category,trouble)\n",
    "        print(\"제품명 : \",a)\n",
    "        print(\"가격 : \",b)\n",
    "        print(\"link : \",c)\n",
    "        reply = input(\"한번더 추천해 드릴까요? choice y/n\")\n",
    "        if reply == 'n':\n",
    "            break\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제품 카테고리를 입력해 주세요(스킨/토너, 크림, 로션, 에센스/세럼, 앰플) : 스킨\n",
      "\n",
      " 스킨을 선택 해 주셨네요. \n",
      "피부 타입을 입력해 주세요. (지성,건성,복합성)복합성\n",
      "\n",
      " 복합성을 선택 해 주셨네요. \n",
      "피부 톤을 입력해 주세요 (웜톤,쿨톤)웜톤\n",
      "\n",
      " 웜톤 을 선택해 주셨네요. \n",
      "당신의 피부 고민을 선택해 주세요. (없을 0 입력)\n",
      "['잡티', '미백', '주름', '각질', '트러블', '블랙헤드', '피지과다', '민감성', '모공', '탄력', '홍조', '아토피']\n",
      "선택 listt =  ['복합성', '웜톤']\n",
      "트러블\n",
      "당신의 피부 고민을 선택해 주세요. (없을 0 입력)\n",
      "['잡티', '미백', '주름', '각질', '트러블', '블랙헤드', '피지과다', '민감성', '모공', '탄력', '홍조', '아토피']\n",
      "선택 listt =  ['복합성', '웜톤', '트러블']\n",
      "0\n",
      "제품명 :  닥터지로얄블랙스네일퍼스트에센스165mL\n",
      "가격 :  21,420\n",
      "link :  https://www.oliveyoung.co.kr/store/goods/getGoodsDetail.do?goodsNo=A000000142567&dispCatNo=1000001000100010001\n",
      "한번더 추천해 드릴까요? choice y/ny\n",
      "제품 카테고리를 입력해 주세요(스킨/토너, 크림, 로션, 에센스/세럼, 앰플) : 로션\n",
      "\n",
      " 로션을 선택 해 주셨네요. \n",
      "피부 타입을 입력해 주세요. (지성,건성,복합성)지성\n",
      "\n",
      " 지성을 선택 해 주셨네요. \n",
      "피부 톤을 입력해 주세요 (웜톤,쿨톤)웜톤\n",
      "\n",
      " 웜톤 을 선택해 주셨네요. \n",
      "당신의 피부 고민을 선택해 주세요. (없을 0 입력)\n",
      "['잡티', '미백', '주름', '각질', '트러블', '블랙헤드', '피지과다', '민감성', '모공', '탄력', '홍조', '아토피']\n",
      "선택 listt =  ['지성', '웜톤']\n",
      "홍조\n",
      "당신의 피부 고민을 선택해 주세요. (없을 0 입력)\n",
      "['잡티', '미백', '주름', '각질', '트러블', '블랙헤드', '피지과다', '민감성', '모공', '탄력', '홍조', '아토피']\n",
      "선택 listt =  ['지성', '웜톤', '홍조']\n",
      "각질\n",
      "당신의 피부 고민을 선택해 주세요. (없을 0 입력)\n",
      "['잡티', '미백', '주름', '각질', '트러블', '블랙헤드', '피지과다', '민감성', '모공', '탄력', '홍조', '아토피']\n",
      "선택 listt =  ['지성', '웜톤', '홍조', '각질']\n",
      "0\n",
      "제품명 :  듀크레이 케라크닐 매트 기름종이 로션 30ml\n",
      "가격 :  28,000\n",
      "link :  https://www.oliveyoung.co.kr/store/goods/getGoodsDetail.do?goodsNo=A000000142184&dispCatNo=1000001000100010002\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
